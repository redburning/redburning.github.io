<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SunshineDxiao&#39;s Blog</title>
  
  <subtitle>一个有情怀的攻城狮，将搬砖进行到底！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.douxiao.org/"/>
  <updated>2018-05-30T14:04:07.000Z</updated>
  <id>http://www.douxiao.org/</id>
  
  <author>
    <name>SunshineDxiao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>YOLO-v3论文阅读笔记</title>
    <link href="http://www.douxiao.org/2018/05/30/YOLO-v3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://www.douxiao.org/2018/05/30/YOLO-v3论文阅读笔记/</id>
    <published>2018-05-30T02:18:21.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>YOLO-v2论文阅读笔记</title>
    <link href="http://www.douxiao.org/2018/05/30/YOLO-v2%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://www.douxiao.org/2018/05/30/YOLO-v2论文阅读笔记/</id>
    <published>2018-05-30T02:18:15.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>YOLO-v1论文阅读笔记</title>
    <link href="http://www.douxiao.org/2018/05/30/YOLO-v1%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://www.douxiao.org/2018/05/30/YOLO-v1论文阅读笔记/</id>
    <published>2018-05-30T02:18:04.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<p><font size="5" color="lightseagreen"> 很久之前就读过YOLO的论文，因为最近YOLO-v3已经发布，有必要系统的做一下总结，这篇博客是总结YOLO-v1，在15年11份发布的版本。<br> </font><br><a id="more"></a></p><h1 id="一、背景介绍"><a href="#一、背景介绍" class="headerlink" title="一、背景介绍"></a>一、背景介绍</h1><p>　　YOLO将第一次实现将物体检测作为回归问题求解。基于一个单独的end-to-end网络，完成从原始图像的输入，到<strong>物体位置和类别及相应的置信概率</strong>的输出，在目标检测网络中属于one-stage 方法。</p><p>　　下图是yolo和基于R-CNN系列的目标检测的对比：</p><div align="center"><br><img src="http://ow7va355d.bkt.clouddn.com/yolo-vs-rcnn.jpg" alt=""><br><br>图1<br></div><h1 id="二、核心算法"><a href="#二、核心算法" class="headerlink" title="二、核心算法"></a>二、核心算法</h1><h2 id="2-1-网络结构"><a href="#2-1-网络结构" class="headerlink" title="2.1 网络结构"></a>2.1 网络结构</h2><p>　　YOLO网络结构借鉴了GoogLeNet分类网络结构，但未使用Inception module，而是使用的是1X1卷积层（1X1是为了跨通道信息整合，同时可以认为应用了一个全卷积神经网络）同时还使用了3X3卷积层简单替代。</p><p>　　<strong>下图</strong>：卷积层用来提取图像特征，全连接层用来预测图像位置和类别概率值。</p><div align="center"><br><img src="http://ow7va355d.bkt.clouddn.com/yolo1_network_arch.png" alt=""><br>图2<br><img src="http://ow7va355d.bkt.clouddn.com/yolo-1.png" alt=""><br>图3<br></div><h2 id="2-2网络实现图解"><a href="#2-2网络实现图解" class="headerlink" title="2.2网络实现图解"></a>2.2网络实现图解</h2><p>图解YOLO，如下图所示：</p><div align="center"><br><br><img src="http://ow7va355d.bkt.clouddn.com/yolo-4.png" alt=""><br><img src="http://ow7va355d.bkt.clouddn.com/yolo-5.png" alt=""><br><img src="http://ow7va355d.bkt.clouddn.com/yolo-6.png" alt=""><br><img src="http://ow7va355d.bkt.clouddn.com/yolo-7.png" alt=""><br><img src="http://ow7va355d.bkt.clouddn.com/yolo-8.png" alt=""><br><img src="http://ow7va355d.bkt.clouddn.com/yolo-9.png" alt=""><br><img src="http://ow7va355d.bkt.clouddn.com/yolo-10.png" alt=""><br><img src="http://ow7va355d.bkt.clouddn.com/yolo-11.png" alt=""><br><img src="http://ow7va355d.bkt.clouddn.com/yolo-12.png" alt=""><br><img src="http://ow7va355d.bkt.clouddn.com/yolo-13.png" alt=""><br><br>图4<br></div><h2 id="2-3损失函数"><a href="#2-3损失函数" class="headerlink" title="2.3损失函数"></a>2.3损失函数</h2><p>　　YOLO使用均方和误差作为loss函数来优化模型参数，即网络输出的SxSx(Bx5 + C)维向量与真实图像的对应SxSx(Bx5 + C)维向量的均方和误差。如下式所示。其中，coordError、iouError和classError分别代表<strong>预测数据与标定数据之间的坐标误差</strong>、<strong>IOU误差</strong>和<strong>分类误差</strong>。</p><p><img src="http://ow7va355d.bkt.clouddn.com/yolo-14.png" alt=""></p><p>　　YOLO对上式loss的计算进行了如下修正。</p><p>　　[1]位置相关误差（坐标、IOU）与分类误差对网络loss的贡献值是不同的，因此YOLO在计算loss时，使用λ{coord} = 5修正coordError。</p><p>　　[2]在计算IOU误差时，包含物体的格子与不包含物体的格子，二者的IOU误差对网络loss的贡献值是不同的。若采用相同的权值，那么不包含物体的格子的confidence值近似为0，变相放大了包含物体的格子的confidence误差在计算网络参数梯度时的影响。为解决这个问题，YOLO 使用λ{noobj} =0.5修正iouError。（注此处的‘包含’是指存在一个物体，它的中心坐标落入到格子内）。</p><p>　　[3]对于相等的误差值，大物体误差对检测的影响应小于小物体误差对检测的影响。这是因为，相同的位置偏差占大物体的比例远小于同等偏差占小物体的比例。YOLO将物体大小的信息项（w和h）进行求平方根来改进这个问题。（注：这个方法并不能完全解决这个问题）。</p><p>综上，YOLO在训练过程中Loss计算如下式所示：</p><p>loss计算如下式：</p><p><img src="http://ow7va355d.bkt.clouddn.com/yolo-15.jpg" alt=""></p><p><img src="http://ow7va355d.bkt.clouddn.com/yolo-16.png" alt=""></p><h2 id="2-4-训练"><a href="#2-4-训练" class="headerlink" title="2.4 训练"></a>2.4 训练</h2><p>　　YOLO模型训练分为两步：<br>　　1）预训练。使用ImageNet<br>1000类数据训练YOLO网络的前20个卷积层+1个average池化层+1个全连接层。训练图像分辨率resize到224x224。</p><p>　　2）用步骤1）得到的前20个卷积层网络参数来初始化YOLO模型前20个卷积层的网络参数，然后用VOC 20类标注数据进行YOLO模型训练。为提高图像精度，在训练检测模型时，将输入图像分辨率resize到448x448。</p><h1 id="三、参考文献"><a href="#三、参考文献" class="headerlink" title="三、参考文献"></a>三、参考文献</h1><ol><li><a href="https://zhuanlan.zhihu.com/p/25236464" target="_blank" rel="noopener">YOLO详解</a></li><li><a href="https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.p" target="_blank" rel="noopener">YOLO PPT</a></li><li><a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">YOLO论文</a></li><li><a href="https://github.com/thtrieu/darkflow" target="_blank" rel="noopener">github</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;5&quot; color=&quot;lightseagreen&quot;&gt; 很久之前就读过YOLO的论文，因为最近YOLO-v3已经发布，有必要系统的做一下总结，这篇博客是总结YOLO-v1，在15年11份发布的版本。&lt;br&gt; &lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="http://www.douxiao.org/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="yolo" scheme="http://www.douxiao.org/tags/yolo/"/>
    
      <category term="object-detection" scheme="http://www.douxiao.org/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>深度学习——SSD论文阅读笔记</title>
    <link href="http://www.douxiao.org/2018/05/14/SSD%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://www.douxiao.org/2018/05/14/SSD物体检测论文阅读笔记/</id>
    <published>2018-05-14T00:09:01.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<p><font size="5" color="lightseagreen"> 　　这篇文章是总结SSD算法，其英文的全名是Single Shot MultiBox Detector，Single Shot 代表了给算法属于one-stage方法，MultiBox指的是多边框预测，本文详细的介绍了SSD的工作原理，结构框架和与其他网络比如VGG,YOLO,R-CNN等网络对比。<br> </font><br><a id="more"></a></p><h1 id="一、背景介绍"><a href="#一、背景介绍" class="headerlink" title="一、背景介绍"></a>一、背景介绍</h1><p>　　基于深度学习的目标检测方法主要有两种<strong>（1）two-stage方法：R-CNN系列，其主要思路是先通过启发式方法（selective search）或者CNN网络（RPN）产生一系列的候选框，然后对这些候选框进行分类和回归，two-stage方法的优势是准确率高；（2）one-stage方法是，如YOLO和SSD,其主要思路是均匀的在图片的不同位置进行密集的抽样，抽样是采用不同尺度和长宽比，然后利用CNN提取特征后直接进行分类和回归，整个过程只需要一步。</strong>速度明显提高。<br>　　本文主要介绍one-stage方法。<strong>以下是YOLO和SSD的对比</strong>：<br>　　YOLO能够达到实时的效果，但是缺点还是比较明显的:<br>　　　　1）一个网格只能够预测两个物体；<br>　　　　2）对物体的尺度比较敏感，对尺度变化较大的物体泛化能力较差；<br>　　　　3）难以检测小目标，而且定位不准。<br>　　<strong>针对YOLO的不足</strong>，<strong>SSD</strong>同时兼顾速度和mAP,两者不同点,换句话说也是<strong>改进点</strong>：<br>　　　　1）SSD采用CNN直接进行检测，然而YOLO在全连接层之后进行检测；<br>　　　　2）SSD提取的不同尺度的feature map来做检测，大尺度的特征图（靠前的特征图）用来检测比较小的物体，而小尺度的特征图用来检测大物体；<br>　　　　3）SSD采用了不同尺度和不同长宽比的先验框（Prior Box ，Default Box，在faster r-cnn中叫做锚，Anchor）。</p><h1 id="二、核心算法"><a href="#二、核心算法" class="headerlink" title="二、核心算法"></a>二、核心算法</h1><h2 id="2-1-多尺度的Feature-Map"><a href="#2-1-多尺度的Feature-Map" class="headerlink" title="2.1 多尺度的Feature Map"></a>2.1 多尺度的Feature Map</h2><p>　　作者同时采用低层和高层的不同尺度的feature map做检测。<br>　　所谓多尺度采用大小不同的特征图，CNN网络一般前面的特征图比较大，后面会逐渐采用stride=2的卷积或者pool来降低特征图大小，这正如图3所示，<strong>一个比较大的特征图和一个比较小的特征图，它们都用来做检测。</strong>这样做的好处是比较大的特征图来用来检测相对较小的目标，而小的特征图负责检测大目标，如图4所示，8x8的特征图可以划分更多的单元，但是其每个单元的先验框尺度比较小。</p><div align="center"><br> <img src="http://ow7va355d.bkt.clouddn.com/feature_map1.jpg" alt=""><br></div>  <h2 id="2-2-Default-box-prior-box"><a href="#2-2-Default-box-prior-box" class="headerlink" title="2.2 Default box (prior box)"></a>2.2 Default box (prior box)</h2><p>　　<strong>Default box</strong>，是指在feature map的每个小格(cell)上都有一系列固定大小的box，如下图有4个（下图中的虚线框，仔细看格子的中间有比格子还小的一个box）。<br>　　所以假设每个feature map包含:<br>　　1) mxn个cell;<br>　　2) 每个cell预测k个default box<br>　　3)每个default box有 c个类别+4个offset(坐标偏移量)<br>　　综上每个feature map的输出是：<strong>k·(c+4)·(mxn)</strong>。</p><div align="center"><br><img src="http://ow7va355d.bkt.clouddn.com/ssd1.jpg" alt=""><br></div> <p>　　<strong>训练中还有一个东西：prior box</strong>，是指实际中选择的default box（每一个feature map cell 不是k个default box都取）。<strong>也就是说default box是一种概念，prior box则是实际的选取</strong>。<br>　　训练中一张完整的图片送进网络获得各个feature map，对于<strong>正样本训练</strong>来说，需要先将prior box与ground truth box做匹配，匹配成功说明这个prior box所包含的是个目标，但离完整目标的ground truth box还有段距离，训练的目的是保证default box的分类confidence的同时将prior box尽可能回归到ground truth box。<br>　　<strong>举个列子：假设一个训练样本中有2个ground truth box，所有的feature map中获取的　prior box一共有8732个。那个可能分别有10、20个prior box能分别与这2个ground truth box匹配上。训练的损失包含定位损失和回归损失两部分。</strong><br>　　这里用到的 default box 和Faster RCNN中的 anchor 很像，在Faster RCNN中 anchor 只用在最后一个卷积层，但是在本文中，default box 是应用在多个不同层的feature map上。</p><h2 id="2-3多尺度feature-map-和default-box-图解"><a href="#2-3多尺度feature-map-和default-box-图解" class="headerlink" title="2.3多尺度feature map 和default box 图解"></a>2.3多尺度feature map 和default box 图解</h2><p> 　　<strong>多尺度 feature map 得到的 default box及其四个位置的偏移和21个类别的置信度</strong>对于不同尺度的fature map( 38x38x512  19x19x1024 10x10x512 5x5x256 3x3x256 1x1x256 )<br>　　<strong>以5x5x256,default box=6为例</strong></p><div align="center"><br><img src="http://ow7va355d.bkt.clouddn.com/ssd3.jpg" alt=""><br><br><img src="http://ow7va355d.bkt.clouddn.com/ssd5.jpg" alt=""><br></div><p>　　<strong>按照不同的 scale 和 ratio 生成，k 个 default boxes，这种结构有点类似于 Faster R-CNN 中的 Anchor。(此处k=6所以：5x5x3=45 boxes)</strong></p><div align="center"><br> <img src="http://ow7va355d.bkt.clouddn.com/ssd6.jpg" alt=""><br> <img src="http://ow7va355d.bkt.clouddn.com/ssd13.jpg" alt=""><br> <img src="http://ow7va355d.bkt.clouddn.com/ssd7.jpg" alt=""><br></div><h2 id="2-4-Default-box的scale和aspect-ratiodio定义方法"><a href="#2-4-Default-box的scale和aspect-ratiodio定义方法" class="headerlink" title="2.4 Default box的scale和aspect ratiodio定义方法"></a>2.4 Default box的scale和aspect ratiodio定义方法</h2><p>　　那么default box的scale（大小）和aspect ratio（横纵比）要怎么定呢？假设我们用m个feature maps做预测，那么对于每个featuer map而言其default box的scale是按以下公式计算的： </p><div align="center"><br><img src="http://ow7va355d.bkt.clouddn.com/ssd-defaultbox.png" alt=""><br></div><p>　　每一个 default box 的中心，设置为：(i+0.5/|fk|,j+0.5/|fk|)，其中，|fk| 是第 k 个 feature map 的大小，同时，i,j∈[0,|fk|)。<br>　　可以看出这种<strong>default box在不同的feature层有不同的scale</strong>，<strong>在同一个feature层又有不同的aspect ratio</strong>，因此基本上可以覆盖输入图像中的各种形状和大小的object！</p><h2 id="2-4-正负样本"><a href="#2-4-正负样本" class="headerlink" title="2.4 正负样本"></a>2.4 正负样本</h2><p>　　<br>　　将<strong>prior box </strong>和 <strong>grount truth box</strong> 按照IOU（JaccardOverlap）进行匹配，匹配成功则这个prior box就是positive example（正样本），如果匹配不上，就是negative example（负样本），显然这样产生的负样本的数量要远远多于正样本。这里将前向loss进行排序，选择最高的num_sel个prior box序号集合 D。那么如果Match成功后的正样本序号集合P。那么最后正样本集为 P−D∩P，负样本集为 D−D∩P。同时可以通过规范num_sel的数量（是正样本数量的三倍）来控制使得最后正、负样本的比例在 1：3 左右。<br>　　在结合 feature maps 上，所有不同尺度、不同 aspect ratios 的 default boxes，它们预测的 predictions 之后。可以想见，我们有许多个 predictions，包含了物体的不同尺寸、形状。如下图，狗狗的 ground truth box 与 4×4 feature map 中的红色 box 吻合，所以其余的 boxes 都看作负样本。 </p><div align="center"><br> <img src="http://ow7va355d.bkt.clouddn.com/ssd12.png" alt=""><br></div><p>　　在生成一系列的 predictions 之后，会产生很多个符合 ground truth box 的 predictions boxes，但同时，不符合 ground truth boxes 也很多，而且这个 negative boxes，远多于 positive boxes。这会造成 negative boxes、positive boxes 之间的不均衡。训练时难以收敛。<br>　　因此，本文采取，先将每一个物体位置上对应 predictions（default boxes）是 negative 的 boxes 进行排序，按照 default boxes 的 confidence 的大小。 选择最高的几个，保证最后 negatives、positives 的比例在 3:1。本文通过实验发现，这样的比例可以更快的优化，训练也更稳定。 </p><h2 id="2-5-Data-augmentation"><a href="#2-5-Data-augmentation" class="headerlink" title="2.5 Data augmentation"></a>2.5 Data augmentation</h2><p>　　本文同时对训练数据做了 data augmentation，数据增广。<br>　　每一张训练图像，随机的进行如下几种选择：<br>　　１）使用原始的图像<br>　　２）随机采样多个 patch(CropImage)，与物体之间最小的 jaccard overlap 为：0.1，0.3，0.5，0.7 与 0.9<br>　　采样的 patch 是原始图像大小比例是 [0.3，1.0]，aspect ratio 在 0.5 或 2。<br>　　当 groundtruth box 的 中心（center）在采样的 patch 中且在采样的 patch中 groundtruth box面积大于0时，我们保留CropImage。<br>　　在这些采样步骤之后，每一个采样的 patch 被 resize 到固定的大小，并且以 0.5 的概率随机的 水平翻转（horizontally flipped，翻转不翻转看prototxt，默认不翻转）<br>　　这样一个样本被诸多batch_sampler采样器采样后会生成多个候选样本，然后从中随机选一个样本送人网络训练。</p><h2 id="2-6-Loss-Function损失函数"><a href="#2-6-Loss-Function损失函数" class="headerlink" title="2.6 Loss Function损失函数"></a>2.6 Loss Function损失函数</h2><p>　　<strong>损失函数方面：</strong>和Faster RCNN的基本一样，由分类和回归两部分组成，可以参考Faster RCNN，这里不细讲。总之，回归部分的loss是希望预测的box和prior box的差距尽可能跟ground truth和prior box的差距接近，这样预测的box就能尽量和ground truth一样。</p><div align="center"><br> <img src="http://ow7va355d.bkt.clouddn.com/ssd8.png" alt=""><br></div><p>　　上面得到的8732个目标框经过Jaccard Overlap筛选剩下几个了；其中不满足的框标记为负数，其余留下的标为正数框。紧随其后：</p><div align="center"><br><img src="http://ow7va355d.bkt.clouddn.com/ssd9.png" alt=""><br></div><p>　　1. N 是与 ground truth box 相匹配的 prior boxes 个数;<br>　　2. localization loss（loc） 是 Fast R-CNN 中 Smooth L1 Loss，用在 predict box（l） 与 ground truth box（g） 参数（即中心坐标位置，width、height）中，回归 bounding boxes 的中心位置，<br>以及 width、height;<br>　　3. confidence loss（conf） 是 Softmax Loss，输入为每一类的置信度 c;<br>　　4. 权重项 α，可在protxt中设置 loc_weight，默认设置为1。</p><h1 id="三、网络结构"><a href="#三、网络结构" class="headerlink" title="三、网络结构"></a>三、网络结构</h1><h2 id="3-1整体网络结构"><a href="#3-1整体网络结构" class="headerlink" title="3.1整体网络结构"></a>3.1整体网络结构</h2><p>该论文的网络结构实在VGG16的基础上改进的.<br>如下图：<br>　　　1)使用5个conv stage;<br>　　　2)用了astrous算法将FC6、FC7两个卷积层转化成为两个卷积层；<br>　　　3)在额外增加3个卷积层和一个average pooling层；<br>　　　4)不同层次的feature map 分别用于default box的偏移和不同类别的得分预测；</p><p><strong>优点：</strong><br>　　1)增加的feature map的大小变化比较大，允许能够检测不同尺度下的物体。<br>　　2)低层的feature map感受野比较小,高层的感受野比较大，在不同的feature map下进行卷积，可以达到多尺度的目的。<br>　　3)相比YOLO后面存在两个全连接层，全连接层以后的输出都会观察到整幅图像，然而SSD去掉了全连接层,每个输出只会感受到目标周围的信息,包括上下文，并且用了不同的feature map 预测不同的长宽比。这样比YOLO增加了预测更多的box。</p><p>　　算法的主网络结构是VGG16，将最后两个全连接层改成卷积层，并随后增加了4个卷积层来构造网络结构。对其中5种不同的卷积层的输出（feature map）分别用两个不同的 3×3 的卷积核进行卷积，一个输出分类用的confidence，每个default box 生成21个类别confidence；一个输出回归用的 localization，每个 default box 生成4个坐标值（x, y, w, h）。此外，这5个feature map还经过 PriorBox 层生成 prior box（生成的是坐标）。上述5个feature map中每一层的default box的数量是给定的(8732个)。最后将前面三个计算结果分别合并然后传给loss层。</p><div align="center"><br><img src="http://ow7va355d.bkt.clouddn.com/ssd10.png" alt=""><br></div><p>更具体的框架如下：</p><div align="center"><br><img src="http://ow7va355d.bkt.clouddn.com/ssd11.png" alt=""><br></div><h1 id="四、参考文献"><a href="#四、参考文献" class="headerlink" title="四、参考文献"></a>四、参考文献</h1><ol><li><a href="https://zhuanlan.zhihu.com/p/24954433" target="_blank" rel="noopener">SSD</a></li><li><a href="https://blog.csdn.net/u010167269/article/details/52563573" target="_blank" rel="noopener">论文阅读：SSD: Single Shot MultiBox Detector</a></li><li><a href="https://blog.csdn.net/wfei101/article/details/78597442" target="_blank" rel="noopener">SSD详解Default box的解读</a></li><li><a href="https://www.cnblogs.com/xuanyuyt/p/7447111.html" target="_blank" rel="noopener">深度学习笔记（七）SSD论文阅读笔记</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;5&quot; color=&quot;lightseagreen&quot;&gt; 　　这篇文章是总结SSD算法，其英文的全名是Single Shot MultiBox Detector，Single Shot 代表了给算法属于one-stage方法，MultiBox指的是多边框预测，本文详细的介绍了SSD的工作原理，结构框架和与其他网络比如VGG,YOLO,R-CNN等网络对比。&lt;br&gt; &lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="http://www.douxiao.org/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="SSD" scheme="http://www.douxiao.org/tags/SSD/"/>
    
      <category term="目标检测" scheme="http://www.douxiao.org/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>深度学习——CNN卷积神经网络</title>
    <link href="http://www.douxiao.org/2018/05/10/CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E8%AF%BB/"/>
    <id>http://www.douxiao.org/2018/05/10/CNN卷积神经网络解读/</id>
    <published>2018-05-10T11:24:41.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<p><font size="5" color="lightseagreen">   卷积神经网络的生动总结，加深理解！<br> </font><br><a id="more"></a></p><h1 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h1><p>　　卷积神经网络的产生是为了<font color="red"><strong> 解决深层神经网络参数多训练难的问题，并获得更好的分类效果。</strong> </font>  深度学习出现之前，传统的神经网络层数变多时，反向传播将会变得非常困难，并且对一些高维的输入往往需要大量的参数，因此复杂的神经网络的训练变得非常困难。同时<strong>由于全连接网络是提取前一层的全部信息，整体的特征拟合能力较低，且易于拟合于局部样本。</strong></p><p>　　<strong>卷积神经网络的实现来源于对视觉神经系统的研究</strong>，科学家发现，<strong>人对事物的观察是通过对不同特征捕获的综合</strong>，视觉神经系统中有专门负责不同特征感知的视觉元。从对视觉神经系统的研究出发，神经网络的研究者提出用一个神经元每次观察输入的部分特征（比如图片的一小块），然后通过逐步移动的方法观察整个输入的方法，然后<strong>用多个这种神经元提取输入的输入的不同特征，最后在通过一个全连接网络对这些特征进行整合，最终达成分类效果</strong>。下图形象的描述了这种特征的提取。</p><div align="center"><br><img src="http://ow7va355d.bkt.clouddn.com/%E5%85%A8%E8%BF%9E%E6%8E%A5%E4%B8%8E%E7%A5%9E%E7%BB%8F%E5%85%83.jpg" alt=""><br></div><p>　　通过多个神经元代替一个全连接层，参数量可以大大的减少，且由于图像存在大量的相同的基本特征，所以卷积神经网络在计算机视觉领域取得了很好的效果。</p><h1 id="二、简介"><a href="#二、简介" class="headerlink" title="二、简介"></a>二、简介</h1><p><strong>卷积神经网络就是由卷积层、池化层、全连接层构成的具有局部感知和权值共享能力的深层神经网络</strong></p><div align="center"><br><img src="http://ow7va355d.bkt.clouddn.com/CCN1" alt=""><br></div><br> 卷积神经网络的特点：<br> 1）<strong>局部连接</strong> 。 可以提取数据的局部特征。<br> 2）<strong>权值共享</strong> 。 大大的降低了训练的难度。<br> 3）<strong>池化操作</strong> 。 数据降维的同时，保留有用的信息。<br> 4）<strong>全连接层</strong> 。 对提取的特征进行组织综合，输出识别物体的分类情况。<br><div align="center"><br><img src="http://ow7va355d.bkt.clouddn.com/CNN2.png" alt=""><br></div> <h1 id="三、重要概念"><a href="#三、重要概念" class="headerlink" title="三、重要概念"></a>三、重要概念</h1><h2 id="3-1卷积"><a href="#3-1卷积" class="headerlink" title="3.1卷积"></a>3.1卷积</h2><p>   卷积的本质就是加权叠加，最后通过滑动窗口（filter）观察整个输入，输出一个feature map。</p><h3 id="一维卷积："><a href="#一维卷积：" class="headerlink" title="一维卷积："></a>一维卷积：</h3><div align="center"><br><img src="http://img.my.csdn.net/uploads/201301/09/1357719312_6678.png" alt=""><br></div><br>   图中的M向量为卷积核，N向量为输入，P向量为输出。其中P[2] = N[0] <em> M[0] + … + N[4] </em> M[4]。<br><br>### 二维卷积：（图像）<br><div align="center"><br><img src="https://img-blog.csdn.net/20170503110450187?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzE0NTY1OTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""><br></div><p>卷积层中卷积核的使用，一般如上图所示。卷积层是卷积核在上一级输入层上通过逐一滑动窗口计算而得，卷积核中的每一个参数都相当于传统神经网络中的权值参数，与对应的局部像素相连接，将卷积核的各个参数与对应的局部像素值相乘之和，（通常还要再加上一个偏置参数），得到卷积层上的结果（即feature map）。</p><div align="center"><br><img src="https://img-blog.csdn.net/20160702215705128" alt=""><br></div><h3 id="多通道多卷积核"><a href="#多通道多卷积核" class="headerlink" title="多通道多卷积核"></a>多通道多卷积核</h3><div align="center"><br> <img src="https://img-blog.csdn.net/20160707204048899" alt=""><br></div><p>1) input: 7<em>7</em>3 RGB)<br>2) fiters: 3*3  strides: 2<br>3) output: 2 feature maps  hidden layers:2</p><p>说明： <strong>卷积核的权重矩阵就是训练时要学习的参数，就是要提取的特征，神经网络再根据卷积提取的特征去观察输入。</strong></p><div align="center"><br> <img src="http://upload-images.jianshu.io/upload_images/145616-cdf0a3911ba67b0f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br></div><h1 id="3-2激活函数"><a href="#3-2激活函数" class="headerlink" title="3.2激活函数"></a>3.2激活函数</h1><p>   卷积神经网络中，最常用的激活函数是relu : f(x)= max(0, x)</p><div align="center"><br><img src="http://upload-images.jianshu.io/upload_images/2256672-0ac9923bebd3c9dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640" alt=""><br></div><p><strong>Relu函数作为激活函数，有下面几大优势：</strong></p><p>1)  <strong>速度快 </strong>和sigmoid函数需要计算指数和倒数相比，relu函数其实就是一个max(0,x)，计算代价小很多。<br>2) <strong>减轻梯度消失问题</strong> 回顾计算梯度的公式∇=σ′δx。其中，σ′是sigmoid函数的导数。在使用反向传播算法进行梯度计算时，每经过一层sigmoid神经元，梯度就要乘上一个σ′。从下图可以看出，σ′函数最大值是1/4。因此，乘一个会导致梯度越来越小，这对于深层网络的训练是个很大的问题。而relu函数的导数是1，不会导致梯度变小。当然，激活函数仅仅是导致梯度减小的一个因素，但无论如何在这方面relu的表现强于sigmoid。使用relu激活函数可以让你训练更深的网络。<br>3) <strong>稀疏性</strong> 通过对大脑的研究发现，大脑在工作的时候只有大约5%的神经元是激活的，而采用sigmoid激活函数的人工神经网络，其激活率大约是50%。有论文声称人工神经网络在15%-30%的激活率时是比较理想的。因为relu函数在输入小于0时是完全不激活的，因此可以获得一个更低的激活率。</p><div align="center"><br><img src="http://upload-images.jianshu.io/upload_images/145616-2c9ee822f6daa1e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br></div><h1 id="3-3池化层"><a href="#3-3池化层" class="headerlink" title="3.3池化层"></a>3.3池化层</h1><div align="center"><br><img src="https://img-blog.csdn.net/20160703121026432" alt=""><br></div><br>池化层里我们用的maxpooling，将主要特征保留，舍去多余无用特征,这样就可以实现信息压缩，比如下图所示<br><div align="center"><br><img src="http://upload-images.jianshu.io/upload_images/145616-88c19d8616068d06.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br></div><p>　　经过多轮的卷积和池化，将得到最终的特征表示</p><div align="center"><br><img src="http://upload-images.jianshu.io/upload_images/145616-25c57dafffd49ce8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br></div><h1 id="3-4-FC层"><a href="#3-4-FC层" class="headerlink" title="3.4 FC层"></a>3.4 FC层</h1><p>　　全连接层(Fully connected layers ,FC)在整个卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数等操作将原始数据映射到隐层特征空间的话，全连接层则起到将<strong>分布式特征表示</strong>映射到样本标记空间的作用。<br>　　<strong>在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽（注1）。</strong><br>　　++目前由于全连接层参数冗余（仅全连接层参数就可占整个网络参数80%左右），近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。++需要指出的是，用GAP替代FC的网络通常有较好的预测性能。</p><p>　　<strong>注1: </strong>有关卷积操作“实现”全连接层，有必要多啰嗦几句。以VGG-16为例，对224x224x3的输入，最后一层卷积可得输出为7x7x512，如后层是一层含4096个神经元的FC，则可用卷积核为7x7x512x4096的全局卷积来实现这一全连接运算过程，其中该卷积核参数如下：“filter size = 7, padding = 0, stride = 1, D_in = 512, D_out = 4096”经过此卷积操作后可得输出为1x1x4096。如需再次叠加一个2048的FC，则可设定参数为“filter size = 1, padding = 0, stride = 1, D_in = 4096, D_out = 2048”的卷积层操作。</p><p>　　最后用全连接对特征进行拟合：</p><div align="center"><br> <img src="http://upload-images.jianshu.io/upload_images/145616-e2e9a7c59444b138.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br></div><p>　　并输出不同分类的概率</p><div align="center"><br>  <img src="http://upload-images.jianshu.io/upload_images/145616-e507135cb66e02b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""><br></div><p>　　<strong>问题：</strong>怎样把一个3<em>3</em>5的输出，转化成1*4096形式？</p><div align="center"><br>  <img src="https://pic3.zhimg.com/80/v2-4d1ed82851e96dd58620f451d8c1e98e_hd.jpg" alt=""><br><br></div><p>　　可以理解为在中间做一个卷积：</p><div align="center"><br>  <img src="https://pic4.zhimg.com/80/v2-677c85adc52245a0c3bd6c766e057da8_hd.jpg" alt=""><br></div><p>　　从上图我们可以看出，我们用一个3x3x5的filter 去卷积激活函数的输出，得到的结果就是一个fully connected layer 的一个神经元的输出，这个输出就是一个值因为我们有4096个神经元我们实际就是用一个3x3x5x4096的卷积层去卷积激活函数的输出。</p><h1 id="3-5-卷积神经网络的可视化"><a href="#3-5-卷积神经网络的可视化" class="headerlink" title="3.5 卷积神经网络的可视化"></a>3.5 卷积神经网络的可视化</h1><div align="center"><br> <img src="https://img-blog.csdn.net/20160702205047459" alt=""><br></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;font size=&quot;5&quot; color=&quot;lightseagreen&quot;&gt;   卷积神经网络的生动总结，加深理解！&lt;br&gt; &lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="工作" scheme="http://www.douxiao.org/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="CNN" scheme="http://www.douxiao.org/tags/CNN/"/>
    
      <category term="DeepLearning" scheme="http://www.douxiao.org/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>用pyqt5构建人脸识别界面</title>
    <link href="http://www.douxiao.org/2018/03/12/%E7%94%A8pyqt5%E6%9E%84%E5%BB%BA%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%95%8C%E9%9D%A2/"/>
    <id>http://www.douxiao.org/2018/03/12/用pyqt5构建人脸识别界面/</id>
    <published>2018-03-12T01:00:38.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<font size="5" color="lightseagreen">  这篇博客是总结之前用Pyqt5、dlib、opencv实现的一个实时人脸识别同时具有人脸录入功能的软件，后续还会增加基于深度学习的人脸识别功能。</font><a id="more"></a><h1 id="一、效果视频"><a href="#一、效果视频" class="headerlink" title="一、效果视频"></a>一、效果视频</h1><div style="max-width:960px; margin:0 auto 10px;"><br><div style="position: relative;width:100%;padding-bottom:56.25%;height:0;"><br><iframe style="position: absolute;top: 0;left: 0;width: 100%;height: 100%;" src="http://player.youku.com/embed/XMzQ2NDg1NDIxNg==" frameborder="0" allowfullscreen></iframe><br></div><br></div><h1 id="二、源码解析"><a href="#二、源码解析" class="headerlink" title="二、源码解析"></a>二、源码解析</h1><h2 id="1、UI布局"><a href="#1、UI布局" class="headerlink" title="1、UI布局"></a>1、UI布局</h2><p>　　该软件界面只是使用了简单的UI界面，包括按键QPushButton、显示视屏是用的QLabel不断刷新图片来显示视频，录入姓名是用的QTextEdit。<br>  <a href="https://zhuanlan.zhihu.com/xdbcb8" target="_blank" rel="noopener">参考PyQT教程：</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_ui</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment"># 布局设置</span></span><br><span class="line">    self.layout_main = QHBoxLayout()  <span class="comment"># 整体框架是水平布局</span></span><br><span class="line">    self.layout_button = QVBoxLayout()  <span class="comment"># 按键布局是垂直布局</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按钮设置</span></span><br><span class="line">    self.btn_open_cam = QPushButton(<span class="string">'打开相机'</span>)</span><br><span class="line">    self.btn_photo = QPushButton(<span class="string">'拍照'</span>)</span><br><span class="line">    self.btn_input_name = QPushButton(<span class="string">'录入人名'</span>)</span><br><span class="line">    self.btn_detection_face = QPushButton(<span class="string">'人脸检测'</span>)</span><br><span class="line">    self.btn_recognition_face = QPushButton(<span class="string">'人脸识别'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    self.btn_quit = QPushButton(<span class="string">'退出'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 显示视频</span></span><br><span class="line">    self.label_show_camera = QLabel()</span><br><span class="line">    self.label_move = QLabel()</span><br><span class="line">    self.label_move.setFixedSize(<span class="number">100</span>, <span class="number">200</span>)</span><br><span class="line">    <span class="comment"># 显示文本框</span></span><br><span class="line">    self.text = QTextEdit(self)</span><br><span class="line"></span><br><span class="line">    self.label_show_camera.setFixedSize(<span class="number">800</span>, <span class="number">600</span>)</span><br><span class="line">    self.label_show_camera.setAutoFillBackground(<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 布局</span></span><br><span class="line">    self.layout_button.addWidget(self.btn_open_cam)</span><br><span class="line">    self.layout_button.addWidget(self.btn_photo)</span><br><span class="line">    self.layout_button.addWidget(self.btn_input_name)</span><br><span class="line">    self.layout_button.addWidget(self.btn_detection_face)</span><br><span class="line">    self.layout_button.addWidget(self.btn_recognition_face)</span><br><span class="line">    self.layout_button.addWidget(self.btn_quit)</span><br><span class="line">    self.layout_button.addWidget(self.label_move)</span><br><span class="line">    self.layout_button.addWidget(self.text)</span><br><span class="line"></span><br><span class="line">    self.layout_main.addLayout(self.layout_button)</span><br><span class="line">    self.layout_main.addWidget(self.label_show_camera)</span><br><span class="line"></span><br><span class="line">    self.setLayout(self.layout_main)</span><br><span class="line">    self.setGeometry(<span class="number">300</span>, <span class="number">300</span>, <span class="number">600</span>, <span class="number">400</span>)</span><br><span class="line">    self.setWindowTitle(<span class="string">"人脸识别软件"</span>)</span><br></pre></td></tr></table></figure></p><h2 id="2、按键函数介绍"><a href="#2、按键函数介绍" class="headerlink" title="2、按键函数介绍"></a>2、按键函数介绍</h2><p>　　<font size="5" color="red"><strong>注意：不同按键切换前需要关闭当前功能。否则操作会出现错误。</strong></font></p><h3 id="1-def-btn-open-cam-click-self"><a href="#1-def-btn-open-cam-click-self" class="headerlink" title="1) def btn_open_cam_click(self)"></a>1) def btn_open_cam_click(self)</h3><p>　　打开摄像头函数 btn_open_cam_click(self)，该函数主要是用来打开和关闭摄像头，具体是开启摄像头界面刷新定时器timer_camera.start(30),30ms出发一次信号槽， <code>self.timer_camera.timeout.connect(self.show_camera)</code>同时检查摄像头是否正确连接摄像头，如果没有会弹出一个消息提示框。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">btn_open_cam_click</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.timer_camera.isActive() == <span class="keyword">False</span>:</span><br><span class="line">        flag = self.cap.open(self.cap_num)</span><br><span class="line">        <span class="keyword">if</span> flag == <span class="keyword">False</span>:</span><br><span class="line">            msg = QMessageBox.warning(self, <span class="string">u"Warning"</span>, <span class="string">u"请检测相机与电脑是否连接正确"</span>, buttons=QMessageBox.Ok,</span><br><span class="line">                                      defaultButton=QMessageBox.Ok)</span><br><span class="line">        <span class="comment"># if msg==QtGui.QMessageBox.Cancel:</span></span><br><span class="line">        <span class="comment">#                     pass</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.timer_camera.start(<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">            self.btn_open_cam.setText(<span class="string">u'关闭相机'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.timer_camera.stop()</span><br><span class="line">        self.cap.release()</span><br><span class="line">        self.label_show_camera.clear()</span><br><span class="line">        self.btn_open_cam.setText(<span class="string">u'打开相机'</span>)</span><br></pre></td></tr></table></figure><h3 id="2-def-show-camera-self"><a href="#2-def-show-camera-self" class="headerlink" title="2) def show_camera(self):"></a>2) def show_camera(self):</h3><p>　　摄像头显示函数，该函数也相当于主函数了，这里有个很重要的变量<code>self.btn_flag</code>该变量用来切换按钮不同的模式，有打开、关闭摄像头、人脸检测、人脸识别等功能。<br>　　第一是加载人脸级联分类器haarcascade_frontalface_default.xml，该分类器是检测人脸用的分类器。其实该分类器在人脸识别中并未用到。只是在人脸检测中用到。<br>　　第二是需要用将图片转化才能在QT界面中显示，<code>self.showImage = QImage(show.data, show.shape[1], show.shape[0], QImage.Format_RGB888)</code><br>　　<font color="red">第三是人脸识别功能要开启线程，这在同时识别多个人时特别关键，特别将是检测不同人脸，并标定不同标签来识别人，放在了不同的线程中。</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_camera</span><span class="params">(self)</span>:</span></span><br><span class="line">    harr_filepath = cv2.data.haarcascades + <span class="string">"haarcascade_frontalface_default.xml"</span>  <span class="comment"># 系统安装的是opencv-contrib-python</span></span><br><span class="line">    classifier = cv2.CascadeClassifier(harr_filepath)  <span class="comment"># 加载人脸特征分类器</span></span><br><span class="line">    <span class="keyword">if</span> self.btn_flag == <span class="number">0</span>:</span><br><span class="line">        ret, self.image = self.cap.read()</span><br><span class="line">        show = cv2.resize(self.image, (<span class="number">800</span>, <span class="number">600</span>))</span><br><span class="line">        show = cv2.cvtColor(show, cv2.COLOR_BGR2RGB)  <span class="comment"># 这里指的是显示原图</span></span><br><span class="line">        <span class="comment"># opencv 读取图片的样式，不能通过Qlabel进行显示，需要转换为Qimage QImage(uchar * data, int width,</span></span><br><span class="line">        <span class="comment"># int height, Format format, QImageCleanupFunction cleanupFunction = 0, void *cleanupInfo = 0)</span></span><br><span class="line">        self.showImage = QImage(show.data, show.shape[<span class="number">1</span>], show.shape[<span class="number">0</span>], QImage.Format_RGB888)</span><br><span class="line">        self.label_show_camera.setPixmap(QPixmap.fromImage(self.showImage))</span><br><span class="line">    <span class="keyword">elif</span> self.btn_flag == <span class="number">1</span>:  <span class="comment"># 人脸检测</span></span><br><span class="line">        ret_1, self.image_1 = self.cap.read()</span><br><span class="line">        show_0 = cv2.resize(self.image_1, (<span class="number">800</span>, <span class="number">600</span>))</span><br><span class="line">        show_1 = cv2.cvtColor(show_0, cv2.COLOR_BGR2RGB)</span><br><span class="line">        gray_image = cv2.cvtColor(show_0, cv2.COLOR_BGR2GRAY)</span><br><span class="line">        faces = classifier.detectMultiScale(gray_image, <span class="number">1.3</span>, <span class="number">5</span>)  <span class="comment"># 1.3和5是特征的最小、最大检测窗口，它改变检测结果也会改变</span></span><br><span class="line">        <span class="keyword">for</span> (x, y, w, h) <span class="keyword">in</span> faces:</span><br><span class="line">            cv2.rectangle(show_1, (x, y), (x + w, y + h), (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">2</span>)  <span class="comment"># 画出人脸</span></span><br><span class="line">        detect_image = QImage(show_1.data, show_1.shape[<span class="number">1</span>], show_1.shape[<span class="number">0</span>],</span><br><span class="line">                              QImage.Format_RGB888)</span><br><span class="line">        self.label_show_camera.setPixmap(QPixmap.fromImage(detect_image))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> self.btn_flag == <span class="number">2</span>:  <span class="comment"># 人脸识别</span></span><br><span class="line">        ret_2, self.image_2 = self.cap.read()</span><br><span class="line">        show_2 = cv2.resize(self.image_2, (<span class="number">800</span>, <span class="number">600</span>))</span><br><span class="line">        self.show_3 = cv2.cvtColor(show_2, cv2.COLOR_BGR2RGB)</span><br><span class="line">        gray_image = cv2.cvtColor(show_2, cv2.COLOR_BGR2GRAY)</span><br><span class="line">        self.dets = self.detector(gray_image, <span class="number">1</span>)  <span class="comment"># 对视频中的人脸进行标定</span></span><br><span class="line">        self.dist = []  <span class="comment"># 声明一个数组</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> len(self.dets):</span><br><span class="line">            <span class="comment"># print('Can`t get face.')</span></span><br><span class="line">            detect_image = QImage(self.show_3.data, self.show_3.shape[<span class="number">1</span>], self.show_3.shape[<span class="number">0</span>],</span><br><span class="line">                                  QImage.Format_RGB888)</span><br><span class="line">            self.label_show_camera.setPixmap(QPixmap.fromImage(detect_image))</span><br><span class="line">        <span class="comment"># 这里开启了一个人脸识别的线程，会自动为for循环分配线程，这里为进一步在同一个视频中，识别不同的人脸准备</span></span><br><span class="line">        t = threading.Thread(target=self.face_thread, name=<span class="string">'Face_Thread'</span>)</span><br><span class="line">        t.start()</span><br><span class="line">        t.join()</span><br></pre></td></tr></table></figure><h3 id="3-def-face-thread-self"><a href="#3-def-face-thread-self" class="headerlink" title="3) def face_thread(self):"></a>3) def face_thread(self):</h3><p>　　线程函数中特别重要是一点是，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算欧式距离</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> self.descriptors:  <span class="comment"># 遍历之前提取的候选人的128D向量</span></span><br><span class="line">     <span class="comment"># 计算欧式距离，有多少个候选人就有多少个距离,放到dist数组中。</span></span><br><span class="line">     dist_ = np.linalg.norm(i - d_test)</span><br><span class="line">     self.dist.append(dist_)</span><br><span class="line">     <span class="comment"># 候选人和距离组成一个dict字典</span></span><br><span class="line">     c_d = dict(zip(self.candidate, self.dist))</span><br><span class="line"><span class="comment">#注意这里必须把dist[]之前的数据pop出去，才能确保每次不同线程计算的不同人的向量。从而不同线程识别不同的人。</span></span><br><span class="line"><span class="comment">#因为pop()只是把list中的最后一个pop出去，所以需要一个循环。</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(self.candidate)):</span><br><span class="line">    self.dist.pop()</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">face_thread</span><span class="params">(self)</span>:</span>  <span class="comment"># 这里多线程是解决，同时识别不同人脸的关键</span></span><br><span class="line"></span><br><span class="line">       print(<span class="string">'thread %s is running...'</span> % threading.current_thread().name)</span><br><span class="line">       <span class="keyword">for</span> k, face <span class="keyword">in</span> enumerate(self.dets):  <span class="comment"># 遍历视频中所有人脸 ，</span></span><br><span class="line"></span><br><span class="line">           print(<span class="string">'thread %s &gt;&gt;&gt; %s'</span> % (threading.current_thread().name, k))</span><br><span class="line">           shape = self.landmark(self.show_3, face)  <span class="comment"># 检测人脸特征点</span></span><br><span class="line">           face_descriptor = self.facerec.compute_face_descriptor(self.show_3, shape)  <span class="comment"># 计算人脸的128D向量</span></span><br><span class="line">           d_test = np.array(face_descriptor)</span><br><span class="line">           x1 = face.top() <span class="keyword">if</span> face.top() &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">           y1 = face.bottom() <span class="keyword">if</span> face.bottom() &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">           x2 = face.left() <span class="keyword">if</span> face.left() &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">           y2 = face.right() <span class="keyword">if</span> face.right() &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">           cv2.rectangle(self.show_3, (x2, x1), (y2, y1), (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>), <span class="number">3</span>)</span><br><span class="line">           <span class="comment"># print(x2, x1, y2, y1)</span></span><br><span class="line">           <span class="comment"># 计算欧式距离</span></span><br><span class="line">           <span class="keyword">for</span> i <span class="keyword">in</span> self.descriptors:  <span class="comment"># 遍历之前提取的候选人的128D向量</span></span><br><span class="line">               dist_ = np.linalg.norm(i - d_test)  <span class="comment"># 计算欧式距离，有多少个候选人就有多少个距离,放到dist数组中。</span></span><br><span class="line">               self.dist.append(dist_)</span><br><span class="line">               <span class="comment"># 候选人和距离组成一个dict字典</span></span><br><span class="line">           c_d = dict(zip(self.candidate, self.dist))</span><br><span class="line">           <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(self.candidate)):  <span class="comment"># 注意这里必须把dist[]之前的数据pop出去，才能确保每次不同线程计算的不同人的向量。</span></span><br><span class="line">               self.dist.pop()</span><br><span class="line">           <span class="comment"># sorted将dict字典中数排序，按key顺序（第二个关键字）</span></span><br><span class="line">           cd_sorted = sorted(c_d.items(), key=<span class="keyword">lambda</span> d: d[<span class="number">1</span>])</span><br><span class="line">           <span class="keyword">if</span> cd_sorted[<span class="number">0</span>][<span class="number">1</span>] &gt; <span class="number">0.32</span>:</span><br><span class="line">               cv2.putText(self.show_3, <span class="string">'Unknown'</span>, (x2, x1), cv2.FONT_HERSHEY_SIMPLEX, <span class="number">1.0</span>,</span><br><span class="line">                           (<span class="number">0</span>, <span class="number">255</span>, <span class="number">255</span>), <span class="number">2</span>)</span><br><span class="line">           <span class="keyword">else</span>:</span><br><span class="line">               cv2.putText(self.show_3, cd_sorted[<span class="number">0</span>][<span class="number">0</span>], (x2, x1), cv2.FONT_HERSHEY_SIMPLEX, <span class="number">1.0</span>,</span><br><span class="line">                       (<span class="number">0</span>, <span class="number">255</span>, <span class="number">255</span>), <span class="number">2</span>)</span><br><span class="line">           <span class="comment"># 各参数依次是：照片/添加的文字/左上角坐标/字体/字体大小/颜色/字体粗细</span></span><br><span class="line">           print(<span class="string">"\n The person is: "</span>, cd_sorted[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">           detect_image = QImage(self.show_3.data, self.show_3.shape[<span class="number">1</span>], self.show_3.shape[<span class="number">0</span>],</span><br><span class="line">                                 QImage.Format_RGB888)</span><br><span class="line">           self.label_show_camera.setPixmap(QPixmap.fromImage(detect_image))</span><br></pre></td></tr></table></figure><h3 id="4-def-photo-face-self"><a href="#4-def-photo-face-self" class="headerlink" title="4) def photo_face(self):"></a>4) def photo_face(self):</h3><p>　 拍照函数：以系统当前时间命名。使用的是<code>datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">photo_face</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># photo_save_path = '/home/dx/Desktop/detect_face_pyqt5/candidate-faces'</span></span><br><span class="line"></span><br><span class="line">    photo_save_path = os.path.join(os.path.dirname(os.path.abspath(<span class="string">'__file__'</span>)),</span><br><span class="line">                                   <span class="string">'candidate-faces/'</span>)</span><br><span class="line">    <span class="comment"># self.time_flag.append(datetime.now().strftime("%Y%m%d%H%M%S"))</span></span><br><span class="line">    self.showImage.save(photo_save_path + datetime.now().strftime(<span class="string">"%Y%m%d%H%M%S"</span>) + <span class="string">".jpg"</span>)</span><br><span class="line">    QMessageBox.information(self, <span class="string">"Information"</span>,</span><br><span class="line">                            self.tr(<span class="string">"拍照成功!"</span>))</span><br></pre></td></tr></table></figure></p><h3 id="5-def-input-name-self"><a href="#5-def-input-name-self" class="headerlink" title="5) def input_name(self):"></a>5) def input_name(self):</h3><p>　　实现功能是录入人名，保存人名到name.txt文件<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_name</span><span class="params">(self)</span>:</span></span><br><span class="line">    name_path = os.path.join(os.path.dirname(os.path.abspath(<span class="string">'__file__'</span>)),  <span class="comment"># 将人名文件放在candidate-faces文件夹下</span></span><br><span class="line">                             <span class="string">'candidate-faces/'</span>)</span><br><span class="line">    file_name_path = os.path.join(self.faces_folder_path, <span class="string">'name.txt'</span>)</span><br><span class="line">    <span class="keyword">if</span> self.input_flag == <span class="number">0</span>:</span><br><span class="line">        self.input_flag = <span class="number">1</span></span><br><span class="line">        self.fname = QFileDialog.getOpenFileName(self, <span class="string">'打开文件'</span>, name_path, <span class="string">"Text Files(*.txt)"</span>)</span><br><span class="line">        <span class="keyword">if</span> self.fname[<span class="number">0</span>]:  <span class="comment"># self.fname[0] 是/home/dx/Desktop/detect_face_pyqt5/candidate-faces/name.txt</span></span><br><span class="line">            <span class="keyword">with</span> open(self.fname[<span class="number">0</span>], <span class="string">'r'</span>, encoding=<span class="string">'gb18030'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                self.text.setText(f.read())</span><br><span class="line">        self.btn_input_name.setText(<span class="string">'保存人名'</span>)</span><br><span class="line">    <span class="keyword">elif</span> self.input_flag == <span class="number">1</span>:</span><br><span class="line">        self.input_flag = <span class="number">0</span></span><br><span class="line">        self.cont = self.text.toPlainText()  <span class="comment"># 获取文本框内容</span></span><br><span class="line">        f = open(file_name_path, <span class="string">'w'</span>)</span><br><span class="line">        f.write(self.cont)</span><br><span class="line">        <span class="comment">#print(self.cont)</span></span><br><span class="line">        self.btn_input_name.setText(<span class="string">'录入人名'</span>)</span><br></pre></td></tr></table></figure></p><h3 id="6-dlib-para-init-self-def-readfile-self"><a href="#6-dlib-para-init-self-def-readfile-self" class="headerlink" title="6) dlib_para_init(self): def readfile(self):"></a>6) dlib_para_init(self): def readfile(self):</h3><p>　<br>　　该函数作用是加载，人脸特征提取器，人脸识别分类，同时加载候选人文件夹，从name.txt加载候选人信息，并按照时间顺序排序，这里很关键。这样才能把照片和人名有序的联系起来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 人脸关键点检测器</span></span><br><span class="line">        <span class="comment">#  face_landmark_path    = '/home/dx/Desktop/detect_face_pyqt5/shape_predictor_68_face_landmarks.dat'</span></span><br><span class="line">        face_landmark_path = os.path.join(os.path.dirname(os.path.abspath(<span class="string">'__file__'</span>)),</span><br><span class="line">                                          <span class="string">'shape_predictor_68_face_landmarks.dat'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 人脸识别模型</span></span><br><span class="line">        <span class="comment">#  face_recognize_path   = '/home/dx/Desktop/detect_face_pyqt5/dlib_face_recognition_resnet_model_v1.dat'</span></span><br><span class="line">        face_recognize_path = os.path.join(os.path.dirname(os.path.abspath(<span class="string">'__file__'</span>)),</span><br><span class="line">                                           <span class="string">'dlib_face_recognition_resnet_model_v1.dat'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 候选人文件夹</span></span><br><span class="line">        <span class="comment">#  faces_folder_path     = '/home/dx/Desktop/detect_face_pyqt5/candidate-faces'</span></span><br><span class="line">        self.faces_folder_path = os.path.join(os.path.dirname(os.path.abspath(<span class="string">'__file__'</span>)),</span><br><span class="line">                                         <span class="string">'candidate-faces/'</span>)</span><br><span class="line">        <span class="comment"># 1.加载正脸检测器</span></span><br><span class="line">        self.detector = dlib.get_frontal_face_detector()</span><br><span class="line">        <span class="comment"># 2.加载人脸关键点检测器</span></span><br><span class="line">        self.landmark = dlib.shape_predictor(face_landmark_path)</span><br><span class="line">        <span class="comment"># 3. 加载人脸识别模型</span></span><br><span class="line">        self.facerec = dlib.face_recognition_model_v1(face_recognize_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 候选人脸描述子list</span></span><br><span class="line">        self.descriptors = []</span><br><span class="line">        <span class="comment"># 对文件夹下的每一个人脸进行:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1.人脸检测</span></span><br><span class="line">        <span class="comment"># 2.关键点检测</span></span><br><span class="line">        <span class="comment"># 3.描述子提取</span></span><br><span class="line">        time_flag = []  <span class="comment"># 获取照片中时间</span></span><br><span class="line">        file_glob = os.path.join(self.faces_folder_path, <span class="string">"*.jpg"</span>)</span><br><span class="line">        file_list = []</span><br><span class="line">        file_list.extend(glob.glob(file_glob))</span><br><span class="line">        print(file_list)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(file_list)):</span><br><span class="line">            tmp = str(file_list[i])</span><br><span class="line">            tmp_1 = tmp[<span class="number">51</span>:<span class="number">65</span>]  <span class="comment"># 截取字符串,截取时间</span></span><br><span class="line">            time_flag.append(tmp_1)</span><br><span class="line">        <span class="comment"># print(time_flag)</span></span><br><span class="line">        cand_d = dict(zip(file_list, time_flag))</span><br><span class="line">        cand_sorted = sorted(cand_d.items(), key=<span class="keyword">lambda</span> d: d[<span class="number">1</span>])  <span class="comment"># 按字典的第二个关键字排序</span></span><br><span class="line">        <span class="comment"># print(cand_sorted)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> range(<span class="number">0</span>, len(cand_sorted)):</span><br><span class="line"></span><br><span class="line">            print(<span class="string">"Processing file: &#123;&#125;"</span>.format(cand_sorted[f][<span class="number">0</span>]))</span><br><span class="line">            self.img = io.imread(cand_sorted[f][<span class="number">0</span>])</span><br><span class="line">            <span class="comment"># win.clear_overlay()</span></span><br><span class="line">            <span class="comment"># win.set_image(img)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 1.人脸检测</span></span><br><span class="line">            dets = self.detector(self.img, <span class="number">1</span>)  <span class="comment"># 人脸标定</span></span><br><span class="line">            print(<span class="string">"Number of faces detected: &#123;&#125;"</span>.format(len(dets)))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> k, d <span class="keyword">in</span> enumerate(dets):</span><br><span class="line">                <span class="comment"># 2.关键点检测</span></span><br><span class="line">                shape = self.landmark(self.img, d)</span><br><span class="line">                <span class="comment"># 画出人脸区域和和关键点</span></span><br><span class="line">                <span class="comment"># win.clear_overlay()</span></span><br><span class="line">                <span class="comment"># win.add_overlay(d)</span></span><br><span class="line">                <span class="comment"># win.add_overlay(shape)</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 3.描述子提取，128D向量</span></span><br><span class="line">                face_descriptor = self.facerec.compute_face_descriptor(self.img, shape)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 转换为numpy array</span></span><br><span class="line">                v = np.array(face_descriptor)</span><br><span class="line">                self.descriptors.append(v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对需识别人脸进行同样处理</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取描述子，不再注释</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 候选人名单</span></span><br><span class="line"></span><br><span class="line">       <span class="comment"># self.candidate = ['dwh', 'whr', 'zjr', 'dx', 'dx', 'whr', 'dx']</span></span><br><span class="line">        self.candidate = self.readfile()  <span class="comment"># 读取候选人,从name.txt中</span></span><br><span class="line">        print(self.candidate)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">readfile</span><span class="params">(self)</span>:</span></span><br><span class="line">        file_name_path = os.path.join(self.faces_folder_path, <span class="string">'name.txt'</span>)</span><br><span class="line">        <span class="keyword">with</span> open(file_name_path, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            content = f.read().splitlines()  <span class="comment"># 去掉行尾的换行符\n 并保存为list</span></span><br><span class="line">            <span class="comment">#content = [line.rstrip('\n') for line in f]</span></span><br><span class="line">            <span class="keyword">return</span> content</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;font size=&quot;5&quot; color=&quot;lightseagreen&quot;&gt;  这篇博客是总结之前用Pyqt5、dlib、opencv实现的一个实时人脸识别同时具有人脸录入功能的软件，后续还会增加基于深度学习的人脸识别功能。&lt;/font&gt;
    
    </summary>
    
      <category term="工作" scheme="http://www.douxiao.org/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="pyqt5" scheme="http://www.douxiao.org/tags/pyqt5/"/>
    
      <category term="dlib" scheme="http://www.douxiao.org/tags/dlib/"/>
    
  </entry>
  
  <entry>
    <title>TCP和UDP之间的区别</title>
    <link href="http://www.douxiao.org/2018/02/26/TCP%E5%92%8CUDP%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://www.douxiao.org/2018/02/26/TCP和UDP之间的区别/</id>
    <published>2018-02-26T01:06:43.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[  <font size="5" color="lightseagreen"> 新年的第一篇博客，新的一年继续加油努力，这这一年里一定要遇到更好的自己，这篇博客是就自己做的一个APP中用到了socket,总结一下TCP和UDP通信。</font><br><a id="more"></a><br><br><br>TCP和UDP之间的区别<br><br>　　1）　TCP/IP和UDP最大的区别:<font color="red"> TCP/IP是面向连接的，UDP是非面向连接的。通俗一点说就是:TCP/IP管发管到，UDP管发不管到。</font><br>　　2）　TCP/IP偏重的是点对点的通信，使用时服务器server和client的区别显而易见，而UDP可以使用组播实现一对多，通信更像一个群聊系统，几乎没有客户端和服务器的区别。<br>　　3）　在具体的编写结构上，<font color="forestgreen">TCP/IP是通过创建socket对象进行链接，从连接对象上得到输入输出流，在流中读写从而通信。而UDP则是在本机创建DatagramSocket对象，将数据包装成DatagramPacket包来发送，其中的包里有要发送的IP地址和端口port。</font><p>　　TCP（Transmission Control Protocol，传输控制协议）是基于连接的协议，也就是说，在正式收发数据前，必须和对方建立可靠的连接。一个TCP连接必须要经过<strong>三次“对话”才能建立起来</strong>，其中的过程非常复杂，我们这里只做简单、形象的介绍，你只要做到能够理解这个过程即可。我们来看看这三次对话的简单过程:<font color="purple">主机A向主机B发出连接请求数据包:“我想给你发数据，可以吗？”，这是第一次对话；主机B向主机A发送同意连接和要求同步（同步就是两台主机一个在发送，一个在接收，协调工作）的数据包:“可以，你什么时候发？”，这是第二次对话；主机A再发出一个数据包确认主机B的要求同步:“我现在就发，你接着吧！”，这是第三次对话。<strong>三次“对话”的目的是使数据包的发送和接收同步，经过三次“对话”之后，主机A才向主机B正式发送数据</strong>。</font></p><p>　　TCP协议能为应用程序提供可靠的通信连接，使一台计算机发出的字节流无差错地发往网络上的其他计算机，对可靠性要求高的数据通信系统往往使用TCP协议传输数据。<br>　　面向非连接:就是在正式通信前不必与对方先建立连接，不管对方状态就直接发送。这与现在手机短信非常相似:你在发短信的时候，只需要输入对方手机号就OK了。<br>　　<font color="red">UDP（User Data Protocol，用户数据报协议）是与TCP相对应的协议。它是面向非连接的协议，它不与对方建立连接，而是直接就把数据包发送过去！</font><br>　　比如，我们经常使用“ping”命令来测试两台主机之间TCP/IP通信是否正常，其实 <strong>“ping”命令的原理就是向对方主机发送UDP数据包</strong>，然后对方主机确认收到数据包，如果数据包是否到达的消息及时反馈回来，那么网络就是通的。例如，在默认状态下，一次“ping”操作发送4个数据包（如图2所示）。大家可以看到，发送的数据包数量是4包，收到的也是4包（因为对方主机收到后会发回一个确认收到的数据包）。这充分说明了UDP协议是面向非连接的协议，没有建立连接的过程。正因为UDP协议没有连接的过程，所以它的通信效果高；但也正因为如此，它的可靠性不如TCP协议高。QQ就使用UDP发消息，因此有时会出现收不到消息的情况。<br>　　TCP协议和UDP协议各有所长、各有所短，适用于不同要求的通信环境。TCP协议和UDP协议之间的差别如附表所示。</p><table><br><thead><br><tr><br><th>　　　　　　　</th><br><th>TCP</th><br><th>　　　UDP　　</th><br></tr><br></thead><br><tbody><br><tr><br><td>是否连接</td><br><td>面向连接</td><br><td>面向非连接　</td><br></tr><br><tr><br><td>传输可靠性　</td><br><td>可靠　　　　</td><br><td>不可靠　　</td><br></tr><br><tr><br><td>应用场合　　　　</td><br><td>传输大量的数据</td><br><td>少量的数据　　</td><br></tr><br><tr><br><td>速度　　　　　　</td><br><td>　慢　　　　　</td><br><td>快　　　　　　</td><br></tr><br></tbody><br></table><p>　　<strong>IP协议的基本功能是提供数据传输、数据包编止、数据包路由，分段等。通过ip编止约定</strong>，可以成功的将数据通过路由传输到正确的网络或者子网。ip协议是一种无连接的协议。<br>　　<strong>面向连接的tcp,是基于连接的协议</strong>，也就是说在正式收发数据前，必须和对方建立可靠的连接，该协议能提供可靠的数据传输，保证数据准确无误的到达目的地。<br>　　<font color="forestgreen">QQ就是udp发信息的，因此有时会出现收不到信息的时候此外，udp程序结构比较简单，tcp对系统资源的要求比udp多，udp和tcp协议是传输层的协议，而ip协议是网络层的协议，tcp的连接需要三次握手过程，断开需要4次。</font></p><pre><code>&lt;/div&gt;</code></pre><p>  </p>]]></content>
    
    <summary type="html">
    
      &lt;font size=&quot;5&quot; color=&quot;lightseagreen&quot;&gt; 新年的第一篇博客，新的一年继续加油努力，这这一年里一定要遇到更好的自己，这篇博客是就自己做的一个APP中用到了socket,总结一下TCP和UDP通信。&lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="工作" scheme="http://www.douxiao.org/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="TCP" scheme="http://www.douxiao.org/tags/TCP/"/>
    
      <category term="UDP" scheme="http://www.douxiao.org/tags/UDP/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04安装多版本Opencv</title>
    <link href="http://www.douxiao.org/2018/01/20/Ubuntu16-04%E5%AE%89%E8%A3%85%E5%A4%9A%E7%89%88%E6%9C%ACOpencv/"/>
    <id>http://www.douxiao.org/2018/01/20/Ubuntu16-04安装多版本Opencv/</id>
    <published>2018-01-20T01:23:13.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>　<font size="5" color="lightseagreen">由于opencv更新了新的版本opencv3.4，又不想把之前的opencv3.2卸载，所以尝试一下安装多版本的opencv，安装记录如下。<br> </font><br><a id="more"></a></p><h1 id="一、opencv3-4的一些特性（和opencv3-2比较）"><a href="#一、opencv3-4的一些特性（和opencv3-2比较）" class="headerlink" title="一、opencv3.4的一些特性（和opencv3.2比较）"></a><a href="#一、opencv3-4的一些特性（和opencv3-2比较）" title="一、opencv3.4的一些特性（和opencv3.2比较）"></a>一、opencv3.4的一些特性（和opencv3.2比较）</h1><p> 1、将DNN模块从opencv_contrib推广到主存储库，并对其进行了改进和加速，一个外部的BLAS实现不在需要，对于GPU，使用Halide进行实验DNN加速。<br> 2、增加了faster-rcnn支持。<br> 3、英特尔媒体SDK现在可以被我们的视频模块利用来进行硬件加速的视频编码/解码。支持MPEG1 / 2和H.264。<br> 4、现在可以使用标志ENABLE_CXX11将OpenCV构建为C++ 11库。为C++ 11程序员添加了一些很酷的功能。</p><h1 id="二、下载opencv3-4"><a href="#二、下载opencv3-4" class="headerlink" title="二、下载opencv3.4"></a><a href="#二、下载opencv3-4" title="二、下载opencv3.4"></a>二、下载opencv3.4</h1><p>  <a href="https://opencv.org/releases.html" target="_blank" rel="noopener">opencv3.4下载地址</a><br>  <a href="https://github.com/opencv/opencv_contrib/releases" target="_blank" rel="noopener">opencv_contrib下载地址</a><br>  目录如下图所示（里面还包含之前opencv3.2的安装目录）：<br>  <img src="http://ow7va355d.bkt.clouddn.com/opencv%E7%9B%AE%E5%BD%95.png" alt=""></p><h1 id="三、安装步骤"><a href="#三、安装步骤" class="headerlink" title="三、安装步骤"></a><a href="#三、安装步骤" title="三、安装步骤"></a>三、安装步骤</h1><h2 id="1、Update-packages"><a href="#1、Update-packages" class="headerlink" title="1、Update packages"></a><a href="#1、Update-packages" title="1、Update packages"></a>1、Update packages</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update</div><div class="line">sudo apt-get upgrade</div></pre></td></tr></table></figure><h2 id="2、Install-OS-libraries"><a href="#2、Install-OS-libraries" class="headerlink" title="2、Install OS libraries"></a><a href="#2、Install-OS-libraries" title="2、Install OS libraries"></a>2、Install OS libraries</h2><h3 id="1-删除先前安装-x264"><a href="#1-删除先前安装-x264" class="headerlink" title="1)删除先前安装 x264"></a><a href="#1-删除先前安装-x264" title="1)删除先前安装 x264"></a>1)删除先前安装 x264</h3><p><code>sudo apt-get remove x264 libx264-dev</code></p><h3 id="2-安装依赖"><a href="#2-安装依赖" class="headerlink" title="2)安装依赖"></a><a href="#2-安装依赖" title="2)安装依赖"></a>2)安装依赖</h3><p>安装依赖解释如下图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#构建系统               编译依赖         跟踪安装      环境配置脚本      Intel汇编器  GCC的Fortran编译前端</span></div><div class="line">sudo apt-get install build-essential checkinstall cmake pkg-config    yasm      gfortran</div><div class="line"><span class="comment"># 图片库：            JPG库的头文件   JPEG-2000库头文件   PNG库头文件   TIFF库头文件</span></div><div class="line">sudo apt-get install libjpeg8-dev  libjasper-dev     libpng12-dev  libtiff5-dev</div><div class="line"><span class="comment"># 视频库：            FFmpeg库头文件  FFmpeg格式库头文件  缩放库头文件     IEEE-1394规范摄像头库头文件  Xvid解码库头文件 x264库</span></div><div class="line">sudo apt-get install libavcodec-dev libavformat-dev   libswscale-dev libdc1394<span class="number">-22</span>-dev    x264        v4l-utils</div><div class="line"><span class="comment"># 播放器库：           Xine2的头文件   Video-For-Linux（v4l）</span></div><div class="line">sudo apt-get install libxine2-dev   libv4l-dev</div><div class="line"><span class="comment">#                    自由解码库头文件         GStreamer头文件</span></div><div class="line">sudo apt-get install libgstreamer0<span class="number">.10</span>-dev  libgstreamer-plugins-base0<span class="number">.10</span>-dev</div><div class="line"><span class="comment"># GUI库：             Qt5库头文件   GTK2库头文件</span></div><div class="line">sudo apt-get install qt5-default  libgtk2<span class="number">.0</span>-dev</div><div class="line"><span class="comment"># 运算库：             C++多核多线程库头文件   ATLAS线性代数库头文件</span></div><div class="line">sudo apt-get install libtbb-dev           libatlas-base-dev</div><div class="line"><span class="comment"># 音频库：             AAC解码库头文件   MP3库头文件       Theora库头文件  Vorbis库头文件</span></div><div class="line">sudo apt-get install libfaac-dev     libmp3lame-dev   libtheora-dev libvorbis-dev libxvidcore-dev</div><div class="line"></div><div class="line"><span class="comment"># 语音识别库：         窄域库头文件          宽域库头文件</span></div><div class="line">sudo apt-get install libopencore-amrnb-dev libopencore-amrwb-dev</div><div class="line"></div><div class="line"> </div><div class="line"><span class="comment"># Optional dependencies</span></div><div class="line"><span class="comment"># Protobuf，其实这个蛮常用的</span></div><div class="line">sudo apt-get install libprotobuf-dev protobuf-compiler</div><div class="line"><span class="comment"># Google的日志和命令行标识处理库</span></div><div class="line">sudo apt-get install libgoogle-glog-dev libgflags-dev</div><div class="line">用来访问处理各种摄像头的库头文件       C++线性代数模板库头文件  HDF5科学数据文件库头文件  API文档生成工具</div><div class="line">sudo apt-get install libgphoto2-dev libeigen3-dev         libhdf5-dev            doxygen</div></pre></td></tr></table></figure><h3 id="3-安装python环境"><a href="#3-安装python环境" class="headerlink" title="3)安装python环境"></a><a href="#3-安装python环境" title="3)安装python环境"></a>3)安装python环境</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install python-dev python-pip python3-dev python3-pip</div><div class="line">sudo -H pip2 install -U pip numpy scipy matplotlib scikit-image scikit-learn ipython==<span class="number">5.4</span></div><div class="line">sudo -H pip3 install -U pip numpy scipy matplotlib scikit-image scikit-learn ipython</div></pre></td></tr></table></figure><h3 id="执行完上述指令后-我们需要解决当前存在于contrib模块中的一个问题，特别是在freetype模块中，它允许您绘制UTF-8字符串。如果遇到类似于ImportError的错误：-usr-local-lib-libopencv-freetype-so-3-2：undefined-symbol：hb-shape，这个问题将解决它：注意需要更改contrib路径。"><a href="#执行完上述指令后-我们需要解决当前存在于contrib模块中的一个问题，特别是在freetype模块中，它允许您绘制UTF-8字符串。如果遇到类似于ImportError的错误：-usr-local-lib-libopencv-freetype-so-3-2：undefined-symbol：hb-shape，这个问题将解决它：注意需要更改contrib路径。" class="headerlink" title="执行完上述指令后,我们需要解决当前存在于contrib模块中的一个问题，特别是在freetype模块中，它允许您绘制UTF-8字符串。如果遇到类似于ImportError的错误：/usr/local/lib/libopencv_freetype.so.3.2：undefined symbol：hb_shape，这个问题将解决它：注意需要更改contrib路径。"></a><a href="#执行完上述指令后-我们需要解决当前存在于contrib模块中的一个问题，特别是在freetype模块中，它允许您绘制UTF-8字符串。如果遇到类似于ImportError的错误：-usr-local-lib-libopencv-freetype-so-3-2：undefined-symbol：hb-shape，这个问题将解决它：注意需要更改contrib路径。" title="执行完上述指令后,我们需要解决当前存在于contrib模块中的一个问题，特别是在freetype模块中，它允许您绘制UTF-8字符串。如果遇到类似于ImportError的错误：/usr/local/lib/libopencv_freetype.so.3.2：undefined symbol：hb_shape，这个问题将解决它：注意需要更改contrib路径。"></a>执行完上述指令后,我们需要解决当前存在于contrib模块中的一个问题，特别是在freetype模块中，它允许您绘制UTF-8字符串。如果遇到类似于ImportError的错误：/usr/local/lib/libopencv_freetype.so.3.2：undefined symbol：hb_shape，这个问题将解决它：注意需要更改contrib路径。</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sed -i <span class="string">‘s/$&#123;freetype2_LIBRARIES&#125; $&#123;harfbuzz_LIBRARIES&#125;/$&#123;FREETYPE_LIBRARIES&#125; $&#123;HARFBUZZ_LIBRARIES&#125;/g’</span> ../opencv_contrib<span class="number">-3.</span>４<span class="number">.0</span>/modules/freetype/CMakeLists.txt</div></pre></td></tr></table></figure><h3 id="４）创建build路径"><a href="#４）创建build路径" class="headerlink" title="４）创建build路径"></a><a href="#４）创建build路径" title="４）创建build路径"></a>４）创建build路径</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">cd /data/opencv/opencv<span class="number">-3.4</span><span class="number">.0</span></div><div class="line">mkdir install</div><div class="line">mkdir build</div><div class="line">cd build</div></pre></td></tr></table></figure><h3 id="5-运行cmake"><a href="#5-运行cmake" class="headerlink" title="5)运行cmake"></a><a href="#5-运行cmake" title="5)运行cmake"></a>5)运行cmake</h3><p>注意:CMAKE_INSTALL_PREFIX预安装路径需要更改</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 通过cmake生成完整编译配置脚本：</span></div><div class="line"><span class="comment"># 1. 构建正式RELEASE版</span></div><div class="line"><span class="comment"># 2. 编译好的库安装到/data/opencv/opencv-3.4.0/install/</span></div><div class="line"><span class="comment"># 3. 安装C的例子</span></div><div class="line"><span class="comment"># 4. 安装Python的例子</span></div><div class="line"><span class="comment"># 5. 开启TBB、V4L、Qt、OpenGL特性</span></div><div class="line"><span class="comment"># 6. 编译安装开源社区贡献库</span></div><div class="line"><span class="comment"># 7. 编译其他例子</span></div><div class="line">cmake  -D CMAKE_BUILD_TYPE=RELEASE \</div><div class="line">　     -D CMAKE_INSTALL_PREFIX=/data/opencv/opencv<span class="number">-3.4</span><span class="number">.0</span>/install/ \</div><div class="line">　　   -D INSTALL_PYTHON_EXAMPLES=ON \</div><div class="line">　　   -D INSTALL_C_EXAMPLES=ON \</div><div class="line">      -D WITH_TBB=ON \</div><div class="line">      -D WITH_V4L=ON \</div><div class="line">      -D WITH_QT=ON \</div><div class="line">      -D WITH_OPENGL=ON \</div><div class="line">　　   -D OPENCV_EXTRA_MODULES_PATH=/data/opencv/opencv_contrib<span class="number">-3.4</span><span class="number">.0</span>/modules \</div><div class="line">　　　 -D BUILD_EXAMPLES=ON ..</div></pre></td></tr></table></figure><h3 id="6-编译安装"><a href="#6-编译安装" class="headerlink" title="6)编译安装"></a><a href="#6-编译安装" title="6)编译安装"></a>6)编译安装</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># find out number of CPU cores in your machine</span></div><div class="line">nproc</div><div class="line"><span class="comment"># substitute 4 by output of nproc</span></div><div class="line">make -j4</div><div class="line">sudo make install -j4</div></pre></td></tr></table></figure><h2 id="2、系统配置"><a href="#2、系统配置" class="headerlink" title="2、系统配置"></a><a href="#2、系统配置" title="2、系统配置"></a>2、系统配置</h2><p>安装完成之后需要将opencv写入环境，更改为opencv3.4.0</p><p>方法：$sudo gedit /etc/ld.so.conf.d/opencv.conf</p><p>将<code>/usr/local/lib</code>替换为<br><code>/data/opencv/opencv-3.4.0/install/lib</code><br>使用下面的动态库管理命令ldconfig，让opencv的相关链接库被系统<br><code>$ sudo ldconfig</code><br>如果成功的话，正确的结果将会发现前面动态库的名字<br><code>$ sudo ldconfig -v &amp;gt;temp.so.txt &amp;amp;&amp;amp; cat temp.so.txt|grep opencv</code></p><h3 id="指定头文件位置"><a href="#指定头文件位置" class="headerlink" title="指定头文件位置"></a><a href="#指定头文件位置" title="指定头文件位置"></a>指定头文件位置</h3><p><code>$sudo gedit /etc/profile</code><br>在文件末尾添加</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">export PKG_CONFIG_PATH=/data/opencv/opencv<span class="number">-3.4</span><span class="number">.0</span>/install/lib/ pkgconfig</div><div class="line">export LD_LIBRARY_PATH=/data/opencv/opencv<span class="number">-3.4</span><span class="number">.0</span>/install/lib</div></pre></td></tr></table></figure><p>保存之后</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo ldconfig</div><div class="line">$ source /etc/profile</div></pre></td></tr></table></figure><h3 id="添加python接口"><a href="#添加python接口" class="headerlink" title="添加python接口"></a><a href="#添加python接口" title="添加python接口"></a>添加python接口</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo ln -s /data/opencv/opencv<span class="number">-3.4</span><span class="number">.0</span>/install/lib/python2<span class="number">.7</span>/dist-packages/cv2.so /usr/local/lib/python2<span class="number">.7</span>/dist-packages</div><div class="line">sudo ln -s /data/opencv/opencv<span class="number">-3.4</span><span class="number">.0</span>/lib/python3<span class="number">.5</span>/dist-packages/cv2.cpython<span class="number">-35</span>m-x86_64-linux-gnu.so /usr/local/lib/python3<span class="number">.5</span>/dist-packages</div></pre></td></tr></table></figure><h2 id="3、版本查看"><a href="#3、版本查看" class="headerlink" title="3、版本查看"></a><a href="#3、版本查看" title="3、版本查看"></a>3、版本查看</h2><h3 id="opencv版本查看"><a href="#opencv版本查看" class="headerlink" title="opencv版本查看"></a><a href="#opencv版本查看" title="opencv版本查看"></a>opencv版本查看</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$pkg-config –modversion opencv  </div><div class="line">$pkg-config –cflags opencv  </div><div class="line">$pkg-config –libs opencv</div></pre></td></tr></table></figure><p>参考资料：</p><p><a href="https://www.learnopencv.com/install-opencv3-on-ubuntu/" target="_blank" rel="noopener">1、Install OpenCV3 on Ubuntu</a><br><a href="https://www.jianshu.com/p/9baa48f032ca" target="_blank" rel="noopener">2、ubuntu 16.04 安装 opencv+contrib (3.2.0)</a><br><a href="http://www.samontab.com/web/2017/06/installing-opencv-3-2-0-with-contrib-modules-in-ubuntu-16-04-lts/" target="_blank" rel="noopener">3、Installing OpenCV 3.2.0 with contrib modules in Ubuntu 16.04 LTS</a><br><a href="http://blog.csdn.net/heyijia0327/article/details/54575245" target="_blank" rel="noopener">4、ubuntu 安装使用多版本opencv</a></p><p><a href="http://blog.csdn.net/qq_34952119/article/details/71501652" target="_blank" rel="noopener">5、Ubuntu 多版本Opencv安装配置教程</a><br><a href="https://bruceking.site/2017/10/27/install-opencv3-on-ubuntu16/" target="_blank" rel="noopener">6、在Ubuntu 16.04安装OpenCV 3</a></p><p>下面附上我的cmake编译结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div></pre></td><td class="code"><pre><div class="line">cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/data/opencv/opencv<span class="number">-3.4</span><span class="number">.0</span>/install/ 　-D PYTHON3_EXECUTABLE=/usr/bin/python3 　-D PYTHON_INCLUDE_DIR=/usr/include/python3<span class="number">.5</span>  　-D PYTHON_LIBRARY=/usr/lib/x86_64-linux-gnu/libpython3<span class="number">.5</span>m.so 　　-D PYTHON3_NUMPY_INCLUDE_DIRS=/usr/local/lib/python3<span class="number">.5</span>/dist-packages/numpy/core/include 　　-D INSTALL_PYTHON_EXAMPLES=ON 　　-D INSTALL_C_EXAMPLES=ON    -D WITH_TBB=ON    -D WITH_V4L=ON    -D WITH_QT=ON    -D WITH_OPENGL=ON 　　-D OPENCV_EXTRA_MODULES_PATH=/data/opencv/opencv_contrib<span class="number">-3.4</span><span class="number">.0</span>/modules 　　-D PYTHON_EXECUTABLE=/usr/lib/python3 　　-D BUILD_EXAMPLES=ON ..</div><div class="line">– Looking <span class="keyword">for</span> ccache - <span class="keyword">not</span> found</div><div class="line">– Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found suitable version <span class="string">“1.2.8”</span>, minimum required <span class="keyword">is</span> <span class="string">“1.2.3”</span>) </div><div class="line">– Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version <span class="string">“1.2.8”</span>) </div><div class="line">– Checking <span class="keyword">for</span> module <span class="string">‘gstreamer-base-1.0’</span></div><div class="line">–   No package <span class="string">‘gstreamer-base-1.0’</span> found</div><div class="line">– Checking <span class="keyword">for</span> module <span class="string">‘gstreamer-video-1.0’</span></div><div class="line">–   No package <span class="string">‘gstreamer-video-1.0’</span> found</div><div class="line">– Checking <span class="keyword">for</span> module <span class="string">‘gstreamer-app-1.0’</span></div><div class="line">–   No package <span class="string">‘gstreamer-app-1.0’</span> found</div><div class="line">– Checking <span class="keyword">for</span> module <span class="string">‘gstreamer-riff-1.0’</span></div><div class="line">–   No package <span class="string">‘gstreamer-riff-1.0’</span> found</div><div class="line">– Checking <span class="keyword">for</span> module <span class="string">‘gstreamer-pbutils-1.0’</span></div><div class="line">–   No package <span class="string">‘gstreamer-pbutils-1.0’</span> found</div><div class="line">– Looking <span class="keyword">for</span> linux/videodev.h</div><div class="line">– Looking <span class="keyword">for</span> linux/videodev.h - <span class="keyword">not</span> found</div><div class="line">– Looking <span class="keyword">for</span> linux/videodev2.h</div><div class="line">– Looking <span class="keyword">for</span> linux/videodev2.h - found</div><div class="line">– Looking <span class="keyword">for</span> sys/videoio.h</div><div class="line">– Looking <span class="keyword">for</span> sys/videoio.h - <span class="keyword">not</span> found</div><div class="line">– Checking <span class="keyword">for</span> module <span class="string">‘libavresample’</span></div><div class="line">–   No package <span class="string">‘libavresample’</span> found</div><div class="line">– Found TBB: /usr/lib/x86_64-linux-gnu/libtbb.so</div><div class="line">– found Intel IPP (ICV version): <span class="number">2017.0</span><span class="number">.3</span> [<span class="number">2017.0</span><span class="number">.3</span>]</div><div class="line">– at: /data/opencv/opencv<span class="number">-3.4</span><span class="number">.0</span>/build/<span class="number">3</span>rdparty/ippicv/ippicv_lnx</div><div class="line">– found Intel IPP IW sources: <span class="number">2017.0</span><span class="number">.3</span></div><div class="line">– at: /data/opencv/opencv<span class="number">-3.4</span><span class="number">.0</span>/build/<span class="number">3</span>rdparty/ippicv/ippiw_lnx</div><div class="line">– CUDA detected: <span class="number">8.0</span></div><div class="line">– CUDA NVCC target flags: -gencode;arch=compute_20,code=sm_20;-gencode;arch=compute_30,code=sm_30;-gencode;arch=compute_35,code=sm_35;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-D_FORCE_INLINES</div><div class="line">– LAPACK(Atlas): LAPACK_LIBRARIES: /usr/lib/liblapack.so;/usr/lib/libcblas.so;/usr/lib/libatlas.so</div><div class="line">– LAPACK(Atlas): Support <span class="keyword">is</span> enabled.</div><div class="line">– Could NOT find Pylint (missing:  PYLINT_EXECUTABLE) </div><div class="line">– VTK <span class="keyword">is</span> <span class="keyword">not</span> found. Please set -DVTK_DIR <span class="keyword">in</span> CMake to VTK build directory, <span class="keyword">or</span> to VTK install subdirectory <span class="keyword">with</span> VTKConfig.cmake file</div><div class="line">– Excluding <span class="keyword">from</span> source files list: /data/opencv/opencv<span class="number">-3.4</span><span class="number">.0</span>/build/modules/imgproc/accum.neon.cpp</div><div class="line">– </div><div class="line">– General configuration <span class="keyword">for</span> OpenCV <span class="number">3.4</span><span class="number">.0</span> =====================================</div><div class="line">–   Version control:               unknown</div><div class="line">– </div><div class="line">–   Platform:</div><div class="line">–     Timestamp:                   <span class="number">2018</span><span class="number">-01</span><span class="number">-26</span>T07:<span class="number">32</span>:<span class="number">49</span>Z</div><div class="line">–     Host:                        Linux <span class="number">4.10</span><span class="number">.1</span><span class="number">-041001</span>-generic x86_64</div><div class="line">–     CMake:                       <span class="number">3.5</span><span class="number">.1</span></div><div class="line">–     CMake generator:             Unix Makefiles</div><div class="line">–     CMake build tool:            /usr/bin/make</div><div class="line">–     Configuration:               RELEASE</div><div class="line">– </div><div class="line">–   CPU/HW features:</div><div class="line">–     Baseline:                    SSE SSE2 SSE3</div><div class="line">–       requested:                 SSE3</div><div class="line">–     Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2</div><div class="line">–       requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2</div><div class="line">–       SSE4_1 (<span class="number">3</span> files):          + SSSE3 SSE4_1</div><div class="line">–       SSE4_2 (<span class="number">1</span> files):          + SSSE3 SSE4_1 POPCNT SSE4_2</div><div class="line">–       FP16 (<span class="number">2</span> files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX</div><div class="line">–       AVX (<span class="number">5</span> files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX</div><div class="line">–       AVX2 (<span class="number">9</span> files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2</div><div class="line">– </div><div class="line">–   C/C++:</div><div class="line">–     Built as dynamic libs?:      YES</div><div class="line">–     C++ Compiler:                /usr/bin/c++  (ver <span class="number">5.4</span><span class="number">.0</span>)</div><div class="line">–     C++ flags (Release):         -fsigned-char -W -Wall -Werror=<span class="keyword">return</span>-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -fdiagnostics-show-option -Wno-long-long -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG  -DNDEBUG</div><div class="line">–     C++ flags (Debug):           -fsigned-char -W -Wall -Werror=<span class="keyword">return</span>-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wundef -Winit-self -Wpointer-arith -Wshadow -Wsign-promo -Wuninitialized -Winit-self -Wno-narrowing -Wno-delete-non-virtual-dtor -Wno-comment -fdiagnostics-show-option -Wno-long-long -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -fvisibility-inlines-hidden -g  -O0 -DDEBUG -D_DEBUG</div><div class="line">–     C Compiler:                  /usr/bin/cc</div><div class="line">–     C flags (Release):           -fsigned-char -W -Wall -Werror=<span class="keyword">return</span>-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -fdiagnostics-show-option -Wno-long-long -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -O3 -DNDEBUG  -DNDEBUG</div><div class="line">–     C flags (Debug):             -fsigned-char -W -Wall -Werror=<span class="keyword">return</span>-type -Werror=non-virtual-dtor -Werror=address -Werror=sequence-point -Wformat -Werror=format-security -Wmissing-declarations -Wmissing-prototypes -Wstrict-prototypes -Wundef -Winit-self -Wpointer-arith -Wshadow -Wuninitialized -Winit-self -Wno-narrowing -Wno-comment -fdiagnostics-show-option -Wno-long-long -pthread -fomit-frame-pointer -ffunction-sections -fdata-sections  -msse -msse2 -msse3 -fvisibility=hidden -g  -O0 -DDEBUG -D_DEBUG</div><div class="line">–     Linker flags (Release):</div><div class="line">–     Linker flags (Debug):</div><div class="line">–     ccache:                      NO</div><div class="line">–     Precompiled headers:         YES</div><div class="line">–     Extra dependencies:          dl m pthread rt /usr/lib/x86_64-linux-gnu/libGLU.so /usr/lib/x86_64-linux-gnu/libGL.so /usr/lib/x86_64-linux-gnu/libtbb.so cudart nppc nppial nppicc nppicom nppidei nppif nppig nppim nppist nppisu nppitc npps cublas cufft -L/usr/local/cuda<span class="number">-8.0</span>/lib64</div><div class="line">–     <span class="number">3</span>rdparty dependencies:</div><div class="line">– </div><div class="line">–   OpenCV modules:</div><div class="line">–     To be built:                 calib3d core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev dnn features2d flann highgui imgcodecs imgproc ml objdetect photo python2 python3 python_bindings_generator shape stitching superres ts video videoio videostab</div><div class="line">–     Disabled:                    js world</div><div class="line">–     Disabled by dependency:      -</div><div class="line">–     Unavailable:                 java viz</div><div class="line">–     Applications:                tests perf_tests apps</div><div class="line">–     Documentation:               YES (/usr/bin/doxygen <span class="number">1.8</span><span class="number">.11</span>)</div><div class="line">–     Non-free algorithms:         NO</div><div class="line">– </div><div class="line">–   GUI: </div><div class="line">–     QT:                          YES (ver <span class="number">5.5</span><span class="number">.1</span>)</div><div class="line">–       QT OpenGL support:         YES (Qt5::OpenGL <span class="number">5.5</span><span class="number">.1</span>)</div><div class="line">–     GTK+:                        NO</div><div class="line">–     OpenGL support:              YES (/usr/lib/x86_64-linux-gnu/libGLU.so /usr/lib/x86_64-linux-gnu/libGL.so)</div><div class="line">–     VTK support:                 NO</div><div class="line">– </div><div class="line">–   Media I/O: </div><div class="line">–     ZLib:                        /usr/lib/x86_64-linux-gnu/libz.so (ver <span class="number">1.2</span><span class="number">.8</span>)</div><div class="line">–     JPEG:                        /usr/lib/x86_64-linux-gnu/libjpeg.so (ver )</div><div class="line">–     WEBP:                        build (ver encoder: <span class="number">0x020e</span>)</div><div class="line">–     PNG:                         /usr/lib/x86_64-linux-gnu/libpng.so (ver <span class="number">1.2</span><span class="number">.54</span>)</div><div class="line">–     TIFF:                        /usr/lib/x86_64-linux-gnu/libtiff.so (ver <span class="number">42</span> / <span class="number">4.0</span><span class="number">.6</span>)</div><div class="line">–     JPEG <span class="number">2000</span>:                   /usr/lib/x86_64-linux-gnu/libjasper.so (ver <span class="number">1.900</span><span class="number">.1</span>)</div><div class="line">–     OpenEXR:                     build (ver <span class="number">1.7</span><span class="number">.1</span>)</div><div class="line">– </div><div class="line">–   Video I/O:</div><div class="line">–     DC1394:                      YES (ver <span class="number">2.2</span><span class="number">.4</span>)</div><div class="line">–     FFMPEG:                      YES</div><div class="line">–       avcodec:                   YES (ver <span class="number">56.60</span><span class="number">.100</span>)</div><div class="line">–       avformat:                  YES (ver <span class="number">56.40</span><span class="number">.101</span>)</div><div class="line">–       avutil:                    YES (ver <span class="number">54.31</span><span class="number">.100</span>)</div><div class="line">–       swscale:                   YES (ver <span class="number">3.1</span><span class="number">.101</span>)</div><div class="line">–       avresample:                NO</div><div class="line">–     GStreamer:                   </div><div class="line">–       base:                      YES (ver <span class="number">0.10</span><span class="number">.36</span>)</div><div class="line">–       video:                     YES (ver <span class="number">0.10</span><span class="number">.36</span>)</div><div class="line">–       app:                       YES (ver <span class="number">0.10</span><span class="number">.36</span>)</div><div class="line">–       riff:                      YES (ver <span class="number">0.10</span><span class="number">.36</span>)</div><div class="line">–       pbutils:                   YES (ver <span class="number">0.10</span><span class="number">.36</span>)</div><div class="line">–     libv4l/libv4l2:              NO</div><div class="line">–     v4l/v4l2:                    linux/videodev2.h</div><div class="line">–     gPhoto2:                     YES</div><div class="line">– </div><div class="line">–   Parallel framework:            TBB (ver <span class="number">4.4</span> interface <span class="number">9002</span>)</div><div class="line">– </div><div class="line">–   Trace:                         YES (<span class="keyword">with</span> Intel ITT)</div><div class="line">– </div><div class="line">–   Other third-party libraries:</div><div class="line">–     Intel IPP:                   <span class="number">2017.0</span><span class="number">.3</span> [<span class="number">2017.0</span><span class="number">.3</span>]</div><div class="line">–            at:                   /data/opencv/opencv<span class="number">-3.4</span><span class="number">.0</span>/build/<span class="number">3</span>rdparty/ippicv/ippicv_lnx</div><div class="line">–     Intel IPP IW:                sources (<span class="number">2017.0</span><span class="number">.3</span>)</div><div class="line">–               at:                /data/opencv/opencv<span class="number">-3.4</span><span class="number">.0</span>/build/<span class="number">3</span>rdparty/ippicv/ippiw_lnx</div><div class="line">–     Lapack:                      YES (/usr/lib/liblapack.so /usr/lib/libcblas.so /usr/lib/libatlas.so)</div><div class="line">–     Eigen:                       YES (ver <span class="number">3.2</span><span class="number">.92</span>)</div><div class="line">–     Custom HAL:                  NO</div><div class="line">– </div><div class="line">–   NVIDIA CUDA:                   YES (ver <span class="number">8.0</span>, CUFFT CUBLAS)</div><div class="line">–     NVIDIA GPU arch:             <span class="number">20</span> <span class="number">30</span> <span class="number">35</span> <span class="number">37</span> <span class="number">50</span> <span class="number">52</span> <span class="number">60</span> <span class="number">61</span></div><div class="line">–     NVIDIA PTX archs:</div><div class="line">– </div><div class="line">–   OpenCL:                        YES (no extra features)</div><div class="line">–     Include path:                /data/opencv/opencv<span class="number">-3.4</span><span class="number">.0</span>/<span class="number">3</span>rdparty/include/opencl/<span class="number">1.2</span></div><div class="line">–     Link libraries:              Dynamic load</div><div class="line">– </div><div class="line">–   Python <span class="number">2</span>:</div><div class="line">–     Interpreter:                 /usr/bin/python2<span class="number">.7</span> (ver <span class="number">2.7</span><span class="number">.12</span>)</div><div class="line">–     Libraries:                   /usr/lib/x86_64-linux-gnu/libpython2<span class="number">.7</span>.so (ver <span class="number">2.7</span><span class="number">.12</span>)</div><div class="line">–     numpy:                       /usr/local/lib/python2<span class="number">.7</span>/dist-packages/numpy/core/include (ver <span class="number">1.13</span><span class="number">.1</span>)</div><div class="line">–     packages path:               lib/python2<span class="number">.7</span>/dist-packages</div><div class="line">– </div><div class="line">–   Python <span class="number">3</span>:</div><div class="line">–     Interpreter:                 /usr/bin/python3 (ver <span class="number">3.5</span><span class="number">.2</span>)</div><div class="line">–     Libraries:                   /usr/lib/x86_64-linux-gnu/libpython3<span class="number">.5</span>m.so (ver <span class="number">3.5</span><span class="number">.2</span>)</div><div class="line">–     numpy:                       /usr/lib/python3/dist-packages/numpy/core/include (ver <span class="number">1.11</span><span class="number">.0</span>)</div><div class="line">–     packages path:               lib/python3<span class="number">.5</span>/dist-packages</div><div class="line">– </div><div class="line">–   Python (<span class="keyword">for</span> build):            /usr/bin/python2<span class="number">.7</span></div><div class="line">– </div><div class="line">–   Java:</div><div class="line">–     ant:                         NO</div><div class="line">–     JNI:                         /usr/lib/jvm/java<span class="number">-8</span>-oracle/include /usr/lib/jvm/java<span class="number">-8</span>-oracle/include/linux /usr/lib/jvm/java<span class="number">-8</span>-oracle/include</div><div class="line">–     Java wrappers:               NO</div><div class="line">–     Java tests:                  NO</div><div class="line">– </div><div class="line">–   Matlab:                        YES</div><div class="line">–     mex:                         /usr/local/MATLAB/R2015b/bin/mex</div><div class="line">–     Compiler/generator:          Not working (bindings will <span class="keyword">not</span> be generated)</div><div class="line">– </div><div class="line">–   Install to:                    /data/opencv/opencv<span class="number">-3.4</span><span class="number">.0</span>/install</div><div class="line">– —————————————————————–</div><div class="line">– </div><div class="line">– Configuring done</div><div class="line">– Generating done</div><div class="line">– Build files have been written to: /data/opencv/opencv<span class="number">-3.4</span><span class="number">.0</span>/build</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　&lt;font size=&quot;5&quot; color=&quot;lightseagreen&quot;&gt;由于opencv更新了新的版本opencv3.4，又不想把之前的opencv3.2卸载，所以尝试一下安装多版本的opencv，安装记录如下。&lt;br&gt; &lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="工作" scheme="http://www.douxiao.org/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="opencv" scheme="http://www.douxiao.org/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>python环境配置</title>
    <link href="http://www.douxiao.org/2017/12/26/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    <id>http://www.douxiao.org/2017/12/26/python环境配置/</id>
    <published>2017-12-26T01:00:21.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　<font size="5" color="lightseagreen"> 这篇博客是总结Python环境配置问题，各种坑因为系统安装了好几个Python环境，系统默认的Python环境变量只有一个，在这里如果需要多个版本的Python,自己使用的虚拟环境，virtualenv。</font><br><a id="more"></a></p><p><strong>Linux如何执行命令</strong></p><p>Linux的终端输入任何一条命令，Shell都会去到几个和环境变量（$PATH）相关的文件中寻找对应的路径下是否有可以执行的文件。这些可执行的文件里放着的是我们输入命令和如何执行这些命令的说明。只有找到了这个文件shell才知道如何执行命令，这个过程可以简化理解为：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">输入命令 -&gt; shell 读入命令 -&gt; 查找文件 -&gt; 定位文件中的环境变量 -&gt; 浏览环境变量对应的路径下的文件 -&gt; 在文件里寻找命令和执行方法 -&gt; 找到了，按照要求执行 -&gt; 找不到，输出找不到</div></pre></td></tr></table></figure><p><strong>shell常用的和环境变量相关的命令</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ env和$printenv用于展示环境变量配置文件的内容</div><div class="line">$ set展示所有的变量</div><div class="line">$ export将变量导出到接下来的程序环境中</div><div class="line">$ echo 输出如果跟着变量名，会把变量的值输出</div></pre></td></tr></table></figure><p><strong>相关环境变量配置文件</strong><br>涉及到的文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">/etc/profile</div><div class="line">/etc/bashrc</div><div class="line">/etc/zshrc</div></pre></td></tr></table></figure><p>如果是etc目录下则是所有的用户共享的配置文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">~/.profile</div><div class="line">~/.bash_profile</div><div class="line">~/.bashrc</div><div class="line">~/.zshrc</div></pre></td></tr></table></figure><p><strong>环境变量示例</strong></p><p><code>export PATH=&amp;quot;/usr/bin:/usr/local/bin&amp;quot;</code></p><p>export表示将变量导出到接下来的程序环境中。冒号是链接符号，这里的程序会顺序的逐个去找对应的执行文件。bin文件夹下一般存放的是可执行的二进制文件，当输入命令的时候，程序会逐个文件去找，如果找到了就会执行，找不到，就会反馈找不到。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　&lt;font size=&quot;5&quot; color=&quot;lightseagreen&quot;&gt; 这篇博客是总结Python环境配置问题，各种坑因为系统安装了好几个Python环境，系统默认的Python环境变量只有一个，在这里如果需要多个版本的Python,自己使用的虚拟环境，virtualenv。&lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="工作" scheme="http://www.douxiao.org/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="python" scheme="http://www.douxiao.org/tags/python/"/>
    
      <category term="env" scheme="http://www.douxiao.org/tags/env/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu软件创建图标</title>
    <link href="http://www.douxiao.org/2017/12/20/Ubuntu%E8%BD%AF%E4%BB%B6%E5%88%9B%E5%BB%BA%E5%9B%BE%E6%A0%87/"/>
    <id>http://www.douxiao.org/2017/12/20/Ubuntu软件创建图标/</id>
    <published>2017-12-20T14:50:51.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>　<font size="5" color="lightseagreen">  自己习惯了用ubuntu，特别是在安装了mac主题桌面后有一个非常棒的UI，但是美中不足的是安装一些软件后系统不能像windows下面那样自动创建图标，最近发现了一个特别有用的教程,用来创建Ubuntu图标。</font></p><a id="more"></a><p>举例。给自己的系统创建pycharm的图标<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /data/opt/pycharm</span><br><span class="line">touch pycharm.desktop</span><br><span class="line">gedit pycharm.desktop</span><br></pre></td></tr></table></figure></p><p>在打开的文件中填写以下代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[Desktop Entry]</span><br><span class="line">Name=Pycharm</span><br><span class="line">Type=Application</span><br><span class="line">Icon=/data/opt/pycharm/bin/pycharm.svg</span><br><span class="line">Exec=sh /data/opt/pycharm/bin/pycharm.sh</span><br></pre></td></tr></table></figure></p><p>注意:向eclipse本身点击就能够打开，最后一行应该这样写<br><code>Exec=/data/opt/eclipse</code><br>保存，执行一下命令<br><code>chmod a+x pycharm.desktop</code></p><p>这时你会发现图标发生变化，然后把图标复制到桌面就能够使用。</p><p><strong>特别注意:</strong>复制上面代码的时候注意每一行后面都不能有空格,很多同学制作失败就是因为copy上空格了,不过此时这个快捷方式还不能使用,你会发现图标并没有发生变化,双击也会出错.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　&lt;font size=&quot;5&quot; color=&quot;lightseagreen&quot;&gt;  自己习惯了用ubuntu，特别是在安装了mac主题桌面后有一个非常棒的UI，但是美中不足的是安装一些软件后系统不能像windows下面那样自动创建图标，最近发现了一个特别有用的教程,用来创建Ubuntu图标。&lt;/font&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="工作" scheme="http://www.douxiao.org/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="desktop" scheme="http://www.douxiao.org/tags/desktop/"/>
    
      <category term="ubuntu" scheme="http://www.douxiao.org/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>tfmodel_Object_Detection_API学习笔记</title>
    <link href="http://www.douxiao.org/2017/12/19/tfmodel-Object-Detection-API%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://www.douxiao.org/2017/12/19/tfmodel-Object-Detection-API学习笔记/</id>
    <published>2017-12-19T14:50:51.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<font size="5" color="lightseagreen"> 这篇博客是总结自己在使用tensorflow models里面的物体检测的API的方法以及调试中遇到的错误及心得。</font><a id="more"></a><h1 id="一、安装"><a href="#一、安装" class="headerlink" title="一、安装"></a><a href="#一、安装" title="一、安装"></a><strong>一、安装</strong></h1><h2 id="1、dependencies"><a href="#1、dependencies" class="headerlink" title="1、dependencies"></a><a href="#1、dependencies" title="1、dependencies"></a>1、dependencies</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Protobuf <span class="number">2.6</span></div><div class="line">Pillow <span class="number">1.0</span></div><div class="line">lxml</div><div class="line">tf Slim (which <span class="keyword">is</span> included <span class="keyword">in</span> the <span class="string">“tensorflow/models/research/“</span> checkout)</div><div class="line">Jupyter notebook</div><div class="line">Matplotlib</div><div class="line">Tensorflow</div></pre></td></tr></table></figure><h2 id="2、install-tensorflow"><a href="#2、install-tensorflow" class="headerlink" title="2、install tensorflow"></a><a href="#2、install-tensorflow" title="2、install tensorflow"></a>2、install tensorflow</h2><p> For detailed steps to install Tensorflow, follow the <a href="https://www.tensorflow.org/install/" target="_blank" rel="noopener">Tensorflow installation instructions</a></p><h2 id="3、install-denpendencies"><a href="#3、install-denpendencies" class="headerlink" title="3、install denpendencies"></a><a href="#3、install-denpendencies" title="3、install denpendencies"></a>3、install denpendencies</h2><p>  The remaining libraries can be installed on Ubuntu 16.04 using via apt-get:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install protobuf-compiler python-pil python-lxml</div><div class="line">sudo pip install jupyter</div><div class="line">sudo pip install matplotlib</div></pre></td></tr></table></figure><p>Alternatively, users can install dependencies using pip:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sudo pip install pillow</div><div class="line">sudo pip install lxml</div><div class="line">sudo pip install jupyter</div><div class="line">sudo pip install matplotlib</div></pre></td></tr></table></figure><h2 id="4、protobuf-compilation"><a href="#4、protobuf-compilation" class="headerlink" title="4、protobuf compilation"></a><a href="#4、protobuf-compilation" title="4、protobuf compilation"></a>4、protobuf compilation</h2><p> Tensorflow Object Detection API使用Protobufs来配置模型和训练参数。在可以使用框架之前，必须编译Protobuf库。这应该通过从tensorflow/ models/research/目录运行以下命令来完成:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># From tensorflow/models/research/</span></div><div class="line">protoc object_detection/protos/*.proto –python_out=.</div></pre></td></tr></table></figure><h2 id="6、Testing-the-Installation"><a href="#6、Testing-the-Installation" class="headerlink" title="6、Testing the Installation"></a><a href="#6、Testing-the-Installation" title="6、Testing the Installation"></a>6、Testing the Installation</h2><p> You can test that you have correctly installed the Tensorflow Object Detection<br>API by running the following command:</p><p><code>python object_detection/builders/model_builder_test.py</code></p><h1 id="二、实际使用经验"><a href="#二、实际使用经验" class="headerlink" title="二、实际使用经验"></a><a href="#二、实际使用经验" title="二、实际使用经验"></a><strong>二、实际使用经验</strong></h1><h2 id="1、安装tensorflow"><a href="#1、安装tensorflow" class="headerlink" title="1、安装tensorflow"></a><a href="#1、安装tensorflow" title="1、安装tensorflow"></a>1、安装tensorflow</h2><p>   自己采用的是virtualenv安装虚拟python环境的tensorflowpy2的models 放在tensorflowpy2/lib/python2.7/site-ackages/tensorflow/models路径下。<br>   用pycharm新建工程路径为models/research/object_detection。并使用该虚拟的python27环境。执行上面一系列的编译环境。<br>   然后自己执行，object_detection文件下面的object_detection_tutorial.ipynb文件,这里会下载一些权重参数可能会比较慢。<br>   在实行imports这个框的时候，需要把1.4.0版本改成自己对应的版本。</p><h2 id="2、遇到的问题"><a href="#2、遇到的问题" class="headerlink" title="2、遇到的问题"></a><a href="#2、遇到的问题" title="2、遇到的问题"></a>2、遇到的问题</h2><p>IOError: (‘http error’, 302, ‘Found’, <httplib.httpmessage instance="" at="" 0x7f6ed7dc40e0="">)</httplib.httpmessage></p><p>这个错误的解决办法是:需要重新编译proto。</p><p>如图<img src="http://ow7va355d.bkt.clouddn.com/Screenshot%20from%202017-12-20%2010-39-11.png" alt=""></p><font color="red" size="10"> 解决No module named ‘object_detection’问题</font><p>这个问题困扰了自己很久今天终于解决了，因为自己的tensorflow是安装在virualenv虚拟环境下的，开始自己以为虚拟环境下的tensorflow无法加载在系统环境变量下安装的modules，但是觉得应该有解决办法，下面是自己的探索之路:</p><font color="purple"><strong> 探索一、</strong>虚拟环境如何加载系统环境:</font><br>实测默认情况下虚拟环境不会依赖系统环境的global site-packages。比如系统环境里安装了MySQLdb模块，在虚拟环境里import MySQLdb会提示ImportError。如果想依赖系统环境的第三方软件包，可以使用参数–system-site-packages。<br><br><code>virtualenv --system-site-packages [虚拟环境名称]</code><br><br><a href="https://snailvfx.github.io/2016/05/11/virtualenv/" target="_blank" rel="noopener">参考链接:virtualenv虚拟环境</a><br><br> <font color="purple"><strong> 探索二、</strong>如何把models中的库添加到系统的python环境中:</font><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">  <span class="comment"># From tensorflow/models/research/</span></div><div class="line">export PYTHONPATH=$PYTHONPATH:<code>pwd</code>:<code>pwd</code>/slim</div></pre></td></tr></table></figure><p> pwd是指代当前目录的环境，需要替换过成自己的，比如自己在 ~/.bashrcw文件中这样声明的<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export PYTHONPATH=$PYTHONPATH:/data/tensorflowModels/models/research:/data/tensorflowModels/models/research/slim</div></pre></td></tr></table></figure></p><p> 接下来还要编译Protobuf<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"> <span class="comment"># From tensorflow/models/research/</span></div><div class="line">protoc object_detection/protos/*.proto –python_out=.</div></pre></td></tr></table></figure></p><p>然后，自己重新安装了tensorflow加载系统的环境变量还是出错。</p><p> <a href="https://github.com/tensorflow/models/blob/4f32535fe7040bb1e429ad0e3c948a492a89482d/research/object_detection/g3doc/installation.md" target="_blank" rel="noopener">参考链接:安装modles</a></p>  <font color="purple"><strong> 探索三、</strong>安装slim执行以下命令:</font><p> 在slim目录下执行下面命令</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">python setup.py build</div><div class="line">python setup.py install</div></pre></td></tr></table></figure><p> <a href="https://github.com/tensorflow/models/issues/2031" target="_blank" rel="noopener">参考：安装slim</a></p><p>接下来就可以使用了。</p><p><a href="http://rensanning.iteye.com/blog/2381885" target="_blank" rel="noopener">参考博客:TensorFlow之物体检测</a> </p>]]></content>
    
    <summary type="html">
    
      &lt;font size=&quot;5&quot; color=&quot;lightseagreen&quot;&gt; 这篇博客是总结自己在使用tensorflow models里面的物体检测的API的方法以及调试中遇到的错误及心得。&lt;/font&gt;
    
    </summary>
    
      <category term="工作" scheme="http://www.douxiao.org/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="tensorflow" scheme="http://www.douxiao.org/tags/tensorflow/"/>
    
      <category term="tfmodel" scheme="http://www.douxiao.org/tags/tfmodel/"/>
    
  </entry>
  
  <entry>
    <title>TensorflowPython代码学习笔记</title>
    <link href="http://www.douxiao.org/2017/11/04/TensorflowPython%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://www.douxiao.org/2017/11/04/TensorflowPython代码学习笔记/</id>
    <published>2017-11-04T06:32:48.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　最近在学习Tesorflow机器学习实战这本书，之前一直在学习深度学习相关的理论的知识，如果自己动手写代码，真的无从下手，这本书虽然理论知识很少，但是在tensorflow编程实战，是一个很不错的书，这篇博客是总结在写代码时自己的遇到的不会的地方，也可以说学习tensoflow和python编程笔记。<br><a id="more"></a></p><p>1、<code>train_indices = np.random.choice(len(x_vals), round(0.8*len(x_vals)), replace=False)</code><br>　　round(0.8<em>len(x_vals))该方法返回 0.8</em>len(x_vals)的小数点四舍五入到n个数字<br>   函数原型numpy.random.choice(a, size=None, replace=True, p=None)</p><ul><li>该函数的作用是从给定的一维数组中生成随机数</li><li>a为一维数组类似数据或整数；size为数组维度；p为数组中数据出现的概率</li><li>a为整数时，对应的一位数组为np.arange(a)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">x_val = np.random.choice(<span class="number">5</span>, <span class="number">3</span>, replace=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># 当replace为False时，生成随机数不能有重复的数值</span></span><br><span class="line">print(x_val)</span><br><span class="line">[<span class="number">1</span> <span class="number">0</span> <span class="number">4</span>]</span><br><span class="line">y_val = np.random.choice(<span class="number">5</span>,size=(<span class="number">3</span>,<span class="number">2</span>))</span><br><span class="line">print(y_val)</span><br><span class="line">[[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line"> [<span class="number">4</span>, <span class="number">2</span>],</span><br><span class="line"> [<span class="number">3</span>, <span class="number">3</span>]]</span><br><span class="line">demo_list = [<span class="string">'lenovo'</span>, <span class="string">'sansumg'</span>,<span class="string">'moto'</span>,<span class="string">'xiaomi'</span>, <span class="string">'iphone'</span>]</span><br><span class="line">np.random.choice(demo_list,size=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">array([[<span class="string">'moto'</span>, <span class="string">'iphone'</span>, <span class="string">'xiaomi'</span>],</span><br><span class="line">       [<span class="string">'lenovo'</span>, <span class="string">'xiaomi'</span>, <span class="string">'xiaomi'</span>],</span><br><span class="line">       [<span class="string">'xiaomi'</span>, <span class="string">'lenovo'</span>, <span class="string">'iphone'</span>]],</span><br><span class="line">      dtype=<span class="string">'&lt;U7'</span>)</span><br><span class="line"><span class="comment"># 参数p的长度与参数a的长度需要一致； </span></span><br><span class="line"><span class="comment"># 参数p为概率，p里的数据之和应为1</span></span><br><span class="line">demo_list = [<span class="string">'lenovo'</span>, <span class="string">'sansumg'</span>,<span class="string">'moto'</span>,<span class="string">'xiaomi'</span>, <span class="string">'iphone'</span>]</span><br><span class="line">np.random.choice(demo_list,size=(<span class="number">3</span>,<span class="number">3</span>), p=[<span class="number">0.1</span>,<span class="number">0.6</span>,<span class="number">0.1</span>,<span class="number">0.1</span>,<span class="number">0.1</span>])</span><br><span class="line">array([[<span class="string">'sansumg'</span>, <span class="string">'sansumg'</span>, <span class="string">'sansumg'</span>],</span><br><span class="line">       [<span class="string">'sansumg'</span>, <span class="string">'sansumg'</span>, <span class="string">'sansumg'</span>],</span><br><span class="line">       [<span class="string">'sansumg'</span>, <span class="string">'xiaomi'</span>, <span class="string">'iphone'</span>]],</span><br><span class="line">      dtype=<span class="string">'&lt;U7'</span>)</span><br></pre></td></tr></table></figure><p>２、 numpy.random.seed()</p><ul><li>np.random.seed()的作用：使得随机数据可预测。</li><li>当我们设置相同的seed，每次生成的随机数相同。如果不设置seed，则每次会生成不同的随机数。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">np.random.rand(<span class="number">5</span>)</span><br><span class="line">array([ <span class="number">0.5488135</span> ,  <span class="number">0.71518937</span>,  <span class="number">0.60276338</span>,  <span class="number">0.54488318</span>,  <span class="number">0.4236548</span> ])</span><br><span class="line">np.random.seed(<span class="number">1676</span>)</span><br><span class="line">np.random.rand(<span class="number">5</span>)</span><br><span class="line">array([ <span class="number">0.39983389</span>,  <span class="number">0.29426895</span>,  <span class="number">0.89541728</span>,  <span class="number">0.71807369</span>,  <span class="number">0.3531823</span> ])</span><br><span class="line">np.random.seed(<span class="number">1676</span>)</span><br><span class="line">np.random.rand(<span class="number">5</span>)</span><br><span class="line">array([ <span class="number">0.39983389</span>,  <span class="number">0.29426895</span>,  <span class="number">0.89541728</span>,  <span class="number">0.71807369</span>,  <span class="number">0.3531823</span> ])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　最近在学习Tesorflow机器学习实战这本书，之前一直在学习深度学习相关的理论的知识，如果自己动手写代码，真的无从下手，这本书虽然理论知识很少，但是在tensorflow编程实战，是一个很不错的书，这篇博客是总结在写代码时自己的遇到的不会的地方，也可以说学习tensoflow和python编程笔记。&lt;br&gt;
    
    </summary>
    
      <category term="工作" scheme="http://www.douxiao.org/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="python" scheme="http://www.douxiao.org/tags/python/"/>
    
      <category term="tensorflow" scheme="http://www.douxiao.org/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>surface及surfaceview使用总结</title>
    <link href="http://www.douxiao.org/2017/10/30/surface%E5%8F%8Asurfaceview%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/"/>
    <id>http://www.douxiao.org/2017/10/30/surface及surfaceview使用总结/</id>
    <published>2017-10-30T01:20:21.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>　<font size="5" color="lightseagreen">这篇博客是关于android中的surface、surfaceView以及hoder的使用总结。<br> </font><br><a id="more"></a></p><h1 id="一、什么是Surface"><a href="#一、什么是Surface" class="headerlink" title="一、什么是Surface"></a><a href="#一、什么是Surface" title="一、什么是Surface"></a>一、什么是Surface</h1><p>　　简单的说Surface对应了一块屏幕缓冲区，每个window对应一个Surface，任何View都要画在Surface的Canvas上（后面有原因解释）。传统的view共享一块屏幕缓冲区，所有的绘制必须在UI线程中进行。<br>　　<font color="blue">Surface中的Canvas成员，是专门用于供程序员画图的场所，就像黑板一样；其中的原始缓冲区是用来保存数据的地方；Surface本身的作用类似一个句柄，得到了这个句柄就可以得到其中的Canvas、原始缓冲区以及其它方面的内容。</font></p><h1 id="二、什么是SurfaceView"><a href="#二、什么是SurfaceView" class="headerlink" title="二、什么是SurfaceView"></a><a href="#二、什么是SurfaceView" title="二、什么是SurfaceView"></a>二、什么是SurfaceView</h1><p>　　说SurfaceView是一个View也许不够严谨，然而从定义中pubilc class　SurfaceView extends View{…..}显示SurfaceView确实是派生自View，但是SurfaceView却有自己的Surface，请看SurfaceView的源码。<br>　　SurfaceView就是展示Surface中数据的地方，同时可以认为SurfaceView是用来控制Surface中View的位置和尺寸的。</p><h1 id="三、什么是SufaceHolder"><a href="#三、什么是SufaceHolder" class="headerlink" title="三、什么是SufaceHolder"></a><a href="#三、什么是SufaceHolder" title="三、什么是SufaceHolder"></a>三、什么是SufaceHolder</h1><p>　　SurfaceHolder是一个接口，其作用就像一个关于Surface的监听器，提供访问和控制SurfaceView内嵌的Surface 相关的方法。它通过三个回调方法，让我们可以感知到Surface的创建、销毁或者改变。</p><font color="red">　　 总结：从设计模式的高度来看，Surface、SurfaceView和SurfaceHolder实质上就是广为人知的MVC，即Model-View-Controller。Model就是模型的意思，或者说是数据模型，或者更简单地说就是数据，也就是这里的Surface；View即视图，代表用户交互界面，也就是这里的SurfaceView；SurfaceHolder很明显可以理解为MVC中的Controller（控制器）。</font><h1 id="四、什么是SurfaceHolder-Callback"><a href="#四、什么是SurfaceHolder-Callback" class="headerlink" title="四、什么是SurfaceHolder.Callback"></a><a href="#四、什么是SurfaceHolder-Callback" title="四、什么是SurfaceHolder.Callback"></a>四、什么是SurfaceHolder.Callback</h1><p>　　SurfaceHolder.Callback主要是当底层的Surface被创建、销毁或者改变时提供回调通知，由于绘制必须在Surface被创建后才能进行，因此SurfaceHolder.Callback中的surfaceCreated 和surfaceDestroyed 就成了绘图处理代码的边界。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　&lt;font size=&quot;5&quot; color=&quot;lightseagreen&quot;&gt;这篇博客是关于android中的surface、surfaceView以及hoder的使用总结。&lt;br&gt; &lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="工作" scheme="http://www.douxiao.org/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="android" scheme="http://www.douxiao.org/tags/android/"/>
    
  </entry>
  
  <entry>
    <title>Nvidia-TX1刷机教程</title>
    <link href="http://www.douxiao.org/2017/10/26/Nvidia-TX1%E5%88%B7%E6%9C%BA%E6%95%99%E7%A8%8B/"/>
    <id>http://www.douxiao.org/2017/10/26/Nvidia-TX1刷机教程/</id>
    <published>2017-10-26T00:30:16.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　这篇博客是总结这几天Nvidia Tx1开发板的刷机血泪史，总结遇到的一些问题及解决办法，之所以艰难是因为实验室的网络太差，还有最近十九大的召开，国内所有的vpn都被禁了，下载某些安装包是很困难的，没法google，百度有些问题的解决办法是无法找到的,不过也参考了一些前辈的教程。不过值得庆幸的事，最终还是刷机成功了，写一篇博文来纪念一下。<br><a id="more"></a></p><h1 id="一、开箱检查，并安装系统"><a href="#一、开箱检查，并安装系统" class="headerlink" title="一、开箱检查，并安装系统"></a>一、开箱检查，并安装系统</h1><p>　　开发板连接好AC电源线，<strong>使用HDMI线连接显示器</strong>，自己在这里踩坑了，因为开始用的hdmi转vga，显示器怎么都不显示，用HDMI直连显示器就可以了，开始我的板子会进入命令行界面，开始还没有安装系统，所以没有桌面，需要运行下安装。<br>　　然后登录系统，用户名和密码都是”ubuntu“，登陆后，系统会提示安装驱动以显示正常的图像界面。按照要求进行操作，3步以后驱动就安好了，然后sudo reboot重启系统，就可以进入ubuntu 14.04系统界面。终端输入sudo lshw就可以查看系统硬件信息，基本可以确定开发板的完好。<br>　　<font color="red">注意:开始这里必须用hdmi线直连显示器。</font></p><h1 id="二、刷机准备，下载安装JetPack3-1"><a href="#二、刷机准备，下载安装JetPack3-1" class="headerlink" title="二、刷机准备，下载安装JetPack3.1"></a>二、刷机准备，下载安装JetPack3.1</h1><p>　  开发板安装软件的一个方式是使用JetPack,Jetson TX1出厂时默认的系统以及附加包都比较老旧,没有安装cuda,nvcc,opencv等，部署最新的开发包可以充分利用硬件性能，有利于我们进行深度学习开发。Jetpack3.1是Nvidia提供的最新开发包，包含64bit的Ubuntu16.04操作系统，CUDA 8.0，cuDNN 6.0等。类似于刷安卓手机，我们需要在一台装有ubuntu14.04或16.04的电脑上为开发板更新固件。</p><h2 id="第一步："><a href="#第一步：" class="headerlink" title="第一步："></a>第一步：</h2><p>　　　下载JetPack3.1，需要注册英伟达开发者账号，<a href="https://developer.nvidia.com/embedded/jetpack" target="_blank" rel="noopener">官网链接</a>，<a href="https://developer.nvidia.com/embedded/downloads" target="_blank" rel="noopener">JetPack Download  center</a>。<br>   <img src="http://ow7va355d.bkt.clouddn.com/4.png" alt=""></p><h2 id="第二步："><a href="#第二步：" class="headerlink" title="第二步："></a>第二步：</h2><p>　　　安装JetPack3.1，因为安装需要大概10G左右的空间，所以我没有放在系统的那个盘安装。加入我放在data盘中。<br>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /data</span><br><span class="line">mkdir NvidiaTx1 //然后把下载的文件拷贝到该文件夹的目录下面。</span><br><span class="line">cd NvidiaTx1</span><br><span class="line">./JetPack-L4T<span class="number">-3.1</span>-linux-x64.run</span><br></pre></td></tr></table></figure></p><p>　　这里会进入安装界面，点next,选择TX1,会进入到下面那个界面，因为实验室网络经常抽风，经常会在下面那个界面卡死报错，解决办法重复进几次，自己有一次尝试打开vpn居然就能用，还有把4G网共享给电脑，总之试了很多办法。如果运气好的话，会很顺利完成的。</p><p>  <img src="http://ow7va355d.bkt.clouddn.com/Jetpack_install3.png" alt=""></p><p>　　应该是下面的这个效果，当然自己写教程前已经安装好了，所以在主机部分显示的是no action,正常的应该显示install版本，接下来的下载将是一个漫长的，不断失败的过程，我也搞不懂为什么反正有种绝望想放弃的感觉。</p><p>  <img src="http://ow7va355d.bkt.clouddn.com/Jetpack_install4.png" alt=""><br>　　<br>　　好了点了next,就去忙别的吧，当然下载错误还是那些办法，重新开始，换网。。。。。。<br>　　经过了漫长的等待下载完成。</p><h2 id="第三步："><a href="#第三步：" class="headerlink" title="第三步："></a>第三步：</h2><p>　　安装：全部下载完后，开始了每一项的安装，此时可能会报出cuda安装失败的错误，此时查看日志文件，多半能找到答案。我的做法是打开终端，运行<code>sudo apt-get -f install</code>命令，补全依赖项，然后就可以顺利安装。</p><h1 id="三、开始刷机"><a href="#三、开始刷机" class="headerlink" title="三、开始刷机"></a>三、开始刷机</h1><p>　　刚才开发板所需组件全部下载并安装后，就可以准备刷机了。</p><h2 id="第一步：配置网络"><a href="#第一步：配置网络" class="headerlink" title="第一步：配置网络"></a>第一步：配置网络</h2><p>　　开发板刷机过程中需要全程联网，那么官方推荐的做法就是把电脑与开发板用网线连在同一个路由器下，至于无线连接行不行我没试过，不过为了保证稳定，建议使用网线。那么在弹出的network layout配置中选择路由连接；在network interface中选择以太网端口，不认识的话就用默认选项。<br>  <img src="http://ow7va355d.bkt.clouddn.com/10.png" alt=""><br>  <img src="http://ow7va355d.bkt.clouddn.com/11.png" alt=""><br>  <img src="http://ow7va355d.bkt.clouddn.com/12.png" alt=""><br>  <img src="http://ow7va355d.bkt.clouddn.com/13.png" alt=""></p><h2 id="第二步、开发板连接到电脑，开始刷机"><a href="#第二步、开发板连接到电脑，开始刷机" class="headerlink" title="第二步、开发板连接到电脑，开始刷机"></a>第二步、开发板连接到电脑，开始刷机</h2><p>　1.断开电源，保证开发板处于断电关机状态。<br>　2.用网线连到路由器上，也可插上鼠标键盘。<br>　3.用Micro USB线把开发板连到电脑上（类似于安卓手机连电脑）接通AC电　源，按下power键，开机。<br>　4.长按rec键不松开，然后点按一下reset键(这里会看到灯闪一下)，过2s以后，才松开rec键，此时开发板处于强制恢复模式。</p><p>　　完成以上步骤后，我们还要检查开发板有没有和电脑正确连接，终端输入lsusb 命令，可以看到一些列表，只要发现其中有Nvidia Corp就说明连接正确。<br>　　以上步骤确认无误后，在post installation界面中敲一下enter，就开始了刷机过程。<br>　　<font color="red"> <strong>注意在这里自己遇到一个错误。Making   　　　system.img…<br>　　　rm: cannot remove ‘mnt’: Device or resource busy<br>　　　clearing ext4 mount point failed.</strong></font><br>　　这里需要手动清楚 /mnt文件及目录。<a href="https://devtalk.nvidia.com/default/topic/1021063/jetson-tx2/failed-to-flash-device/post/5197174/" target="_blank" rel="noopener">解决办法链接</a>。</p><p>　　好了下面可以进行正常刷机了。</p><p>  <img src="http://ow7va355d.bkt.clouddn.com/14.png" alt=""><br>　　<br>　　刷机过程中，会出现提示确认GUI桌面是否安装好，此时用HDMI线缆连到显示器上，如果显示ubuntu桌面，说明系统安装好了，按照提示完成后续安装，这将是一个持续几十分钟的过程。完全安好后，退出Jetpack软件即可。<br><strong>　　自己这里遇到的问题是检测不到tx1的IP。只能退出，下面讲解怎样使用JetPack仅安装组件。，如果这个过程很慢而且出错的话那只能手动安装cuda ,openv 等组件了。</strong></p><h1 id="四、JetPack仅安装组件"><a href="#四、JetPack仅安装组件" class="headerlink" title="四、JetPack仅安装组件"></a>四、JetPack仅安装组件</h1><p>　　这就得提到Jetpack的另一个特性：可以不必刷机，单独为Jetson设备安装任何组件。方法其实很简单： </p><p>  　<img src="http://ow7va355d.bkt.clouddn.com/Jetpack_install4.png" alt=""></p><p>   类似上图，把FileSystem and OS、Drivers、Flash OS image to Target这些关于系统的组件通通置为no action，然后选择需要补充安装的组件，注意它们的依赖关系。选择完毕就点next，会出现如下界面，需要填上Tx1的IP： </p><p> <img src="http://ow7va355d.bkt.clouddn.com/IPinstall.png" alt=""></p><p>   就可以开始安装了。这里使用的是SSH远程服务，根本不用数据线，等待一会就安装好了，注意安装过程中尽量不要操作开发板。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　这篇博客是总结这几天Nvidia Tx1开发板的刷机血泪史，总结遇到的一些问题及解决办法，之所以艰难是因为实验室的网络太差，还有最近十九大的召开，国内所有的vpn都被禁了，下载某些安装包是很困难的，没法google，百度有些问题的解决办法是无法找到的,不过也参考了一些前辈的教程。不过值得庆幸的事，最终还是刷机成功了，写一篇博文来纪念一下。&lt;br&gt;
    
    </summary>
    
      <category term="工作" scheme="http://www.douxiao.org/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="Nvidia-Tx1" scheme="http://www.douxiao.org/tags/Nvidia-Tx1/"/>
    
      <category term="嵌入式GPU" scheme="http://www.douxiao.org/tags/%E5%B5%8C%E5%85%A5%E5%BC%8FGPU/"/>
    
  </entry>
  
  <entry>
    <title>让ubuntu下的eclipse支GBK编码</title>
    <link href="http://www.douxiao.org/2017/10/13/%E8%AE%A9ubuntu%E4%B8%8B%E7%9A%84eclipse%E6%94%AFGBK%E7%BC%96%E7%A0%81/"/>
    <id>http://www.douxiao.org/2017/10/13/让ubuntu下的eclipse支GBK编码/</id>
    <published>2017-10-13T05:51:29.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>　<font size="5" color="lightseagreen">  今天，把windows下的工程导入到了Linux下eclipse中，由于以前的工程代码，都是GBK编码的，而Ubuntu默认是不支持GBK编码的。所以，首先我们要先让Ubuntu支持GBK，方法如下:</font><br><a id="more"></a></p><p>1、修改/var/lib/locales/supported.d的权限<br>  <code>sudo chmod -R 777 /var/lib/locales/supported.d</code><br>2、打开/var/lib/locales/supported.d/local文件,在文件中添加<br>（打开文件命令为<code>gedit /var/lib/locales/supported.d/local</code>）<br>     zh_CN.GBK GBK<br>     zh_CN.GB2312 GB2312<br>3、在终端输入<br>  <code>sudo dpkg-reconfigure --force locales</code><br>4、设置eclipse:Windows-&gt;Preferences-&gt;General-&gt;Workspace在Text file encoding下选择other在后面的文本框中输入GBK点击应用即可 ,如果没有GBK的选项， 没关系，直接输入GBK三个字母，Apply，GBK编码的中文，已经不是乱码了。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　&lt;font size=&quot;5&quot; color=&quot;lightseagreen&quot;&gt;  今天，把windows下的工程导入到了Linux下eclipse中，由于以前的工程代码，都是GBK编码的，而Ubuntu默认是不支持GBK编码的。所以，首先我们要先让Ubuntu支持GBK，方法如下:&lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="工作" scheme="http://www.douxiao.org/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="GBK编码" scheme="http://www.douxiao.org/tags/GBK%E7%BC%96%E7%A0%81/"/>
    
      <category term="eclipse" scheme="http://www.douxiao.org/tags/eclipse/"/>
    
  </entry>
  
  <entry>
    <title>深度学习——改善深度神经网络</title>
    <link href="http://www.douxiao.org/2017/10/13/DeepLearning-ai%E8%AF%BE%E7%A8%8B%E2%80%94%E2%80%94%E6%94%B9%E5%96%84%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://www.douxiao.org/2017/10/13/DeepLearning-ai课程——改善深度神经网络/</id>
    <published>2017-10-13T05:51:29.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>　这篇博客内容是总结DeepLearning.ai课程第二课Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization内容。第二课分为四周，第一周是深度学习的实践方面bias，variance，regularization，Vanishing or Exploding gradients，gradient checking；第二周是优化算法Gradient Descent，Momentum，RMSProp and Adam；第三周是超参数调优，批量归一化，深度学习框架。<br><a id="more"></a></p><h1 id="一、初始化"><a href="#一、初始化" class="headerlink" title="一、初始化"></a>一、初始化</h1><h2 id="１、前言"><a href="#１、前言" class="headerlink" title="　　１、前言"></a>　　１、前言</h2><p>　　一个良好的初始化结果能够：加速梯度下降的收敛和增加梯度下降收敛到一个较低的训练（和泛化）误差。下面是构建一个分类器分离下图中的蓝点和红点：<br>  　　　　　　　<img src="http://ow7va355d.bkt.clouddn.com/data1.png" alt=""></p><h2 id="２、零初始化"><a href="#２、零初始化" class="headerlink" title="　　２、零初始化"></a>　　２、零初始化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters_zeros </span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_zeros</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the size of each layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layers_dims[1], 1)</span></span><br><span class="line"><span class="string">                    ...</span></span><br><span class="line"><span class="string">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span></span><br><span class="line"><span class="string">                    bL -- bias vector of shape (layers_dims[L], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims)            <span class="comment"># number of layers in the network</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.zeros((layers_dims[l],layers_dims[l<span class="number">-1</span>]))</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>分类效果如下图：</p><p><img src="http://ow7va355d.bkt.clouddn.com/data3.png" alt=""><br><img src="http://ow7va355d.bkt.clouddn.com/data2.png" alt=""><br>　　通常，将所有权重初始化为零会导致网络不能破坏对称性。 这意味着每一层的每个神经元都会学习同样的东西。</p><h2 id="３、随机初始化"><a href="#３、随机初始化" class="headerlink" title="　　３、随机初始化"></a>　　３、随机初始化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters_random</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_random</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the size of each layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layers_dims[1], 1)</span></span><br><span class="line"><span class="string">                    ...</span></span><br><span class="line"><span class="string">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span></span><br><span class="line"><span class="string">                    bL -- bias vector of shape (layers_dims[L], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)               <span class="comment"># This seed makes sure your "random" numbers will be the as ours</span></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims)            <span class="comment"># integer representing the number of layers</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l],layers_dims[l<span class="number">-1</span>])*<span class="number">10</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>分类效果如下：<br><img src="http://ow7va355d.bkt.clouddn.com/data5.png" alt=""></p><p>总结：<br>　　把权重初始化非常大的随机数不会起到很好的效果<br>  　希望初始化权重到更小的随机数</p><h2 id="４、He初始化"><a href="#４、He初始化" class="headerlink" title="　　４、He初始化"></a>　　４、He初始化</h2><p><img src="http://ow7va355d.bkt.clouddn.com/He%20initialization.png" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters_he</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_he</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the size of each layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layers_dims[1], 1)</span></span><br><span class="line"><span class="string">                    ...</span></span><br><span class="line"><span class="string">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span></span><br><span class="line"><span class="string">                    bL -- bias vector of shape (layers_dims[L], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims) - <span class="number">1</span> <span class="comment"># integer representing the number of layers</span></span><br><span class="line">     </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l],layers_dims[l<span class="number">-1</span>])*np.sqrt(<span class="number">2.</span>/layers_dims[l<span class="number">-1</span>])</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p><p>效果如下：<br><img src="http://ow7va355d.bkt.clouddn.com/data6.png" alt=""></p><h2 id="５、结论"><a href="#５、结论" class="headerlink" title="　　５、结论"></a>　　５、结论</h2><p><img src="http://ow7va355d.bkt.clouddn.com/Conclusions.png" alt=""></p><h1 id="二、正则化"><a href="#二、正则化" class="headerlink" title="二、正则化"></a>二、正则化</h1><h2 id="１、前言-1"><a href="#１、前言-1" class="headerlink" title="　　１、前言"></a>　　１、前言</h2><p>　　深度学习模型有非常大的灵活性和容量，以至于过拟合是非常严重的问题，如果训练的数据集不是足够的大，必定在训练集上有很好的表现，但是训练好的神经网络不能泛化他没有见到过得实例。<br>  　在本节中将会学到：使用正则化在深度学习的模型中。</p><h2 id="２、使用的数据集"><a href="#２、使用的数据集" class="headerlink" title="　　２、使用的数据集"></a>　　２、使用的数据集</h2><p><img src="http://ow7va355d.bkt.clouddn.com/data7.png" alt=""><br>　　数据集的分析：这个数据集有点嘈杂，但是看起来像将左上半部分（蓝色）与右下半部分（红色）分开的对角线将很好地工作。</p><h2 id="３、模型定义"><a href="#３、模型定义" class="headerlink" title="　　３、模型定义"></a>　　３、模型定义</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, learning_rate = <span class="number">0.3</span>, num_iterations = <span class="number">30000</span>, print_cost = True, lambd = <span class="number">0</span>, keep_prob = <span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    print_cost -- If True, print the cost every 10000 iterations</span></span><br><span class="line"><span class="string">    lambd -- regularization hyperparameter, scalar</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learned by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        </span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []                            <span class="comment"># to keep track of the cost</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                        <span class="comment"># number of examples</span></span><br><span class="line">    layers_dims = [X.shape[<span class="number">0</span>], <span class="number">20</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters dictionary.</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        <span class="keyword">if</span> keep_prob == <span class="number">1</span>:</span><br><span class="line">            a3, cache = forward_propagation(X, parameters)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost function</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span>:</span><br><span class="line">            cost = compute_cost(a3, Y)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        <span class="keyword">assert</span>(lambd==<span class="number">0</span> <span class="keyword">or</span> keep_prob==<span class="number">1</span>)    <span class="comment"># it is possible to use both L2 regularization and dropout, </span></span><br><span class="line">                                            <span class="comment"># but this assignment will only explore one at a time</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span> <span class="keyword">and</span> keep_prob == <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation(X, Y, cache)</span><br><span class="line">        <span class="keyword">elif</span> lambd != <span class="number">0</span>:</span><br><span class="line">            grads = backward_propagation_with_regularization(X, Y, cache, lambd)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the loss every 10000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (x1,000)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="４、未使用正则化"><a href="#４、未使用正则化" class="headerlink" title="　　４、未使用正则化"></a>　　４、未使用正则化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y)</span><br><span class="line">print (&quot;On the training set:&quot;)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line">print (&quot;On the test set:&quot;)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><p><img src="http://ow7va355d.bkt.clouddn.com/data8.png" alt=""><br><img src="http://ow7va355d.bkt.clouddn.com/data9.png" alt=""></p><h2 id="５、L2正则化"><a href="#５、L2正则化" class="headerlink" title="　　５、L2正则化"></a>　　５、L2正则化</h2><p><img src="http://ow7va355d.bkt.clouddn.com/%20L2%20Regularization.png" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_cost_with_regularization</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost_with_regularization</span><span class="params">(A3, Y, parameters, lambd)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function with L2 regularization. See formula (2) above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing parameters of the model</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - value of the regularized loss function (formula (2))</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    </span><br><span class="line">    cross_entropy_cost = compute_cost(A3, Y) <span class="comment"># This gives you the cross-entropy part of the cost</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    L2_regularization_cost = (np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))*(<span class="number">1</span>/m)*(lambd/<span class="number">2</span>)</span><br><span class="line">    <span class="comment">### END CODER HERE ###</span></span><br><span class="line">    </span><br><span class="line">    cost = cross_entropy_cost + L2_regularization_cost</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure></p><p>反向传播的正则化<br><img src="http://ow7va355d.bkt.clouddn.com/data10.png" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: backward_propagation_with_regularization</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_regularization</span><span class="params">(X, Y, cache, lambd)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward propagation of our baseline model to which we added an L2 regularization.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation()</span></span><br><span class="line"><span class="string">    lambd -- regularization hyperparameter, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T) + W3*(lambd/m)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T) +  W2*(lambd/m)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T) +  W1*(lambd/m)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br><span class="line">    </span><br><span class="line">parameters = model(train_X, train_Y, lambd = <span class="number">0.7</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure></p><p><img src="http://ow7va355d.bkt.clouddn.com/data11.png" alt=""><br><img src="http://ow7va355d.bkt.clouddn.com/data13.png" alt=""></p><p><strong>观察：</strong><br>　　能够用验证集优化λ的值。<br>　　L2正则化能够使决策边界变得更加光滑。如果λ的值的过大，很可能边界变得过度光滑，导致模型的高偏差。</p><p><strong>L2正则化作用：</strong><br>　　L2正则化依赖于这样一个假设，具有小权重的模型比具有较大权重的模型更加简单。因此，通过惩罚代价函数中的权重的平方值，使得所有的权重变成更小的值，这使得输出的变化变得更加平滑。</p><p> What you should remember – the implications of L2-regularization on:</p><pre><code>The cost computation:    A regularization term is added to the costThe backpropagation function:    There are extra terms in the gradients with respect to weight matricesWeights end up smaller (&quot;weight decay&quot;):    Weights are pushed to smaller values.</code></pre><h2 id="６、Dropout正则化"><a href="#６、Dropout正则化" class="headerlink" title="　　６、Dropout正则化"></a>　　６、Dropout正则化</h2><center> <img src="http://ow7va355d.bkt.clouddn.com/dropout1_kiank.gif" width="80%" height="60%"></center><p>　　At each iteration, you shut down (= set to zero) each neuron of a layer with probability 1−keep_prob or keep it with probability keep_prob(50% here). The dropped neurons don’t contribute to the training in both the forward and backward propagations of the iteration. </p><center> <img src="http://ow7va355d.bkt.clouddn.com/dropout2_kiank.gif" width="80%" height="60%"></center><h3 id="１）带有dropout的前向传播"><a href="#１）带有dropout的前向传播" class="headerlink" title="　　１）带有dropout的前向传播"></a>　　１）带有dropout的前向传播</h3><p><img src="http://ow7va355d.bkt.clouddn.com/dropout.png" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: forward_propagation_with_dropout</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_with_dropout</span><span class="params">(X, parameters, keep_prob = <span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (20, 2)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (20, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (3, 20)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (3, 1)</span></span><br><span class="line"><span class="string">                    W3 -- weight matrix of shape (1, 3)</span></span><br><span class="line"><span class="string">                    b3 -- bias vector of shape (1, 1)</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A3 -- last activation value, output of the forward propagation, of shape (1,1)</span></span><br><span class="line"><span class="string">    cache -- tuple, information stored for computing the backward propagation</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># retrieve parameters</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    b3 = parameters[<span class="string">"b3"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. </span></span><br><span class="line">    D1 = np.random.rand(A1.shape[<span class="number">0</span>],A1.shape[<span class="number">1</span>])                                         <span class="comment"># Step 1: initialize matrix D1 = np.random.rand(..., ...)</span></span><br><span class="line">    D1 = D1&lt;keep_prob                                       <span class="comment"># Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span></span><br><span class="line">    A1 = np.multiply(A1,D1)                                        <span class="comment"># Step 3: shut down some neurons of A1</span></span><br><span class="line">    A1 = A1/keep_prob                                         <span class="comment"># Step 4: scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 4 lines)</span></span><br><span class="line">    D2 = np.random.rand(A2.shape[<span class="number">0</span>],A2.shape[<span class="number">1</span>])                                         <span class="comment"># Step 1: initialize matrix D2 = np.random.rand(..., ...)</span></span><br><span class="line">    D2 = D2&lt;keep_prob                                           <span class="comment"># Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)</span></span><br><span class="line">    A2 = np.multiply(A2,D2)                                        <span class="comment"># Step 3: shut down some neurons of A2</span></span><br><span class="line">    A2 = A2/keep_prob                                         <span class="comment"># Step 4: scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A3, cache</span><br><span class="line"></span><br><span class="line">```  </span><br><span class="line"><span class="comment">### 　　１）带有dropout的反向传播</span></span><br><span class="line"></span><br><span class="line">![](http://ow7va355d.bkt.clouddn.com/dropout1.png)</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="comment"># GRADED FUNCTION: backward_propagation_with_dropout</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_dropout</span><span class="params">(X, Y, cache, keep_prob)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward propagation of our baseline model to which we added dropout.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation_with_dropout()</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dA2 = dA2*D2              <span class="comment"># Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation</span></span><br><span class="line">    dA2 = dA2/keep_prob              <span class="comment"># Step 2: Scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dA1 = dA1*D1              <span class="comment"># Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span></span><br><span class="line">    dA1 = dA1/keep_prob              <span class="comment"># Step 2: Scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br><span class="line">    </span><br><span class="line">parameters = model(train_X, train_Y, keep_prob = <span class="number">0.86</span>, learning_rate = <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure></p><p><img src="http://ow7va355d.bkt.clouddn.com/dropout2.png" alt=""><br><img src="http://ow7va355d.bkt.clouddn.com/dropout3.png" alt=""></p><p> What you should remember about dropout:</p><ul><li>Dropout is a regularization technique.</li><li>You only use dropout during training. Don’t use dropout (randomly eliminate nodes) during test time.</li><li>Apply dropout both during forward and backward propagation.</li><li>During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.</li></ul><h2 id="７、结论"><a href="#７、结论" class="headerlink" title="　　７、结论"></a>　　７、结论</h2><p><img src="http://ow7va355d.bkt.clouddn.com/dropout4.png" alt=""></p><h1 id="三、梯度检查"><a href="#三、梯度检查" class="headerlink" title="三、梯度检查"></a>三、梯度检查</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　这篇博客内容是总结DeepLearning.ai课程第二课Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization内容。第二课分为四周，第一周是深度学习的实践方面bias，variance，regularization，Vanishing or Exploding gradients，gradient checking；第二周是优化算法Gradient Descent，Momentum，RMSProp and Adam；第三周是超参数调优，批量归一化，深度学习框架。&lt;br&gt;
    
    </summary>
    
      <category term="工作" scheme="http://www.douxiao.org/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="DeepLearning" scheme="http://www.douxiao.org/tags/DeepLearning/"/>
    
      <category term="神经网络" scheme="http://www.douxiao.org/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>DeepLearning.ai课程——神经网络和深度学习</title>
    <link href="http://www.douxiao.org/2017/10/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    <id>http://www.douxiao.org/2017/10/12/深度学习——神经网络和深度学习/</id>
    <published>2017-10-12T06:16:05.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　这篇博客内容是总结DeepLearning.ai课程第一课Neural Networks and Deep Learning内容。第一课分为四周，第一周是深度学习的介绍；第二周是神经网络的基础；第三周是浅层神经网络；第四周是深层神经网络。<br><a id="more"></a></p><h1 id="一、Numpy使用"><a href="#一、Numpy使用" class="headerlink" title="一、Numpy使用"></a>一、Numpy使用</h1><h2 id="1、使用numpy构建sigmoid函数"><a href="#1、使用numpy构建sigmoid函数" class="headerlink" title="　　1、使用numpy构建sigmoid函数"></a>　　1、使用numpy构建sigmoid函数</h2><p>  　　　<img src="http://ow7va355d.bkt.clouddn.com/sigmoid.png" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sigmoid</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># this means you can access numpy functions by writing np.function() instead of numpy.function()</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of x</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array of any size</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">sigmoid(x)</span><br><span class="line">array([ <span class="number">0.73105858</span>,  <span class="number">0.88079708</span>,  <span class="number">0.95257413</span>])</span><br></pre></td></tr></table></figure></p><h2 id="2、sigmoid函数的导数"><a href="#2、sigmoid函数的导数" class="headerlink" title="　　2、sigmoid函数的导数"></a>　　2、sigmoid函数的导数</h2><p>　　　　sigmoid_derivative(x)=σ′(x)=σ(x)(1−σ(x))</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sigmoid_derivative</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_derivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.</span></span><br><span class="line"><span class="string">    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    ds -- Your computed gradient.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line">    ds = s*(<span class="number">1</span>-s)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> ds</span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sigmoid_derivative(x) = "</span> + str(sigmoid_derivative(x)))</span><br><span class="line">sigmoid_derivative(x) = [ <span class="number">0.19661193</span>  <span class="number">0.10499359</span>  <span class="number">0.04517666</span>]</span><br></pre></td></tr></table></figure><h2 id="3、Reshaping-数组"><a href="#3、Reshaping-数组" class="headerlink" title="　　3、Reshaping 数组"></a>　　3、Reshaping 数组</h2><p>　　　　常用的两个函数　np.shape 和 np.reshape<br>　　　　X.shape是用来得到矩阵或向量X的维度。<br>　　　　X.reshape()是把X重塑成新的维度。</p><p>　　　　<img src="http://ow7va355d.bkt.clouddn.com/shape.png" alt=""><br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment"># GRADED FUNCTION: image2vector</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">image2vector</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    image -- a numpy array of shape (length, height, depth)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- a vector of shape (length*height*depth, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    v = image.reshape((image.shape[<span class="number">0</span>]*image.shape[<span class="number">1</span>]*image.shape[<span class="number">2</span>]),<span class="number">1</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure></p><h2 id="4、归一化矩阵中的每一行"><a href="#4、归一化矩阵中的每一行" class="headerlink" title="　　4、归一化矩阵中的每一行"></a>　　4、归一化矩阵中的每一行</h2><p>  　　　　在归一化之后梯度下降会变得更快</p><p>  <img src="http://ow7va355d.bkt.clouddn.com/%E5%BD%92%E4%B8%80%E5%8C%96.png" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: normalizeRows</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeRows</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement a function that normalizes each row of the matrix x (to have unit length).</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    x -- A numpy matrix of shape (n, m)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    x -- The normalized (by row) numpy matrix. You are allowed to modify x.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    <span class="comment"># Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)</span></span><br><span class="line">    x_norm = np.linalg.norm(x,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># Divide x by its norm.</span></span><br><span class="line">    x = x/x_norm</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p><h2 id="5、广播和softmax函数"><a href="#5、广播和softmax函数" class="headerlink" title="　　5、广播和softmax函数"></a>　　5、广播和softmax函数</h2><p>  <img src="http://ow7va355d.bkt.clouddn.com/softmax.png" alt=""><br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment"># GRADED FUNCTION: softmax</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Calculates the softmax for each row of the input x.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Your code should work for a row vector and also for matrices of shape (n, m).</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    x -- A numpy matrix of shape (n,m)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    s -- A numpy matrix equal to the softmax of x, of shape (n,m)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    <span class="comment"># Apply exp() element-wise to x. Use np.exp(...).</span></span><br><span class="line">    x_exp = np.exp(x)</span><br><span class="line">    <span class="comment"># Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).</span></span><br><span class="line">    x_sum = np.sum(x_exp,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.</span></span><br><span class="line">    s = x_exp/x_sum</span><br><span class="line">   <span class="comment">### END CODE HERE ###</span></span><br><span class="line">   <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure></p><h2 id="6、实现L1和L2损失函数"><a href="#6、实现L1和L2损失函数" class="headerlink" title="　　6、实现L1和L2损失函数"></a>　　6、实现L1和L2损失函数</h2><p>　　　<img src="http://ow7va355d.bkt.clouddn.com/L1.png" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: L1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L1</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L1 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    loss = np.sum(abs(y-yhat))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line">yhat = np.array([<span class="number">.9</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">.4</span>, <span class="number">.9</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">"L1 = "</span> + str(L1(yhat,y)))</span><br><span class="line">L1 = <span class="number">1.1</span></span><br></pre></td></tr></table></figure></p><p>  <img src="http://ow7va355d.bkt.clouddn.com/L2.png" alt=""></p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: L2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L2 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    loss = np.dot(y-yhat,y-yhat)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line">yhat = np.array([<span class="number">.9</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">.4</span>, <span class="number">.9</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">"L2 = "</span> + str(L2(yhat,y)))</span><br><span class="line">L2 = <span class="number">0.43</span></span><br></pre></td></tr></table></figure><h1 id="二、Logistic回归"><a href="#二、Logistic回归" class="headerlink" title="二、Logistic回归"></a>二、Logistic回归</h1><h2 id="1、用逻辑回归神经网络学习算法识别猫与非猫的算法结构如下图所示："><a href="#1、用逻辑回归神经网络学习算法识别猫与非猫的算法结构如下图所示：" class="headerlink" title="　　1、用逻辑回归神经网络学习算法识别猫与非猫的算法结构如下图所示："></a>　　1、用逻辑回归神经网络学习算法识别猫与非猫的算法结构如下图所示：</h2><p>　　　　　　<img src="http://ow7va355d.bkt.clouddn.com/logistic.png" alt=""></p><h2 id="2、算法的数学表达式如下图所示："><a href="#2、算法的数学表达式如下图所示：" class="headerlink" title="　　2、算法的数学表达式如下图所示："></a>　　2、算法的数学表达式如下图所示：</h2><p><img src="http://ow7va355d.bkt.clouddn.com/logisticmath.png" alt=""></p><h2 id="3、构建算法步骤："><a href="#3、构建算法步骤：" class="headerlink" title="　　3、构建算法步骤："></a>　　3、构建算法步骤：</h2><p>　　　　1）定义模型的结构，像输入特征的个数<br>　　　　2）初始化模型参数<br>　　　　3）循环：<br>　　　　　　　-　计算当前的loss值（前向传播）<br>　　　　　　　-　计算当前的梯度（反向传播）<br>　　　　　　　-　更新参数（梯度下降）</p><h2 id="4、激活函数："><a href="#4、激活函数：" class="headerlink" title="　　4、激活函数："></a>　　4、激活函数：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sigmoid</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    z -- A scalar or numpy array of any size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="comment">### END CODE HERE ###   </span></span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><h2 id="5、初始化参数"><a href="#5、初始化参数" class="headerlink" title="　　5、初始化参数"></a>　　5、初始化参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_with_zeros</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.   </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    dim -- size of the w vector we want (or number of parameters in this case)    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    w -- initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="string">    b -- initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="string">    """</span>    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    w = np.zeros((dim ,<span class="number">1</span>))</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int))    </span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure><h2 id="6、前向传播和反向传播"><a href="#6、前向传播和反向传播" class="headerlink" title="　　6、前向传播和反向传播"></a>　　6、前向传播和反向传播</h2><p><img src="http://ow7va355d.bkt.clouddn.com/%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD.png" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: propagate</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function and its gradient for the propagation explained above</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    cost -- negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="string">    dw -- gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="string">    db -- gradient of the loss with respect to b, thus same shape as b    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    - Write your code step by step for the propagation. np.log(), np.dot()</span></span><br><span class="line"><span class="string">    """</span>    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]    </span><br><span class="line">    <span class="comment"># FORWARD PROPAGATION (FROM X TO COST)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T,X)+b)                                    <span class="comment"># compute activation</span></span><br><span class="line">    cost = <span class="number">-1</span>*((np.sum((Y*np.log(A))+((<span class="number">1</span>-Y)*(np.log(<span class="number">1</span>-A))),axis=<span class="number">1</span>))/m)                                 <span class="comment"># compute cost</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###    </span></span><br><span class="line">    <span class="comment"># BACKWARD PROPAGATION (TO FIND GRAD)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dw = np.dot(X,(A-Y).T)/m</span><br><span class="line">    db = np.sum((A-Y),axis=<span class="number">1</span>)/m</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;    </span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br></pre></td></tr></table></figure></p><h2 id="7、参数更新"><a href="#7、参数更新" class="headerlink" title="　　7、参数更新"></a>　　7、参数更新</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: optimize</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    print_cost -- True to print the loss every 100 steps    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="string">    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="string">    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="string">        1) Calculate the cost and the gradient for the current parameters. Use propagate().</span></span><br><span class="line"><span class="string">        2) Update the parameters using gradient descent rule for w and b.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    costs = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):                </span><br><span class="line">        <span class="comment"># Cost and gradient calculation (≈ 1-4 lines of code)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### </span></span><br><span class="line">        grads, cost = propagate(w,b,X,Y)</span><br><span class="line">        <span class="comment">### END CODE HERE ###        </span></span><br><span class="line">        <span class="comment"># Retrieve derivatives from grads</span></span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]        </span><br><span class="line">        <span class="comment"># update rule (≈ 2 lines of code)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        w = w - learning_rate*dw</span><br><span class="line">        b = b - learning_rate*db</span><br><span class="line">        <span class="comment">### END CODE HERE ###        </span></span><br><span class="line">        <span class="comment"># Record the costs</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 100 training examples</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">    </span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">              <span class="string">"b"</span>: b&#125;</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure><h2 id="8、预测"><a href="#8、预测" class="headerlink" title="　　8、预测"></a>　　8、预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: predict</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>,m))</span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute vector "A" predicting the probabilities of a cat being present in the picture</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T,X)+b)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert probabilities A[0,i] to actual predictions p[0,i]</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">        <span class="keyword">if</span>(A[<span class="number">0</span>,i]&gt;<span class="number">0.5</span>):</span><br><span class="line">            Y_prediction[<span class="number">0</span>,i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>,i] = <span class="number">0</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure><pre><code>Initialize (w,b)Optimize the loss iteratively to learn parameters (w,b):    computing the cost and its gradient    updating the parameters using gradient descentUse the learned (w,b) to predict the labels for a given set of examples</code></pre><h2 id="9、定义模型"><a href="#9、定义模型" class="headerlink" title="　　9、定义模型"></a>　　9、定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Builds the logistic regression model by calling the function you've implemented previously</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string">    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="string">    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string">    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span></span><br><span class="line"><span class="string">    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="string">    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="string">    print_cost -- Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    d -- dictionary containing information about the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize parameters with zeros (≈ 1 line of code)</span></span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Gradient descent (≈ 1 line of code)</span></span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters w and b from dictionary "parameters"</span></span><br><span class="line">    w = parameters[<span class="string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="string">"b"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Predict test/train set examples (≈ 2 lines of code)</span></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    d = &#123;<span class="string">"costs"</span>: costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span>: Y_prediction_test, </span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train, </span><br><span class="line">         <span class="string">"w"</span> : w, </span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span>: num_iterations&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="keyword">True</span>)</span><br><span class="line">Cost after iteration <span class="number">0</span>: <span class="number">0.693147</span></span><br><span class="line">Cost after iteration <span class="number">100</span>: <span class="number">0.584508</span></span><br><span class="line">Cost after iteration <span class="number">200</span>: <span class="number">0.466949</span></span><br><span class="line">......</span><br><span class="line">Cost after iteration <span class="number">1900</span>: <span class="number">0.140872</span></span><br><span class="line">train accuracy: <span class="number">99.04306220095694</span> %</span><br><span class="line">test accuracy: <span class="number">70.0</span> %</span><br></pre></td></tr></table></figure><h2 id="10、不同的学习率对预测结果的影响"><a href="#10、不同的学习率对预测结果的影响" class="headerlink" title="　　10、不同的学习率对预测结果的影响"></a>　　10、不同的学习率对预测结果的影响</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>]</span><br><span class="line">models = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"learning rate is: "</span> + str(i))</span><br><span class="line">    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">1500</span>, learning_rate = i, print_cost = <span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'\n'</span> + <span class="string">"-------------------------------------------------------"</span> + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    plt.plot(np.squeeze(models[str(i)][<span class="string">"costs"</span>]), label= str(models[str(i)][<span class="string">"learning_rate"</span>]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations'</span>)</span><br><span class="line"></span><br><span class="line">legend = plt.legend(loc=<span class="string">'upper center'</span>, shadow=<span class="keyword">True</span>)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(<span class="string">'0.90'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.01</span></span><br><span class="line">train accuracy: <span class="number">99.52153110047847</span> %</span><br><span class="line">test accuracy: <span class="number">68.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.001</span></span><br><span class="line">train accuracy: <span class="number">88.99521531100478</span> %</span><br><span class="line">test accuracy: <span class="number">64.0</span> %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate <span class="keyword">is</span>: <span class="number">0.0001</span></span><br><span class="line">train accuracy: <span class="number">68.42105263157895</span> %</span><br><span class="line">test accuracy: <span class="number">36.0</span> %</span><br></pre></td></tr></table></figure><p><img src="http://ow7va355d.bkt.clouddn.com/learningrate.png" alt=""></p><h1 id="三、单隐层神经网络"><a href="#三、单隐层神经网络" class="headerlink" title="三、单隐层神经网络"></a>三、单隐层神经网络</h1><h2 id="１、前言"><a href="#１、前言" class="headerlink" title="　　１、前言"></a>　　１、前言</h2><p>　通过该内容你将学习到：</p><ul><li>用一个单隐层神经网络实现一个二分类</li><li>使用非线性激活函数</li><li>计算交叉熵损失函数</li><li>实现前向和反向传播</li></ul><p>　使用逻辑回归无法分类：<br>  <img src="http://ow7va355d.bkt.clouddn.com/planer.png" alt=""></p><h2 id="２、神经网络模型"><a href="#２、神经网络模型" class="headerlink" title="　　２、神经网络模型"></a>　　２、神经网络模型</h2><p><img src="http://ow7va355d.bkt.clouddn.com/neuralmodel.png" alt=""></p><h2 id="３、定义神经网络结构"><a href="#３、定义神经网络结构" class="headerlink" title="　　３、定义神经网络结构"></a>　　３、定义神经网络结构</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: layer_sizes</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    n_x -- the size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- the size of the hidden layer(set this to 4) </span></span><br><span class="line"><span class="string">    n_y -- the size of the output layer</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># size of input layer</span></span><br><span class="line">    n_h = <span class="number">4</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment"># size of output layer</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><h2 id="４、初始化模型参数"><a href="#４、初始化模型参数" class="headerlink" title="　　４、初始化模型参数"></a>　　４、初始化模型参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">2</span>) <span class="comment"># we set up a seed so that your output matches ours although the initialization is random.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = np.random.randn(<span class="number">4</span>,<span class="number">2</span>) * <span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((<span class="number">4</span>,<span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(<span class="number">1</span>, <span class="number">4</span>) * <span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span> (W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="５、前向传播"><a href="#５、前向传播" class="headerlink" title="　　５、前向传播"></a>　　５、前向传播</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: forward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters (output of initialization function)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation</span></span><br><span class="line"><span class="string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    b2 = parameters[<span class="string">'b2'</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement Forward Propagation to calculate A2 (probabilities)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    Z1 = np.dot(W1,X)+b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2,A1)+b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure><h2 id="６、计算cost值"><a href="#６、计算cost值" class="headerlink" title="　　６、计算cost值"></a>　　６、计算cost值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cross-entropy cost given in equation (13)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters W1, b1, W2 and b2</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost given equation (13)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of example</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(<span class="number">1</span>-A2),<span class="number">1</span>-Y)</span><br><span class="line">    cost = - np.sum(logprobs) /m</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect. </span></span><br><span class="line">                                <span class="comment"># E.g., turns [[17]] into 17 </span></span><br><span class="line">    <span class="keyword">assert</span>(isinstance(cost, float))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h2 id="７、反向传播"><a href="#７、反向传播" class="headerlink" title="　　７、反向传播"></a>　　７、反向传播</h2><p><img src="http://ow7va355d.bkt.clouddn.com/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD.png" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: backward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation using the instructions above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing our parameters </span></span><br><span class="line"><span class="string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".</span></span><br><span class="line"><span class="string">    X -- input data of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients with respect to different parameters</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First, retrieve W1 and W2 from the dictionary "parameters".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Retrieve also A1 and A2 from dictionary "cache".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A1 = cache[<span class="string">'A1'</span>]</span><br><span class="line">    A2 = cache[<span class="string">'A2'</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)</span></span><br><span class="line">    dZ2 = A2-Y</span><br><span class="line">    dW2 = np.dot(dZ2,A1.T)/m</span><br><span class="line">    db2 = np.sum(dZ2,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)/m</span><br><span class="line">    dZ1 = np.dot(W2.T,dZ2)*(<span class="number">1</span> - np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = np.dot(dZ1,X.T)/m</span><br><span class="line">    db1 = np.sum(dZ1,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)/m</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure></p><p>　好的学习率：　　　　<br>   　　　　<img src="http://ow7va355d.bkt.clouddn.com/sgd.gif" alt=""><br> 　差的学习率：<br>　　　　<img src="http://ow7va355d.bkt.clouddn.com/sgd_bad.gif" alt=""></p><h2 id="８、更新参数"><a href="#８、更新参数" class="headerlink" title="　　８、更新参数"></a>　　８、更新参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Updates parameters using the gradient descent update rule given above</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    b2 = parameters[<span class="string">'b2'</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve each gradient from the dictionary "grads"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    dW1 = grads[<span class="string">'dW1'</span>]</span><br><span class="line">    db1 = grads[<span class="string">'db1'</span>]</span><br><span class="line">    dW2 = grads[<span class="string">'dW2'</span>]</span><br><span class="line">    db2 = grads[<span class="string">'db2'</span>]</span><br><span class="line">    <span class="comment">## END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = W1 - learning_rate*dW1</span><br><span class="line">    b1 = b1 - learning_rate*db1</span><br><span class="line">    W2 = W2 - learning_rate*dW2</span><br><span class="line">    b2 = b2 - learning_rate*db2</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="９、生成模型"><a href="#９、生成模型" class="headerlink" title="　　９、生成模型"></a>　　９、生成模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: nn_model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- dataset of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    num_iterations -- Number of iterations in gradient descent loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, print the cost every 1000 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: "n_x, n_h, n_y". Outputs = "W1, b1, W2, b2, parameters".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 5 lines of code)</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    b2 = parameters[<span class="string">'b2'</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">         </span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">        <span class="comment"># Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="１０、预测"><a href="#１０、预测" class="headerlink" title="　　１０、预测"></a>　　１０、预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: predict</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Using the learned parameters, predicts a class for each example in X</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    predictions -- vector of predictions of our model (red: 0 / blue: 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A2, cache = forward_propagation(X, parameters)</span><br><span class="line">    predictions =  np.around(A2)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><h1 id="四、构建深度神经网络"><a href="#四、构建深度神经网络" class="headerlink" title="四、构建深度神经网络"></a>四、构建深度神经网络</h1><h2 id="１、前言-1"><a href="#１、前言-1" class="headerlink" title="　　１、前言"></a>　　１、前言</h2><ul><li>通过这个编程练习进一步的了解深度学习的结构</li><li>构建函数（例如前向传播，反向传播，逻辑函数，损失函数），这些将有助于分解代码并简化构建神经网络的过程</li><li>根据想要的结构初始化和更新参数</li></ul><h2 id="２、程序构架"><a href="#２、程序构架" class="headerlink" title="　　２、程序构架"></a>　　２、程序构架</h2><p><img src="http://ow7va355d.bkt.clouddn.com/%E6%9E%84%E6%9E%B6.png" alt=""></p><p><strong>　　注意，对于每个正向函数，都有一个相应的反向函数。 这就是为什么在你前向传播的每个模块中，都将在缓存中存储一些值。 缓存的值对于计算梯度是有用的。 在反向传播模块中，您将使用缓存值来计算梯度。 这项任务将显示如何执行这些步骤。</strong></p><h2 id="３、初始化参数"><a href="#３、初始化参数" class="headerlink" title="　　３、初始化参数"></a>　　３、初始化参数</h2><p>　　您将编写两个帮助函数，以初始化模型的参数。 第一个函数将用于初始化两层模型的参数。 第二个将这个初始化过程推广到L层。</p><h3 id="1-两层的神经网络"><a href="#1-两层的神经网络" class="headerlink" title="　　1)两层的神经网络"></a>　　1)两层的神经网络</h3><pre><code>模型结构是: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.初始化权重矩阵：np.random.randn(shape)*0.01.初始化偏置： np.zeros(shape).</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = np.random.randn(n_h,n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h,<span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="2-L层的神经网络"><a href="#2-L层的神经网络" class="headerlink" title="　　2)L层的神经网络"></a>　　2)L层的神经网络</h3><p>　　<br>  　使用这个例子输入Ｘ(12288,209)(m=209的图像，64x64x3=12288)<br><img src="http://ow7va355d.bkt.clouddn.com/L%E5%B1%82.png" alt=""><br>　　代码如下：<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters_deep</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims)            <span class="comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l],layer_dims[l<span class="number">-1</span>]) * <span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p><h2 id="４、前向传播模型"><a href="#４、前向传播模型" class="headerlink" title="　　４、前向传播模型"></a>　　４、前向传播模型</h2><h3 id="１）线性函数"><a href="#１）线性函数" class="headerlink" title="　　１）线性函数"></a>　　１）线性函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: linear_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear part of a layer's forward propagation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- the input of the activation function, also called pre-activation parameter </span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    Z = np.dot(W,A)+b</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (A, W, b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><h3 id="２）激活函数"><a href="#２）激活函数" class="headerlink" title="　　２）激活函数"></a>　　２）激活函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: linear_activation_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- the output of the activation function, also called the post-activation value </span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing "linear_cache" and "activation_cache";</span></span><br><span class="line"><span class="string">             stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><h3 id="3）L层模型"><a href="#3）L层模型" class="headerlink" title="　　3）L层模型"></a>　　3）L层模型</h3><p><img src="http://ow7va355d.bkt.clouddn.com/%EF%BC%AC%E5%B1%82%E6%A8%A1%E5%9E%8B.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: L_model_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- output of initialize_parameters_deep()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    AL -- last post-activation value</span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span></span><br><span class="line"><span class="string">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters) // <span class="number">2</span>                  <span class="comment"># number of layers in the neural network</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement [LINEAR -&gt; RELU]*(L-1). Add "cache" to the "caches" list.</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        A_prev = A </span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        A, cache = relu(np.dot((parameters[<span class="string">'W'</span>+ str(l)]),A_prev)+(parameters[<span class="string">'b'</span>+ str(l)]))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement LINEAR -&gt; SIGMOID. Add "cache" to the "caches" list.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    AL, cache = sigmoid(np.dot((parameters[<span class="string">'W'</span>+ str(L)]),A)+(parameters[<span class="string">'b'</span>+ str(L)]))</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br></pre></td></tr></table></figure><h2 id="５、损失函数"><a href="#５、损失函数" class="headerlink" title="　　５、损失函数"></a>　　５、损失函数</h2><p><img src="http://ow7va355d.bkt.clouddn.com/cost.png" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function defined by equation (7).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute loss from aL and y.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 lines of code)</span></span><br><span class="line">    cost = -np.sum(np.dot(np.log(AL),Y.T) + np.dot(np.log(<span class="number">1</span>-AL),(<span class="number">1</span>-Y).T))/m</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)      <span class="comment"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure></p><h2 id="６、反向传播"><a href="#６、反向传播" class="headerlink" title="　　６、反向传播"></a>　　６、反向传播</h2><h3 id="１）反向传播结构"><a href="#１）反向传播结构" class="headerlink" title="　　１）反向传播结构"></a>　　１）反向传播结构</h3><p><img src="http://ow7va355d.bkt.clouddn.com/Backward%20propagation%20module.png" alt=""></p><h3 id="２）计算导数"><a href="#２）计算导数" class="headerlink" title="　　２）计算导数"></a>　　２）计算导数</h3><p><img src="http://ow7va355d.bkt.clouddn.com/Linear%20backward.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: linear_backward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class="line"><span class="string">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    dW = np.dot(dZ,A_prev.T)/m</span><br><span class="line">    db = np.sum(dZ,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)/m</span><br><span class="line">    dA_prev = np.dot(W.T,dZ)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span> (db.shape == b.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><p>　　Linear-Activation backward<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: linear_activation_backward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient for current layer l </span></span><br><span class="line"><span class="string">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure></p><h3 id="３）L层模型反向传播"><a href="#３）L层模型反向传播" class="headerlink" title="　　３）L层模型反向传播"></a>　　３）L层模型反向传播</h3><p><img src="http://ow7va355d.bkt.clouddn.com/L-Model%20Backward.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: L_model_backward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class="line"><span class="string">                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- A dictionary with the gradients</span></span><br><span class="line"><span class="string">             grads["dA" + str(l)] = ... </span></span><br><span class="line"><span class="string">             grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads["db" + str(l)] = ... </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = len(caches) <span class="comment"># the number of layers</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>]</span><br><span class="line">    Y = Y.reshape(AL.shape) <span class="comment"># after this line, Y is the same shape as AL</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initializing the backpropagation</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line of code)</span></span><br><span class="line">    dAL = -(np.divide(Y, AL)-np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: "AL, Y, caches". Outputs: "grads["dAL"], grads["dWL"], grads["dbL"]</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">    current_cache = caches[L<span class="number">-1</span>]</span><br><span class="line">    grads[<span class="string">"dA"</span> + str(L)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] =  linear_activation_backward(dAL,current_cache,activation=<span class="string">"sigmoid"</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):</span><br><span class="line">        <span class="comment"># lth layer: (RELU -&gt; LINEAR) gradients.</span></span><br><span class="line">        <span class="comment"># Inputs: "grads["dA" + str(l + 2)], caches". Outputs: "grads["dA" + str(l + 1)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 5 lines)</span></span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">"dA"</span>+str(l+<span class="number">2</span>)],current_cache,<span class="string">"relu"</span>)</span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l + <span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db_temp</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><h3 id="4）更新参数"><a href="#4）更新参数" class="headerlink" title="　　4）更新参数"></a>　　4）更新参数</h3><p><img src="http://ow7va355d.bkt.clouddn.com/Update%20Parameters.png" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using gradient descent</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">                  parameters["W" + str(l)] = ... </span></span><br><span class="line"><span class="string">                  parameters["b" + str(l)] = ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter. Use a for loop.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">            parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)]-grads[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]*learning_rate</span><br><span class="line">            parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)]-grads[<span class="string">"db"</span>+str(l+<span class="number">1</span>)]*learning_rate</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p><h1 id="五、用深度神经网络识别猫"><a href="#五、用深度神经网络识别猫" class="headerlink" title="五、用深度神经网络识别猫"></a>五、用深度神经网络识别猫</h1><p><img src="http://ow7va355d.bkt.clouddn.com/reshape.png" alt=""></p><p><img src="http://ow7va355d.bkt.clouddn.com/2-layer%20neural%20network.png" alt=""></p><p><img src="http://ow7va355d.bkt.clouddn.com/L-layer%20deep%20neural%20network.png" alt=""></p><p>两层模型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: two_layer_model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (n_x, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- dimensions of the layers (n_x, n_h, n_y)</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    print_cost -- If set to True, this will print the cost every 100 iterations </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary containing W1, W2, b1, and b2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []                              <span class="comment"># to keep track of the cost</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                           <span class="comment"># number of examples</span></span><br><span class="line">    (n_x, n_h, n_y) = layers_dims</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters dictionary, by calling one of the functions you'd previously implemented</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get W1, b1, W2 and b2 from the dictionary parameters.</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: "X, W1, b1". Output: "A1, cache1, A2, cache2".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        A1, cache1 = linear_activation_forward(X, W1, b1, activation = <span class="string">"relu"</span>)</span><br><span class="line">        A2, cache2 = linear_activation_forward(A1, W2, b2,activation = <span class="string">"sigmoid"</span>)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">        cost = compute_cost(A2, Y)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initializing backward propagation</span></span><br><span class="line">        dA2 = - (np.divide(Y, A2) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - A2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Backward propagation. Inputs: "dA2, cache2, cache1". Outputs: "dA1, dW2, db2; also dA0 (not used), dW1, db1".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        dA1, dW2, db2 =  linear_activation_backward(dA2, cache2, activation=<span class="string">"sigmoid"</span>)</span><br><span class="line">        dA0, dW1, db1 =  linear_activation_backward(dA1, cache1, activation=<span class="string">"relu"</span>)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2</span></span><br><span class="line">        grads[<span class="string">'dW1'</span>] = dW1</span><br><span class="line">        grads[<span class="string">'db1'</span>] = db1</span><br><span class="line">        grads[<span class="string">'dW2'</span>] = dW2</span><br><span class="line">        grads[<span class="string">'db2'</span>] = db2</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 1 line of code)</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve W1, b1, W2, b2 from parameters</span></span><br><span class="line">        W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">        b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">        W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">        b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 100 training example</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, np.squeeze(cost)))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">       </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line"></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p><p>L层模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: L_layer_model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span><span class="comment">#lr was 0.009</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    costs = []                         <span class="comment"># keep track of cost</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Parameters initialization.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">        AL, caches = L_model_forward(X, parameters)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute cost.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">                </span><br><span class="line">        <span class="comment"># Print the cost every 100 training example</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　这篇博客内容是总结DeepLearning.ai课程第一课Neural Networks and Deep Learning内容。第一课分为四周，第一周是深度学习的介绍；第二周是神经网络的基础；第三周是浅层神经网络；第四周是深层神经网络。&lt;br&gt;
    
    </summary>
    
      <category term="工作" scheme="http://www.douxiao.org/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="DeepLearning" scheme="http://www.douxiao.org/tags/DeepLearning/"/>
    
      <category term="神经网络" scheme="http://www.douxiao.org/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>AKJ_UAV_APP调试问题及解决办法</title>
    <link href="http://www.douxiao.org/2017/09/26/AKJ-UAV-APP%E8%B0%83%E8%AF%95%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <id>http://www.douxiao.org/2017/09/26/AKJ-UAV-APP调试问题及解决办法/</id>
    <published>2017-09-26T01:17:00.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>  <font size="5" color="lightseagreen"> 在调试ARDRone parrot2.0飞机APP过程中遇到的一些问题及解决办法。<br> </font><br><a id="more"></a></p><h2 id="问题一、如何导入Vitamio库"><a href="#问题一、如何导入Vitamio库" class="headerlink" title="问题一、如何导入Vitamio库"></a><a href="#问题一、如何导入Vitamio库" title="问题一、如何导入Vitamio库"></a>问题一、如何导入Vitamio库</h2><p><a href="http://blog.csdn.net/rd_w_csdn/article/details/53466372" target="_blank" rel="noopener">解决办法： Vitamio 的导入与简单使用</a><br><a href="http://www.cnblogs.com/happyhacking/p/5365192.html" target="_blank" rel="noopener">Android Studio导入Vitamio多媒体开发框架</a><br>但是按照上面方法还是没有导入成功,只差这一步：<br>在工程settings.gradle 添加<code>include‘：vitamio’</code><br>如图：<br><img src="http://ow7va355d.bkt.clouddn.com/Screenshot%20from%202018-01-20%2011-10-42.png" alt=""></p><h2 id="问题二、关闭MIUI优化后，在调试APP时，会在手机端报问题，但手机变得非常卡顿。"><a href="#问题二、关闭MIUI优化后，在调试APP时，会在手机端报问题，但手机变得非常卡顿。" class="headerlink" title="问题二、关闭MIUI优化后，在调试APP时，会在手机端报问题，但手机变得非常卡顿。"></a><a href="#问题二、关闭MIUI优化后，在调试APP时，会在手机端报问题，但手机变得非常卡顿。" title="问题二、关闭MIUI优化后，在调试APP时，会在手机端报问题，但手机变得非常卡顿。"></a>问题二、关闭MIUI优化后，在调试APP时，会在手机端报问题，但手机变得非常卡顿。</h2><h2 id="问题三：在APP中使用Vitamio视频播放插件时，在android5-0的手机使用没有问题，但是在小米5-android7-0的手机中出现以下问题"><a href="#问题三：在APP中使用Vitamio视频播放插件时，在android5-0的手机使用没有问题，但是在小米5-android7-0的手机中出现以下问题" class="headerlink" title="问题三：在APP中使用Vitamio视频播放插件时，在android5.0的手机使用没有问题，但是在小米5,android7.0的手机中出现以下问题"></a><a href="#问题三：在APP中使用Vitamio视频播放插件时，在android5-0的手机使用没有问题，但是在小米5-android7-0的手机中出现以下问题" title="问题三：在APP中使用Vitamio视频播放插件时，在android5.0的手机使用没有问题，但是在小米5,android7.0的手机中出现以下问题"></a>问题三：在APP中使用Vitamio视频播放插件时，在android5.0的手机使用没有问题，但是在小米5,android7.0的手机中出现以下问题</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">java.lang.UnsatisfiedLinkError: No implementation found <span class="keyword">for</span> <span class="keyword">void</span> io.vov.vitamio.MediaPlayer.native_init() (tried Java_io_vov_vitamio_MediaPlayer_native_1init</div></pre></td></tr></table></figure><p><a href="https://github.com/yixia/VitamioBundle/issues/385" target="_blank" rel="noopener">解决办法：</a>使用<a href="https://www.vitamio.org/Download/" target="_blank" rel="noopener">Vitamio5.2</a></p><p>在使用5.2版本后出现以下问题：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">java.lang.UnsatisfiedLinkError: Expecting an absolute path of the library: libstlport_shared.so</div></pre></td></tr></table></figure><p>解决办法还在上面的链接。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">hi all, i use vitamio version <span class="number">5.2</span>.3 and get the same problem and i have tried all the above solutions and nothing works.</div><div class="line">then i <span class="keyword">try</span> to change the code bit on MediaPlayer.java</div><div class="line"></div><div class="line"><span class="keyword">if</span>(LIB_ROOT==<span class="keyword">null</span>)&#123;</div><div class="line">                System.loadLibrary(<span class="string">“stlport_shared”</span>);</div><div class="line">                System.loadLibrary(<span class="string">“vplayer”</span>);    </div><div class="line">                loadFFmpeg_native(<span class="string">“libffmpeg.so”</span>);</div><div class="line">                loadVVO_native(<span class="string">“libvvo.9.so”</span>);</div><div class="line">                loadVVO_native(<span class="string">“libvvo.9.so”</span>);</div><div class="line">                loadVAO_native(<span class="string">“libvao.0.so”</span>);</div><div class="line">            &#125;<span class="keyword">else</span>&#123;</div><div class="line">                System.load(LIB_ROOT+ <span class="string">“libstlport_shared.so”</span>);</div><div class="line">                System.load(LIB_ROOT+ <span class="string">“libvplayer.so”</span>);</div><div class="line">                loadFFmpeg_native(LIB_ROOT+<span class="string">“libffmpeg.so”</span>);</div><div class="line">                loadVVO_native(LIB_ROOT+ <span class="string">“libvvo.9.so”</span>);</div><div class="line">                loadVAO_native( LIB_ROOT+ <span class="string">“libvao.0.so”</span>);      </div><div class="line">            &#125;</div><div class="line">and replace it with</div><div class="line"></div><div class="line"><span class="keyword">if</span>(LIB_ROOT==<span class="keyword">null</span>)&#123;</div><div class="line">                System.loadLibrary(<span class="string">“stlport_shared”</span>);</div><div class="line">                System.loadLibrary(<span class="string">“vplayer”</span>);    </div><div class="line">                loadFFmpeg_native(<span class="string">“libffmpeg.so”</span>);</div><div class="line">                loadVVO_native(<span class="string">“libvvo.9.so”</span>);</div><div class="line">                loadVVO_native(<span class="string">“libvvo.9.so”</span>);</div><div class="line">                loadVAO_native(<span class="string">“libvao.0.so”</span>);</div><div class="line">            &#125;<span class="keyword">else</span>&#123;</div><div class="line">                  System.loadLibrary(<span class="string">“stlport_shared”</span>);</div><div class="line">                  System.loadLibrary(<span class="string">“vplayer”</span>);</div><div class="line">                  loadFFmpeg_native(<span class="string">“libffmpeg.so”</span>);</div><div class="line">                  loadVVO_native(<span class="string">“libvvo.9.so”</span>);</div><div class="line">                  loadVVO_native(<span class="string">“libvvo.9.so”</span>);</div><div class="line">                  loadVAO_native(<span class="string">“libvao.0.so”</span>);</div><div class="line">            &#125;</div><div class="line">after that I <span class="keyword">try</span> in emulator android <span class="number">7.1</span>.1 and succeed</div><div class="line">I hope <span class="keyword">this</span> way also solve your problem :)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;  &lt;font size=&quot;5&quot; color=&quot;lightseagreen&quot;&gt; 在调试ARDRone parrot2.0飞机APP过程中遇到的一些问题及解决办法。&lt;br&gt; &lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="工作" scheme="http://www.douxiao.org/categories/%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="android" scheme="http://www.douxiao.org/tags/android/"/>
    
  </entry>
  
  <entry>
    <title>搭建Hexo+github博客(二)</title>
    <link href="http://www.douxiao.org/2017/09/23/%E6%90%AD%E5%BB%BAHexo-github%E5%8D%9A%E5%AE%A2-1/"/>
    <id>http://www.douxiao.org/2017/09/23/搭建Hexo-github博客-1/</id>
    <published>2017-09-23T14:31:56.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<font size="5" color="darkturquoise"> 如何在不同电脑上同时写hexo博客？</font><br><a id="more"></a><br><br><font size="5">前言</font><p> 　　Hexo通过将Markdown文件编译成html文件，然后将html文件直接部署到网站上，所以被称作静态博客，因为直接就是访问的最终的html文件，不会有PHP那样的中间处理，所以对浏览器来讲会比较快。但是这样在不同的电脑上写博客的时候，从github上pull下来，只能够得到一些静态页面，而且没有源md文件,所以在每次提交博客部署的时候，就会把原来的博客清空。<br>  　　为了完美解决这个问题，可以<strong>利用github的不同分支来分别保存网站静态文件与hexo源码（md原始文件及主题等）</strong>，实现在不同电脑上都可以自由写博客，这样不好之处就是别人能得到你的博客源文件。<br>　　<br>　　<strong>   注：假设你有工作PC和个人PC，你原有的博客搭建在个人PC上，以下首先介绍个人PC上的操作步骤。</strong></p><h1 id="个人PC"><a href="#个人PC" class="headerlink" title="个人PC"></a><font size="5">个人PC</font></h1><h2 id="在github上新建远程仓库"><a href="#在github上新建远程仓库" class="headerlink" title=" 在github上新建远程仓库"></a><font size="4" color="darkturquoise"> 在github上新建远程仓库</font></h2><p>　　将原来的page项目删除，新建一个和原来名字一样的空项目。不用初始README.md，此时只有一个空的master分支。<br>　　<strong>本地的目录不要动</strong>，到时候需要用你原来博客的配置和文章替换该项目中的文件夹。你可以把你本地的博客目录复制一份作为备份，以免误操作将原来的内容进行改动。</p><h2 id="本地初始化一个Hexo项目"><a href="#本地初始化一个Hexo项目" class="headerlink" title=" 本地初始化一个Hexo项目"></a><font size="4" color="darkturquoise"> 本地初始化一个Hexo项目</font></h2><p>　</p><p>　　新建一个空目录，比如我的文件名为DxBlog,作为你的博客目录。进入该目录，执行以下命令:<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br><span class="line">npm <span class="keyword">install</span></span><br><span class="line">npm <span class="keyword">install</span> hexo-deployer-git <span class="comment">--save</span></span><br></pre></td></tr></table></figure></p><p>　　然后用自己原来博客里的文件替换掉这里的<font color="red">source\ ,  scaffolds\ , themes\ , _config.yml </font> 替换成自己原来博客里的。注意，<strong>这里需要把themes/yelee中的.git/目录删除</strong>，因为在安装博客主题的时候是直接git下来的，会有原始的git信息，如果不删除的话，推送到hexo分支后yelee目录为空，自己的电脑是linux系统./git目录是看不到的　需要用命令行：<code>sudo rm -rf .git/</code>删除。<strong>还有如果之前更改过node_modules文件中的东西同样需要更改，比如球形标签云。</strong></p><h2 id="将整个目录推送到master"><a href="#将整个目录推送到master" class="headerlink" title=" 将整个目录推送到master"></a><font size="4" color="darkturquoise"> 将整个目录推送到master</font></h2><p></p><p>　　要推送到master分支，首先要将该目录初始化为本地Git仓库：<br>  <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br><span class="line"><span class="comment">//把博客目录下所有文件推送到master分支</span></span><br><span class="line">git remote add origin git@github<span class="selector-class">.com</span>:douxiao/douxiao<span class="selector-class">.github</span><span class="selector-class">.io</span><span class="selector-class">.git</span></span><br><span class="line">git add .　<span class="comment">//添加当前工作目录文件到index</span></span><br><span class="line">git commit -m <span class="string">"some comments"</span> <span class="comment">//生成一个commit</span></span><br><span class="line">git push origin master     <span class="comment">//推送服务器</span></span><br></pre></td></tr></table></figure></p><h2 id="在github上新建一个分支"><a href="#在github上新建一个分支" class="headerlink" title=" 在github上新建一个分支"></a><font size="4" color="darkturquoise"> 在github上新建一个分支</font></h2><p></p><p>　　新建一个分支hexo(名字可以自定义)，这时候hexo分支和master分支的内容一样，都是hexo的源文件。<strong>并把hexo设为默认分支，这一步需要在你的库设置里完成</strong>，这样的话在另外一台机器上克隆下来就直接进入hexo分支，并且以后所有操作都是在hexo分支下完成。<br>　　为什么需要这个额外的分支呢？<br>　　因为<code>hexo d</code>只把静态网页文件部署到master分支上，所以你换了另外一台电脑，就无法pull下来继续写博客了。有了hexo分支的话，就可以把hexo分支中的源文件(配置文件、主题样式等)pull下来，再hexo g的话就可以生成一模一样的静态文件了。</p><h2 id="部署博客"><a href="#部署博客" class="headerlink" title=" 部署博客"></a><font size="4" color="darkturquoise"> 部署博客</font></h2><p>　<code>hexo g -d</code><br>　　博客已经成功部署到master分支，这时候到github查看两个分支的内容，hexo分支里是源文件，master里是静态文件。<br>　　<strong>注意：网站根目录下的_config.yml配置文件中branch一定要填master，否则hexo d就会部署到hexo分支下</strong></p><h2 id="关联到远程hexo分支"><a href="#关联到远程hexo分支" class="headerlink" title=" 关联到远程hexo分支"></a><font size="4" color="darkturquoise"> 关联到远程hexo分支</font></h2><p>　　在本地新建一个hexo分支并与远程hexo分支关联：<br>  <figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout -<span class="keyword">b </span>hexo</span><br><span class="line">git pull <span class="keyword">origin </span>hexo</span><br></pre></td></tr></table></figure></p><p>　　另外别忘了，如果有修改的话，要推送到hexo分支上去：<br><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">add</span><span class="bash"> .</span></span><br><span class="line"><span class="bash">git commit -m  <span class="string">""</span></span></span><br><span class="line"><span class="bash">git push origin hexo</span></span><br></pre></td></tr></table></figure></p><h1 id="工作PC"><a href="#工作PC" class="headerlink" title="工作PC"></a><font size="5">工作PC</font></h1><p>　　个人PC上的工作已经完成了，下面讲一下如果你换到了另外一台电脑上，应该如何操作。<br>　　将博客项目克隆下来,<font color="red"> 这里特别注意以后的操作需要在这个克隆下来的文件下操作，因为里面有git信息，在之后的git push才能正确完成，自己在这里踩坑了。</font><br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github<span class="selector-class">.com</span>:douxiao/douxiao<span class="selector-class">.github</span><span class="selector-class">.io</span><span class="selector-class">.git</span></span><br></pre></td></tr></table></figure></p><p>　　克隆下来的仓库和你在个人PC中的目录是一模一样的，所以可以在这基础上继续写博客了。但是由于.gitignore文件中过滤了<strong>node_modules</strong>\，所以克隆下来的目录里没有node_modules\，这是hexo所需要的组件，所以要在该目录中重新安装hexo，但不需要hexo init。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm <span class="keyword">install</span> -g hexo-cli</span><br><span class="line">npm <span class="keyword">install</span></span><br><span class="line">npm <span class="keyword">install</span> hexo-deployer-git <span class="comment">--save</span></span><br></pre></td></tr></table></figure></p><p>　　<strong>注意：这里一般还需要安装一些一些必须的插件，就是说可靠像之前在个人pc上安装的那些插件一样。</strong></p><p>　　新建一篇文章测试<br><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo <span class="keyword">new</span> <span class="string">"work PC test"</span></span><br></pre></td></tr></table></figure></p><p>　　推送到hexo分支<br><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git init </span><br><span class="line">git <span class="keyword">add</span><span class="bash"> .</span></span><br><span class="line"><span class="bash">git commit -m <span class="string">"add work PC test"</span></span></span><br><span class="line"><span class="bash">git push origin hexo</span></span><br></pre></td></tr></table></figure></p><p>　　部署到master分支<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hexo g -d</span></span><br></pre></td></tr></table></figure></p><h1 id="日常操作"><a href="#日常操作" class="headerlink" title="日常操作"></a><font size="5">日常操作</font></h1><p>　　如果上面的过程都操作无误的话，你就可以在任何能联网的电脑上写博客啦。一般写博客的流程是下面这样。<br><strong>写博客前</strong><br>　　不管你本地的仓库是否是最新的，都先pull一下，以防万一：</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">git pull origin hexo</span></span><br></pre></td></tr></table></figure><p>　　<strong>在这里自己遇到的问题是</strong>：每次在git pull之前，需要执行命令 <code>git stash</code><br>执行完git pull后需要执行　｀git stash pop｀，<a href="http://blog.csdn.net/iefreer/article/details/7679631" target="_blank" rel="noopener">Git:代码冲突常见解决方法</a>，<a href="http://www.cppblog.com/deercoder/archive/2011/11/13/160007.aspx" target="_blank" rel="noopener">git stash用法</a><br>　　把最新的pull下来，再开始撰写新的博客。<br>　　写博客<br><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo <span class="keyword">new</span> <span class="string">"title"</span></span><br></pre></td></tr></table></figure></p><p>　　然后打开source/_posts/title.md，撰写博文。<br>　　写完博客<br>　　先推送到hexo分支上：<br><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">add</span><span class="bash"> .</span></span><br><span class="line"><span class="bash">git commit -m <span class="string">"add article xxx"</span></span></span><br><span class="line"><span class="bash">git push origin hexo</span></span><br></pre></td></tr></table></figure></p><p>　　<strong>在这里自己遇到的问题是</strong>：Git-Permission denied (publickey)解决办法因电脑而异。自行谷歌。</p><p>　　最后部署到master分支上：<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hexo g -d</span></span><br></pre></td></tr></table></figure></p><h1 id="常见问题解决"><a href="#常见问题解决" class="headerlink" title="常见问题解决"></a><font size="5">常见问题解决</font></h1><ol><li><a href="https://www.zhihu.com/question/28847824" target="_blank" rel="noopener">配置hexo 为什么运行到 hexo server 这步就没用了？</a></li><li><a href="http://hanhailong.com/2015/10/08/Hexo%E2%80%94%E6%AD%A3%E7%A1%AE%E6%B7%BB%E5%8A%A0RSS%E8%AE%A2%E9%98%85/" target="_blank" rel="noopener">Hexo—正确添加RSS订阅</a></li><li><a href="http://chown-jane-y.coding.me/2017/03/15/%E5%A6%82%E4%BD%95%E5%9C%A8%E4%B8%8D%E5%90%8C%E7%94%B5%E8%84%91%E4%B8%8A%E5%90%8C%E6%97%B6%E5%86%99hexo%E5%8D%9A%E5%AE%A2%EF%BC%9F/" target="_blank" rel="noopener">如何在不同电脑上同时写hexo博客？</a></li><li><a href="http://blog.csdn.net/yemoweiliang/article/details/53215979" target="_blank" rel="noopener">删除本地SSH并新建一个ssh</a></li><li><a href="http://www.cnblogs.com/ayseeing/p/3572582.html" target="_blank" rel="noopener">在github上添加SSH key的步骤：</a></li><li><a href="https://stackoverflow.com/questions/2643502/git-permission-denied-publickey" target="_blank" rel="noopener">Git-Permission denied (publickey)</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;font size=&quot;5&quot; color=&quot;darkturquoise&quot;&gt; 如何在不同电脑上同时写hexo博客？&lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="博客" scheme="http://www.douxiao.org/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="hexo" scheme="http://www.douxiao.org/tags/hexo/"/>
    
      <category term="github" scheme="http://www.douxiao.org/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>搭建Hexo+github博客(一)</title>
    <link href="http://www.douxiao.org/2017/09/23/%E6%90%AD%E5%BB%BAHexo-github%E5%8D%9A%E5%AE%A2/"/>
    <id>http://www.douxiao.org/2017/09/23/搭建Hexo-github博客/</id>
    <published>2017-09-23T11:58:24.000Z</published>
    <updated>2018-05-30T14:04:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　<font size="5" color="lightseagreen"> 　这篇文章是总结自己的博客搭建之路，包括：初始识Hexo、配置博客的环境、初始化博客、域名申请、绑定域名、博客主题的选定、优化配置博客、markdown编辑器的选定、工作和个人笔记本同时写博客，工作电脑是linux系统，个人电脑是windows系统，这篇文章主要是按照Linux系统的安装布置博客。可能会分几篇博文来写。</font><br><a id="more"></a></p><h1 id="一、初识Hexo"><a href="#一、初识Hexo" class="headerlink" title="一、初识Hexo"></a><font size="5"><strong>一、初识Hexo</strong></font></h1><p>　　　<a href="https://hexo.io/zh-cn/docs/index.html" target="_blank" rel="noopener">Hexo</a>是一款基于Node.js ，快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。</p><h1 id="二、安装Git环境"><a href="#二、安装Git环境" class="headerlink" title="二、安装Git环境"></a><font size="5"><strong>二、安装Git环境</strong></font></h1><p>　　　配置git前，需要到<a href="http://www.github.com" target="_blank" rel="noopener">github官网</a>注册自己的账号,然新建一个username.github.io的仓库。<br>　　　<a href="https://git-scm.com/book/zh/v1/%E8%B5%B7%E6%AD%A5-%E5%AE%89%E8%A3%85-Git" target="_blank" rel="noopener">git安装教程</a><br>　　　<a href="https://git-scm.com/book/zh/v1/%E8%B5%B7%E6%AD%A5-%E5%88%9D%E6%AC%A1%E8%BF%90%E8%A1%8C-Git-%E5%89%8D%E7%9A%84%E9%85%8D%E7%BD%AE" target="_blank" rel="noopener">git配置教程</a><br>　　　<a href="https://git-scm.com/book/zh/v1/%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84-Git-%E7%94%9F%E6%88%90-SSH-%E5%85%AC%E9%92%A5" target="_blank" rel="noopener">git生成ssh公钥</a><br>   <figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git<span class="built_in"> config </span>--global user.name <span class="string">"John Doe"</span></span><br><span class="line">$ git<span class="built_in"> config </span>--global user.email johndoe@example.com</span><br></pre></td></tr></table></figure></p><h1 id="三、安装Node-JS环境"><a href="#三、安装Node-JS环境" class="headerlink" title="三、安装Node.JS环境"></a><font size="5"><strong>三、安装Node.JS环境</strong></font></h1><p>  　　　1. <a href="https://nodejs.org/en/" target="_blank" rel="noopener">Node.j安装下载地址</a><br>  　　　2. <a href="https://linux.cn/article-5766-1.html" target="_blank" rel="noopener">在Ubuntu下安装Node.JS的不同方式</a><br>  　　　3. 我使用的是按照命令行的方式，（如果是windows环境，直接下载软件安装就可以了，不过安装后需要重启电脑才能够生效）代码如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> apt-get install nodejs</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> npm -v</span></span><br><span class="line">   5.4.2</span><br><span class="line"><span class="meta">$</span><span class="bash"> npm -v</span></span><br><span class="line">v8.4.0</span><br></pre></td></tr></table></figure></p><p>注意：用命令行安装的版本一般不是很高，需要进行升级，升级教程:<a href="http://www.16boke.com/article/detail/26" target="_blank" rel="noopener">一行命令搞定node.js 版本升级</a></p><h1 id="四、安装Hexo"><a href="#四、安装Hexo" class="headerlink" title="四、安装Hexo"></a><font size="5"><strong>四、<a href="https://hexo.io/zh-cn/docs/setup.html" target="_blank" rel="noopener">安装Hexo</a></strong></font></h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> npm install -g hexo-cli</span></span><br></pre></td></tr></table></figure><p>　　安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hexo init &lt;folder&gt;</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> &lt;folder&gt;</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> npm install</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo new <span class="built_in">test</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo g    <span class="comment">#生成静态文件</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo s    <span class="comment"># 启动服务器。默认情况下，访问网址为： http://localhost:4000/。</span></span></span><br></pre></td></tr></table></figure></p><p>　　现在来介绍常用的Hexo 命令</p><p>　　npm install hexo -g #安装Hexo<br>　　npm update hexo -g #升级<br>　　hexo init #初始化博客<br>　<br>　　命令简写<br>　　hexo n “我的博客” == hexo new “我的博客” #新建文章<br>　　hexo g == hexo generate #生成<br>　　hexo s == hexo server #启动服务预览<br>　　hexo d == hexo deploy #部署</p><p>　　hexo server #Hexo会监视文件变动并自动更新，无须重启服务器<br>　　hexo server -s #静态模式<br>　　hexo server -p 5000 #更改端口<br>　　hexo server -i 192.168.1.1 #自定义 IP<br>　　hexo clean #清除缓存，若是网页正常情况下可以忽略这条命令</p><p><strong>部署推送网站</strong></p><p>　　下一步将我们的Hexo与GitHub关联起来，打开站点的配置文件_config.yml，翻到最后修改为：</p><p>　　deploy:<br>　　type: git<br>　　repo: 这里填入你之前在GitHub上创建仓库的完整路径，记得加上 .git<br>　　branch: master参考如下：<br>　　保存站点配置文件。</p><p>　　其实就是给hexo d 这个命令做相应的配置，让hexo知道你要把blog部署在哪个位置，很显然，我们部署在我们GitHub的仓库里。最后安装Git部署插件，输入命令：</p>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> npm install hexo-deployer-git --save</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo clean </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo g </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo d</span></span><br></pre></td></tr></table></figure><p>　　访问网站：usrname.github.io</p><h1 id="五、注册并绑定域名"><a href="#五、注册并绑定域名" class="headerlink" title="五、注册并绑定域名"></a><font size="5"><strong>五、<a href="http://www.jianshu.com/p/cea41e5c9b2a" target="_blank" rel="noopener">注册并绑定域名</a></strong></font></h1><p>　　<br> 　　<a href="http://www.douxiao.org">域名访问博客</a></p><h1 id="六、更换主题及优化"><a href="#六、更换主题及优化" class="headerlink" title="六、更换主题及优化"></a><font size="5"><strong>六、更换主题及优化</strong></font></h1><p>　1.博客默认的是landscape主题。<br>　2.自己使用了一段时间next,next主题挺不错的自己也配置了很久，<a href="http://theme-next.iissnan.com/theme-settings.html" target="_blank" rel="noopener">Next主题配置教程官网</a>，<a href="https://juejin.im/post/58eb2fd2a0bb9f006928f8c7" target="_blank" rel="noopener">Next主题优化</a>.<br>　3.自己使用的主题是Yelee主题。<br>  　　<a href="http://moxfive.coding.me/yelee/" target="_blank" rel="noopener">Yelee主题使用说明</a><br>  　　<a href="http://www.netcan666.com/2015/12/15/Hexo%E4%B8%AA%E6%80%A7%E5%8C%96%E7%90%83%E5%BD%A2%E6%A0%87%E7%AD%BE%E4%BA%91/#u6548_u679C_u56FE" target="_blank" rel="noopener">Yelee主题增加球形标签云</a><br>  　　<a href="http://ngudream.com/2017/01/24/n-hexo-blog/" target="_blank" rel="noopener">Yelee主题其他优化</a><br>　　<a href="http://www.flametao.cn/2017/01/29/wordcount/" target="_blank" rel="noopener">Yelee主题字数统计</a><br>　4.Yelee主题增加红心，鼠标点击小红心的设置<br>　　将 <a href="http://7u2ss1.com1.z0.glb.clouddn.com/love.js" target="_blank" rel="noopener">love.js</a> 文件下载添加到 hexo/themes/yelee/source/js 文件目录下。<br>　　找到 hexo/themes/yelee/layout/_partial/footer.ejs 文件， 在文件的最后， 添加以下代码：<br>  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">&lt;!--页面点击小红心--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"/js/love.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>  　5.<a href="http://jomsou.me/2017/07/16/livere/" target="_blank" rel="noopener">增加来必力评论系统</a> </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　&lt;font size=&quot;5&quot; color=&quot;lightseagreen&quot;&gt; 　这篇文章是总结自己的博客搭建之路，包括：初始识Hexo、配置博客的环境、初始化博客、域名申请、绑定域名、博客主题的选定、优化配置博客、markdown编辑器的选定、工作和个人笔记本同时写博客，工作电脑是linux系统，个人电脑是windows系统，这篇文章主要是按照Linux系统的安装布置博客。可能会分几篇博文来写。&lt;/font&gt;&lt;br&gt;
    
    </summary>
    
      <category term="博客" scheme="http://www.douxiao.org/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="hexo" scheme="http://www.douxiao.org/tags/hexo/"/>
    
      <category term="github" scheme="http://www.douxiao.org/tags/github/"/>
    
  </entry>
  
</feed>
